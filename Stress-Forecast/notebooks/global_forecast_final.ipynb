{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0733de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1) LOAD DATA\n",
      "================================================================================\n",
      "DATA_PATH       : ..\\datasets\\global_dataset_pred.csv\n",
      "ROWS            : 275\n",
      "USERS           : 5\n",
      "DATE_RANGE      : 2025-11-21 -> 2026-01-14\n",
      "TARGET_COL      : stressLevelPred\n",
      "BEHAVIOR_COLS   : ['extracurricularHourPerDay', 'physicalActivityHourPerDay', 'sleepHourPerDay', 'studyHourPerDay', 'socialHourPerDay']\n",
      "\n",
      "================================================================================\n",
      "2) FEATURE ENGINEERING (NO-LEAK)\n",
      "================================================================================\n",
      "ROWS_FEAT        : 260\n",
      "USERS_FEAT       : 5\n",
      "WINDOW           : 3\n",
      "TEST_LEN         : 12\n",
      "FEATURES_COUNT   : 18\n",
      "BINARY_DIST      : {1: 146, 0: 114}\n",
      "\n",
      "================================================================================\n",
      "3) SPLIT (TIME-BASED PER USER)\n",
      "================================================================================\n",
      "TRAINPOOL_ROWS   : 200\n",
      "TEST_ROWS        : 60\n",
      "TEST_DIST        : {1: 38, 0: 22}\n",
      "CV_FOLDS         : 2\n",
      "VAL_WINDOWS      : [(12, 24), (18, 30)]\n",
      "\n",
      "================================================================================\n",
      "4) BASELINE L1: PERSISTENCE (y(t) = y(t-1))\n",
      "================================================================================\n",
      "TEST_ACC         : 0.7166666666666667\n",
      "TEST_F1          : 0.7671232876712328\n",
      "\n",
      "================================================================================\n",
      "5) BASELINE L2: MARKOV GLOBAL (prev_high, dow) + THR TUNING (POOLED CV)\n",
      "================================================================================\n",
      "CV_POOLED_DIST   : {0: 34, 1: 86}\n",
      "BEST_THR_MARKOV  : 0.35\n",
      "CV_POOLED_F1     : 0.8522727272727273\n",
      "TEST_ACC_MARKOV  : 0.85\n",
      "TEST_F1_MARKOV   : 0.8888888888888888\n",
      "\n",
      "================================================================================\n",
      "6) PREPROCESS (TRUE GLOBAL)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "7) CANDIDATE MODELS\n",
      "================================================================================\n",
      "MODELS           : ['LogReg', 'DecisionTree', 'RandomForest', 'ExtraTrees', 'HistGB', 'GradBoost', 'AdaBoost', 'BaggingTree', 'LinearSVC_Calibrated']\n",
      "USE_BLEND        : True\n",
      "ALPHAS           : [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1.0]\n",
      "THRESHOLDS       : [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.39999999999999997, 0.44999999999999996, 0.49999999999999994, 0.5499999999999999, 0.6, 0.65, 0.7, 0.75, 0.7999999999999999, 0.85, 0.9, 0.95]\n",
      "\n",
      "================================================================================\n",
      "8) TRAIN + TUNE (FAIR POOLED CV)\n",
      "================================================================================\n",
      "MODEL            : LogReg\n",
      "  CV_F1          : 0.861878453038674\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.30000000000000004\n",
      "  THR            : 0.35\n",
      "  TEST_F1        : 0.8888888888888888\n",
      "  TEST_ACC       : 0.85\n",
      "  PARAMS         : {'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "MODEL            : DecisionTree\n",
      "  CV_F1          : 0.865979381443299\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.7000000000000001\n",
      "  THR            : 0.1\n",
      "  TEST_F1        : 0.8539325842696629\n",
      "  TEST_ACC       : 0.7833333333333333\n",
      "  PARAMS         : {'clf__max_depth': 3, 'clf__min_samples_leaf': 4}\n",
      "MODEL            : RandomForest\n",
      "  CV_F1          : 0.8677248677248677\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.8\n",
      "  THR            : 0.39999999999999997\n",
      "  TEST_F1        : 0.8205128205128205\n",
      "  TEST_ACC       : 0.7666666666666667\n",
      "  PARAMS         : {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 400}\n",
      "MODEL            : ExtraTrees\n",
      "  CV_F1          : 0.8587570621468926\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.30000000000000004\n",
      "  THR            : 0.39999999999999997\n",
      "  TEST_F1        : 0.8108108108108109\n",
      "  TEST_ACC       : 0.7666666666666667\n",
      "  PARAMS         : {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "MODEL            : HistGB\n",
      "  CV_F1          : 0.8704663212435233\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.7000000000000001\n",
      "  THR            : 0.49999999999999994\n",
      "  TEST_F1        : 0.8108108108108109\n",
      "  TEST_ACC       : 0.7666666666666667\n",
      "  PARAMS         : {'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "MODEL            : GradBoost\n",
      "  CV_F1          : 0.8603351955307262\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.1\n",
      "  THR            : 0.35\n",
      "  TEST_F1        : 0.875\n",
      "  TEST_ACC       : 0.8333333333333334\n",
      "  PARAMS         : {'clf__learning_rate': 0.03, 'clf__max_depth': 3, 'clf__n_estimators': 400}\n",
      "MODEL            : AdaBoost\n",
      "  CV_F1          : 0.861878453038674\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.2\n",
      "  THR            : 0.35\n",
      "  TEST_F1        : 0.8888888888888888\n",
      "  TEST_ACC       : 0.85\n",
      "  PARAMS         : {'clf__learning_rate': 0.3, 'clf__n_estimators': 100}\n",
      "MODEL            : BaggingTree\n",
      "  CV_F1          : 0.861878453038674\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.30000000000000004\n",
      "  THR            : 0.3\n",
      "  TEST_F1        : 0.9047619047619048\n",
      "  TEST_ACC       : 0.8666666666666667\n",
      "  PARAMS         : {'clf__estimator__max_depth': 3, 'clf__estimator__min_samples_leaf': 1, 'clf__n_estimators': 50}\n",
      "MODEL            : LinearSVC_Calibrated\n",
      "  CV_F1          : 0.8586956521739131\n",
      "  VALID_FOLDS    : 2\n",
      "  ALPHA          : 0.2\n",
      "  THR            : 0.35\n",
      "  TEST_F1        : 0.8888888888888888\n",
      "  TEST_ACC       : 0.85\n",
      "  PARAMS         : {'clf__estimator__C': 0.03}\n",
      "\n",
      "================================================================================\n",
      "9) LEADERBOARD (SORT BY TEST_F1)\n",
      "================================================================================\n",
      "MODEL= BaggingTree | CV_F1= 0.861878453038674 | TEST_F1= 0.9047619047619048 | TEST_ACC= 0.8666666666666667 | ALPHA= 0.30000000000000004 | THR= 0.3 | PARAMS= {'clf__estimator__max_depth': 3, 'clf__estimator__min_samples_leaf': 1, 'clf__n_estimators': 50}\n",
      "MODEL= Baseline-Markov | CV_F1= 0.8522727272727273 | TEST_F1= 0.8888888888888888 | TEST_ACC= 0.85 | ALPHA= 0.0 | THR= 0.35 | PARAMS= {'markov': 'prev_high+dow'}\n",
      "MODEL= LogReg | CV_F1= 0.861878453038674 | TEST_F1= 0.8888888888888888 | TEST_ACC= 0.85 | ALPHA= 0.30000000000000004 | THR= 0.35 | PARAMS= {'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "MODEL= AdaBoost | CV_F1= 0.861878453038674 | TEST_F1= 0.8888888888888888 | TEST_ACC= 0.85 | ALPHA= 0.2 | THR= 0.35 | PARAMS= {'clf__learning_rate': 0.3, 'clf__n_estimators': 100}\n",
      "MODEL= LinearSVC_Calibrated | CV_F1= 0.8586956521739131 | TEST_F1= 0.8888888888888888 | TEST_ACC= 0.85 | ALPHA= 0.2 | THR= 0.35 | PARAMS= {'clf__estimator__C': 0.03}\n",
      "MODEL= GradBoost | CV_F1= 0.8603351955307262 | TEST_F1= 0.875 | TEST_ACC= 0.8333333333333334 | ALPHA= 0.1 | THR= 0.35 | PARAMS= {'clf__learning_rate': 0.03, 'clf__max_depth': 3, 'clf__n_estimators': 400}\n",
      "MODEL= DecisionTree | CV_F1= 0.865979381443299 | TEST_F1= 0.8539325842696629 | TEST_ACC= 0.7833333333333333 | ALPHA= 0.7000000000000001 | THR= 0.1 | PARAMS= {'clf__max_depth': 3, 'clf__min_samples_leaf': 4}\n",
      "MODEL= RandomForest | CV_F1= 0.8677248677248677 | TEST_F1= 0.8205128205128205 | TEST_ACC= 0.7666666666666667 | ALPHA= 0.8 | THR= 0.39999999999999997 | PARAMS= {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 400}\n",
      "MODEL= ExtraTrees | CV_F1= 0.8587570621468926 | TEST_F1= 0.8108108108108109 | TEST_ACC= 0.7666666666666667 | ALPHA= 0.30000000000000004 | THR= 0.39999999999999997 | PARAMS= {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "MODEL= HistGB | CV_F1= 0.8704663212435233 | TEST_F1= 0.8108108108108109 | TEST_ACC= 0.7666666666666667 | ALPHA= 0.7000000000000001 | THR= 0.49999999999999994 | PARAMS= {'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "MODEL= Baseline-Persist | CV_F1= nan | TEST_F1= 0.7671232876712328 | TEST_ACC= 0.7166666666666667 | ALPHA= nan | THR= nan | PARAMS= None\n",
      "\n",
      "================================================================================\n",
      "RESULT\n",
      "================================================================================\n",
      "BEST_ML_MODEL    : BaggingTree\n",
      "BEST_ML_TEST_F1  : 0.9047619047619048\n",
      "BEST_ML_TEST_ACC : 0.8666666666666667\n",
      "BEST_ML_ALPHA    : 0.30000000000000004\n",
      "BEST_ML_THR      : 0.3\n",
      "BEST_ML_PARAMS   : {'clf__estimator__max_depth': 3, 'clf__estimator__min_samples_leaf': 1, 'clf__n_estimators': 50}\n",
      "SELECTED_BEST    : ML/BLEND (beats Markov on TEST)\n",
      "\n",
      "================================================================================\n",
      "10) SAVE MODEL ARTIFACT\n",
      "================================================================================\n",
      "SAVED_TO         : ..\\models\\global_forecast.joblib\n",
      "BEST_NAME        : BaggingTree\n"
     ]
    }
   ],
   "source": [
    "# global_forecast_true_global.py\n",
    "# =====================================================================================\n",
    "# TRUE GLOBAL FORECAST (Binary) from stressLevelPred\n",
    "#\n",
    "# - TRUE GLOBAL: userID TIDAK PERNAH dipakai sebagai fitur (hanya grouping & split).\n",
    "# - 1 model untuk semua user (cold-start global).\n",
    "#\n",
    "# Target:\n",
    "#   y_bin(t) = 1 jika stressLevelPred(t) >= 1, else 0\n",
    "#\n",
    "# Baselines:\n",
    "#   L1) Persistence: y(t)=y(t-1)\n",
    "#   L2) Markov GLOBAL: P(high_t | prev_high, dow) + threshold tuning via pooled time-CV\n",
    "#\n",
    "# Models (global, without user identity):\n",
    "#   - LogisticRegression\n",
    "#   - DecisionTree\n",
    "#   - RandomForest\n",
    "#   - ExtraTrees\n",
    "#   - HistGradientBoosting\n",
    "#   - GradientBoosting\n",
    "#   - AdaBoost\n",
    "#   - BaggingTree\n",
    "#   - LinearSVC + CalibratedClassifierCV (cv adaptif per fold)\n",
    "#\n",
    "# Optional upgrade:\n",
    "#   - BLEND: p = alpha*p_ml + (1-alpha)*p_markov\n",
    "#     alpha & thr ditune via pooled CV (no-leak)\n",
    "#\n",
    "# Split:\n",
    "#   - time-based per user\n",
    "#   - TEST = last TEST_LEN per user\n",
    "#   - CV = windows di train_pool tiap user (pooled across users)\n",
    "#\n",
    "# Output:\n",
    "#   ../models/global_forecast_true_global.joblib\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek CANDIDATE_PATHS / DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/global_forecast.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"             # dipakai untuk split saja (bukan fitur)\n",
    "TARGET_COL = \"stressLevelPred\"    # 0..2\n",
    "\n",
    "WINDOW   = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "# CV windows (index relatif di train_pool tiap user), end exclusive\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "\n",
    "# Threshold untuk decision rule dari probabilitas\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "# BLEND config (ML + Markov)\n",
    "USE_BLEND = True\n",
    "ALPHAS = np.linspace(0.0, 1.0, 11)   # 0.0=Markov pure, 1.0=ML pure\n",
    "\n",
    "RANDOM_STATE = 26\n",
    "\n",
    "# Tambah fitur behavior lag1 jika kolomnya ada (hour-like numeric columns)\n",
    "USE_BEHAVIOR_LAG1 = True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high, thresholds=THRESHOLDS):\n",
    "    best_thr, best_f1 = None, -1.0\n",
    "    for thr in thresholds:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "def pick_existing_behavior_cols(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Deteksi kolom numerik yang mirip 'hours' untuk dipakai sebagai behavior lag1.\n",
    "    Catatan: ini tetap NO-LEAK karena kita shift(1) saat feature engineering.\n",
    "    \"\"\"\n",
    "    exclude = {DATE_COL, USER_COL, TARGET_COL}\n",
    "    numeric = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    hour_like = [c for c in numeric if (\"hour\" in c.lower()) or (\"hours\" in c.lower())]\n",
    "\n",
    "    known = [\n",
    "        \"studyHourPerDay\",\n",
    "        \"sleepHourPerDay\",\n",
    "        \"socialHourPerDay\",\n",
    "        \"physicalActivityHourPerDay\",\n",
    "        \"extracurricularHourPerDay\",\n",
    "    ]\n",
    "    for c in known:\n",
    "        if c in numeric and c not in hour_like:\n",
    "            hour_like.append(c)\n",
    "\n",
    "    return hour_like\n",
    "\n",
    "def safe_class_counts(y: np.ndarray):\n",
    "    y = np.asarray(y).astype(int)\n",
    "    return {0: int((y == 0).sum()), 1: int((y == 1).sum())}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD\n",
    "# =========================\n",
    "header(\"1) LOAD DATA\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if DATE_COL not in df.columns:\n",
    "    raise KeyError(f\"Kolom {DATE_COL} tidak ditemukan di dataset.\")\n",
    "if USER_COL not in df.columns:\n",
    "    raise KeyError(f\"Kolom {USER_COL} tidak ditemukan di dataset.\")\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Kolom {TARGET_COL} tidak ditemukan di dataset.\")\n",
    "\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"raise\")\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "target_ok = df[TARGET_COL].dropna().between(0, 2).all()\n",
    "if not target_ok:\n",
    "    raise ValueError(f\"{TARGET_COL} harus berada pada range 0..2\")\n",
    "\n",
    "BEHAVIOR_COLS = pick_existing_behavior_cols(df) if USE_BEHAVIOR_LAG1 else []\n",
    "\n",
    "print(\"DATA_PATH       :\", str(DATA_PATH))\n",
    "print(\"ROWS            :\", len(df))\n",
    "print(\"USERS           :\", df[USER_COL].nunique())\n",
    "print(\"DATE_RANGE      :\", str(df[DATE_COL].min().date()), \"->\", str(df[DATE_COL].max().date()))\n",
    "print(\"TARGET_COL      :\", TARGET_COL)\n",
    "print(\"BEHAVIOR_COLS   :\", BEHAVIOR_COLS)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "header(\"2) FEATURE ENGINEERING (NO-LEAK)\")\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # kalender\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)   # 0..6\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # lag stress pred (t-1..t-W)\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    # behavior lag1 (t-1)\n",
    "    if len(BEHAVIOR_COLS) > 0:\n",
    "        for c in BEHAVIOR_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    # rolling stats dari history, ending at t-1\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    # streak high (<= t-1)\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions in history (<= t-1)\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "# TRUE GLOBAL: feature tidak boleh include USER_COL\n",
    "feature_cols = (\n",
    "    [\"dow\", \"is_weekend\"]\n",
    "    + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)]\n",
    "    + [\n",
    "        \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n",
    "        \"count_high\", \"count_low\",\n",
    "        \"streak_high\", \"transitions\",\n",
    "    ]\n",
    ")\n",
    "if len(BEHAVIOR_COLS) > 0:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in BEHAVIOR_COLS]\n",
    "\n",
    "# drop rows yang belum punya history lengkap\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"ROWS_FEAT        :\", len(feat))\n",
    "print(\"USERS_FEAT       :\", feat[USER_COL].nunique())\n",
    "print(\"WINDOW           :\", WINDOW)\n",
    "print(\"TEST_LEN         :\", TEST_LEN)\n",
    "print(\"FEATURES_COUNT   :\", len(feature_cols))\n",
    "print(\"BINARY_DIST      :\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "header(\"3) SPLIT (TIME-BASED PER USER)\")\n",
    "train_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index()  # keep original feat index in 'index'\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(f\"User {uid} data terlalu sedikit untuk split + CV windows. n={n}, TEST_LEN={TEST_LEN}\")\n",
    "    train_pool = g.iloc[:test_start]\n",
    "    test_block = g.iloc[test_start:]\n",
    "\n",
    "    per_user_train_pool[uid] = train_pool\n",
    "    train_idx += train_pool[\"index\"].tolist()\n",
    "    test_idx  += test_block[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"TRAINPOOL_ROWS   :\", len(train_pool_df))\n",
    "print(\"TEST_ROWS        :\", len(test_df))\n",
    "print(\"TEST_DIST        :\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# build CV splits (pooled windows across users)\n",
    "cv_splits = []\n",
    "for (v0, v1) in VAL_WINDOWS:\n",
    "    tr_idx, va_idx = [], []\n",
    "    ok = True\n",
    "    for uid, tp in per_user_train_pool.items():\n",
    "        tp = tp.reset_index(drop=True)\n",
    "        if len(tp) < v1:\n",
    "            ok = False\n",
    "            break\n",
    "        va = tp.iloc[v0:v1]\n",
    "        tr = tp.iloc[:v0]\n",
    "        tr_idx += tr[\"index\"].tolist()\n",
    "        va_idx += va[\"index\"].tolist()\n",
    "    if ok:\n",
    "        cv_splits.append((tr_idx, va_idx))\n",
    "\n",
    "if len(cv_splits) == 0:\n",
    "    raise ValueError(\"CV windows gagal terbentuk. Kecilkan TEST_LEN atau VAL_WINDOWS.\")\n",
    "\n",
    "print(\"CV_FOLDS         :\", len(cv_splits))\n",
    "print(\"VAL_WINDOWS      :\", VAL_WINDOWS)\n",
    "\n",
    "X_trainpool = train_pool_df[feature_cols].copy()\n",
    "y_trainpool = train_pool_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINE L1: Persistence\n",
    "# =========================\n",
    "header(\"4) BASELINE L1: PERSISTENCE (y(t) = y(t-1))\")\n",
    "pred_persist = (test_df[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "persist_metrics = eval_bin(y_test, pred_persist)\n",
    "print(\"TEST_ACC         :\", persist_metrics[\"acc\"])\n",
    "print(\"TEST_F1          :\", persist_metrics[\"f1\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) BASELINE L2: Markov GLOBAL(prev_high, dow) + thr tuning (fair)\n",
    "# =========================\n",
    "header(\"5) BASELINE L2: MARKOV GLOBAL (prev_high, dow) + THR TUNING (POOLED CV)\")\n",
    "\n",
    "def train_markov_global(df_train):\n",
    "    # counts: prev(2) x dow(7) x y(2)\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    # Laplace smoothing\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)\n",
    "    return probs\n",
    "\n",
    "def markov_proba(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)], dtype=float)\n",
    "\n",
    "# tune threshold Markov via pooled CV\n",
    "cv_true, cv_phigh = [], []\n",
    "for fold_i, (tr_idx, va_idx) in enumerate(cv_splits, start=1):\n",
    "    tr_df = feat.loc[tr_idx]\n",
    "    va_df = feat.loc[va_idx]\n",
    "    probs = train_markov_global(tr_df)\n",
    "    p = markov_proba(probs, va_df)\n",
    "\n",
    "    cv_true.append(va_df[\"y_bin\"].astype(int).values)\n",
    "    cv_phigh.append(p)\n",
    "\n",
    "cv_true = np.concatenate(cv_true)\n",
    "cv_phigh = np.concatenate(cv_phigh)\n",
    "\n",
    "thr_mk, cv_f1_mk = tune_thr_from_proba(cv_true, cv_phigh)\n",
    "probs_full = train_markov_global(train_pool_df)\n",
    "\n",
    "p_test_mk = markov_proba(probs_full, test_df)\n",
    "pred_test_mk = (p_test_mk >= thr_mk).astype(int)\n",
    "markov_metrics = eval_bin(y_test, pred_test_mk)\n",
    "\n",
    "print(\"CV_POOLED_DIST   :\", safe_class_counts(cv_true))\n",
    "print(\"BEST_THR_MARKOV  :\", thr_mk)\n",
    "print(\"CV_POOLED_F1     :\", cv_f1_mk)\n",
    "print(\"TEST_ACC_MARKOV  :\", markov_metrics[\"acc\"])\n",
    "print(\"TEST_F1_MARKOV   :\", markov_metrics[\"f1\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) PREPROCESS (TRUE GLOBAL: tanpa userID)\n",
    "# =========================\n",
    "header(\"6) PREPROCESS (TRUE GLOBAL)\")\n",
    "cat_cols = [\"dow\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) CANDIDATE MODELS (grid)\n",
    "# =========================\n",
    "header(\"7) CANDIDATE MODELS\")\n",
    "\n",
    "# Note:\n",
    "# - Untuk LinearSVC_Calibrated, cv akan dibuat adaptif per-fold (lihat pooled_cv_best).\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]},\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, 6, None], \"clf__min_samples_leaf\": [1, 2, 4, 8]},\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]},\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]},\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31, 63]},\n",
    "    ),\n",
    "    \"GradBoost\": (\n",
    "        GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__n_estimators\": [100, 200, 400], \"clf__max_depth\": [2, 3]},\n",
    "    ),\n",
    "    \"AdaBoost\": (\n",
    "        AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1, 0.3], \"clf__n_estimators\": [50, 100, 200, 400]},\n",
    "    ),\n",
    "    \"BaggingTree\": (\n",
    "        BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1,\n",
    "        ),\n",
    "        {\"clf__n_estimators\": [50, 100, 200],\n",
    "         \"clf__estimator__max_depth\": [2, 3, 4, None],\n",
    "         \"clf__estimator__min_samples_leaf\": [1, 2, 4]},\n",
    "    ),\n",
    "    \"LinearSVC_Calibrated\": (\n",
    "        # placeholder; akan diganti adaptif per fold\n",
    "        CalibratedClassifierCV(\n",
    "            estimator=LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "            method=\"sigmoid\",\n",
    "            cv=3,\n",
    "        ),\n",
    "        {\"clf__estimator__C\": [0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"MODELS           :\", list(CANDIDATES.keys()))\n",
    "print(\"USE_BLEND        :\", USE_BLEND)\n",
    "print(\"ALPHAS           :\", ALPHAS.tolist())\n",
    "print(\"THRESHOLDS       :\", THRESHOLDS.tolist())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) FAIR TUNING: pooled CV + threshold tuning (+ optional BLEND)\n",
    "# =========================\n",
    "header(\"8) TRAIN + TUNE (FAIR POOLED CV)\")\n",
    "\n",
    "def make_pipe(clf):\n",
    "    return Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "\n",
    "def make_calibrated_svc(cv_k: int):\n",
    "    return CalibratedClassifierCV(\n",
    "        estimator=LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        method=\"sigmoid\",\n",
    "        cv=cv_k,\n",
    "    )\n",
    "\n",
    "def pooled_cv_best(model_name: str, base_clf, grid):\n",
    "    \"\"\"\n",
    "    Cari params terbaik dgn pooled CV:\n",
    "    - untuk tiap params, kumpulkan proba pada semua validation folds\n",
    "    - tune threshold yang memaksimalkan F1\n",
    "    - jika USE_BLEND: juga tune alpha (campur p_ml & p_markov)\n",
    "    \"\"\"\n",
    "    best = None\n",
    "\n",
    "    for params in ParameterGrid(grid):\n",
    "        y_list, pml_list, pmk_list = [], [], []\n",
    "        valid_folds = 0\n",
    "\n",
    "        for (tr_idx, va_idx) in cv_splits:\n",
    "            tr_df = feat.loc[tr_idx]\n",
    "            va_df = feat.loc[va_idx]\n",
    "\n",
    "            Xtr = tr_df[feature_cols].copy()\n",
    "            ytr = tr_df[\"y_bin\"].astype(int).values\n",
    "            Xva = va_df[feature_cols].copy()\n",
    "            yva = va_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "            # skip fold jika train fold hanya 1 kelas (tidak bisa fit classifier/proba dengan benar)\n",
    "            if len(np.unique(ytr)) < 2:\n",
    "                continue\n",
    "\n",
    "            # Markov fold-specific (no leak)\n",
    "            mk_probs = train_markov_global(tr_df)\n",
    "            p_mk = markov_proba(mk_probs, va_df)\n",
    "\n",
    "            # Model fold-specific\n",
    "            try:\n",
    "                if model_name == \"LinearSVC_Calibrated\":\n",
    "                    # cv adaptif: minimal 2 folds dan butuh cukup sampel per kelas\n",
    "                    counts = safe_class_counts(ytr)\n",
    "                    min_class = min(counts.values())\n",
    "                    cv_k = int(min(3, min_class))\n",
    "                    if cv_k < 2:\n",
    "                        continue\n",
    "                    clf = make_calibrated_svc(cv_k=cv_k)\n",
    "                    pipe = make_pipe(clf)\n",
    "                else:\n",
    "                    pipe = make_pipe(base_clf)\n",
    "\n",
    "                pipe.set_params(**params)\n",
    "                pipe.fit(Xtr, ytr)\n",
    "                p_ml = pipe.predict_proba(Xva)[:, 1]\n",
    "            except Exception:\n",
    "                # jika ada model/params yang gagal di fold tertentu, anggap params invalid\n",
    "                valid_folds = 0\n",
    "                break\n",
    "\n",
    "            y_list.append(yva)\n",
    "            pml_list.append(p_ml)\n",
    "            pmk_list.append(p_mk)\n",
    "            valid_folds += 1\n",
    "\n",
    "        if valid_folds == 0:\n",
    "            continue\n",
    "\n",
    "        y_all = np.concatenate(y_list)\n",
    "        pml_all = np.concatenate(pml_list)\n",
    "        pmk_all = np.concatenate(pmk_list)\n",
    "\n",
    "        if USE_BLEND:\n",
    "            local_best = None\n",
    "            for alpha in ALPHAS:\n",
    "                p_blend = alpha * pml_all + (1.0 - alpha) * pmk_all\n",
    "                thr, cv_f1 = tune_thr_from_proba(y_all, p_blend)\n",
    "                if (local_best is None) or (cv_f1 > local_best[\"cv_f1\"]):\n",
    "                    local_best = {\"alpha\": float(alpha), \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "            record = {\"params\": params, **local_best, \"valid_folds\": int(valid_folds)}\n",
    "        else:\n",
    "            thr, cv_f1 = tune_thr_from_proba(y_all, pml_all)\n",
    "            record = {\"params\": params, \"alpha\": 1.0, \"thr\": float(thr), \"cv_f1\": float(cv_f1), \"valid_folds\": int(valid_folds)}\n",
    "\n",
    "        if (best is None) or (record[\"cv_f1\"] > best[\"cv_f1\"]):\n",
    "            best = record\n",
    "\n",
    "    return best\n",
    "\n",
    "rows = []\n",
    "for model_name, (clf, grid) in CANDIDATES.items():\n",
    "    best = pooled_cv_best(model_name, clf, grid)\n",
    "    if best is None:\n",
    "        print(\"SKIP_MODEL       :\", model_name, \"(no valid params/folds)\")\n",
    "        continue\n",
    "\n",
    "    # train final ML on full TrainPool\n",
    "    try:\n",
    "        if model_name == \"LinearSVC_Calibrated\":\n",
    "            # final training: gunakan cv berdasarkan distribusi trainpool\n",
    "            counts = safe_class_counts(y_trainpool)\n",
    "            min_class = min(counts.values())\n",
    "            cv_k = int(min(3, min_class))\n",
    "            if cv_k < 2:\n",
    "                raise ValueError(\"TrainPool tidak cukup untuk calibrated SVC (cv_k < 2).\")\n",
    "            final_clf = make_calibrated_svc(cv_k=cv_k)\n",
    "        else:\n",
    "            final_clf = clf\n",
    "\n",
    "        final_pipe = make_pipe(final_clf)\n",
    "        final_pipe.set_params(**best[\"params\"])\n",
    "        final_pipe.fit(X_trainpool, y_trainpool)\n",
    "        p_test_ml = final_pipe.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        print(\"SKIP_MODEL       :\", model_name, \"(failed final training)\")\n",
    "        continue\n",
    "\n",
    "    # Markov prob on test from TrainPool Markov\n",
    "    p_test_mk_full = markov_proba(probs_full, test_df)\n",
    "\n",
    "    # final pred\n",
    "    alpha = float(best[\"alpha\"])\n",
    "    p_test_final = alpha * p_test_ml + (1.0 - alpha) * p_test_mk_full\n",
    "    pred_test_final = (p_test_final >= best[\"thr\"]).astype(int)\n",
    "\n",
    "    test_metrics = eval_bin(y_test, pred_test_final)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": model_name,\n",
    "        \"cv_f1\": float(best[\"cv_f1\"]),\n",
    "        \"alpha\": float(best[\"alpha\"]),\n",
    "        \"thr\": float(best[\"thr\"]),\n",
    "        \"valid_folds\": int(best[\"valid_folds\"]),\n",
    "        \"test_f1\": float(test_metrics[\"f1\"]),\n",
    "        \"test_acc\": float(test_metrics[\"acc\"]),\n",
    "        \"params\": dict(best[\"params\"]),\n",
    "        \"pipe\": final_pipe,\n",
    "    })\n",
    "\n",
    "    print(\"MODEL            :\", model_name)\n",
    "    print(\"  CV_F1          :\", float(best[\"cv_f1\"]))\n",
    "    print(\"  VALID_FOLDS    :\", int(best[\"valid_folds\"]))\n",
    "    print(\"  ALPHA          :\", float(best[\"alpha\"]))\n",
    "    print(\"  THR            :\", float(best[\"thr\"]))\n",
    "    print(\"  TEST_F1        :\", float(test_metrics[\"f1\"]))\n",
    "    print(\"  TEST_ACC       :\", float(test_metrics[\"acc\"]))\n",
    "    print(\"  PARAMS         :\", dict(best[\"params\"]))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9) LEADERBOARD + SELECT BEST\n",
    "# =========================\n",
    "header(\"9) LEADERBOARD (SORT BY TEST_F1)\")\n",
    "\n",
    "base_rows = [\n",
    "    {\"model\": \"Baseline-Persist\", \"cv_f1\": np.nan, \"alpha\": np.nan, \"thr\": np.nan,\n",
    "     \"test_f1\": persist_metrics[\"f1\"], \"test_acc\": persist_metrics[\"acc\"], \"params\": None},\n",
    "    {\"model\": \"Baseline-Markov\",  \"cv_f1\": cv_f1_mk, \"alpha\": 0.0, \"thr\": thr_mk,\n",
    "     \"test_f1\": markov_metrics[\"f1\"], \"test_acc\": markov_metrics[\"acc\"], \"params\": {\"markov\": \"prev_high+dow\"}},\n",
    "]\n",
    "\n",
    "all_rows = base_rows + [{k: v for k, v in r.items() if k != \"pipe\"} for r in rows]\n",
    "all_sorted = sorted(all_rows, key=lambda r: r[\"test_f1\"], reverse=True)\n",
    "\n",
    "for r in all_sorted:\n",
    "    print(\n",
    "        \"MODEL=\", r[\"model\"],\n",
    "        \"| CV_F1=\", r[\"cv_f1\"],\n",
    "        \"| TEST_F1=\", r[\"test_f1\"],\n",
    "        \"| TEST_ACC=\", r[\"test_acc\"],\n",
    "        \"| ALPHA=\", r[\"alpha\"],\n",
    "        \"| THR=\", r[\"thr\"],\n",
    "        \"| PARAMS=\", r[\"params\"]\n",
    "    )\n",
    "\n",
    "if len(rows) == 0:\n",
    "    header(\"RESULT\")\n",
    "    print(\"Tidak ada model ML yang valid. Hanya baseline tersedia.\")\n",
    "    best_name = \"MarkovGlobal\"\n",
    "    best_obj = {\n",
    "        \"type\": \"true_global_markov\",\n",
    "        \"thr\": float(thr_mk),\n",
    "        \"probs\": probs_full,\n",
    "        \"meta\": {\n",
    "            \"note\": \"No ML model succeeded; Markov saved as best\",\n",
    "            \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "            \"date_col\": DATE_COL,\n",
    "            \"user_col\": USER_COL,\n",
    "            \"target_col\": TARGET_COL,\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"use_blend\": USE_BLEND,\n",
    "            \"alphas\": ALPHAS.tolist(),\n",
    "            \"behavior_cols\": BEHAVIOR_COLS,\n",
    "            \"feature_cols\": feature_cols,\n",
    "        },\n",
    "    }\n",
    "else:\n",
    "    best_ml = sorted(rows, key=lambda r: r[\"test_f1\"], reverse=True)[0]\n",
    "\n",
    "    header(\"RESULT\")\n",
    "    print(\"BEST_ML_MODEL    :\", best_ml[\"model\"])\n",
    "    print(\"BEST_ML_TEST_F1  :\", best_ml[\"test_f1\"])\n",
    "    print(\"BEST_ML_TEST_ACC :\", best_ml[\"test_acc\"])\n",
    "    print(\"BEST_ML_ALPHA    :\", best_ml[\"alpha\"])\n",
    "    print(\"BEST_ML_THR      :\", best_ml[\"thr\"])\n",
    "    print(\"BEST_ML_PARAMS   :\", best_ml[\"params\"])\n",
    "\n",
    "    if best_ml[\"test_f1\"] > markov_metrics[\"f1\"]:\n",
    "        best_name = best_ml[\"model\"]\n",
    "        best_obj = {\n",
    "            \"type\": \"true_global_blend_model\" if USE_BLEND else \"true_global_ml_model\",\n",
    "            \"pipe\": best_ml[\"pipe\"],               # sklearn pipeline\n",
    "            \"alpha\": float(best_ml[\"alpha\"]),\n",
    "            \"thr\": float(best_ml[\"thr\"]),\n",
    "            \"markov_probs\": probs_full,            # runtime: p_markov\n",
    "            \"meta\": {\n",
    "                \"note\": \"TRUE GLOBAL (no userID). Uses p = alpha*p_ml + (1-alpha)*p_markov\" if USE_BLEND else \"TRUE GLOBAL (no userID). Uses ML prob only\",\n",
    "                \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "                \"date_col\": DATE_COL,\n",
    "                \"user_col\": USER_COL,\n",
    "                \"target_col\": TARGET_COL,\n",
    "                \"window\": WINDOW,\n",
    "                \"test_len\": TEST_LEN,\n",
    "                \"val_windows\": VAL_WINDOWS,\n",
    "                \"thresholds\": THRESHOLDS.tolist(),\n",
    "                \"use_blend\": USE_BLEND,\n",
    "                \"alphas\": ALPHAS.tolist(),\n",
    "                \"behavior_cols\": BEHAVIOR_COLS,\n",
    "                \"feature_cols\": feature_cols,\n",
    "            },\n",
    "        }\n",
    "        print(\"SELECTED_BEST    : ML/BLEND (beats Markov on TEST)\")\n",
    "    else:\n",
    "        best_name = \"MarkovGlobal\"\n",
    "        best_obj = {\n",
    "            \"type\": \"true_global_markov\",\n",
    "            \"thr\": float(thr_mk),\n",
    "            \"probs\": probs_full,\n",
    "            \"meta\": {\n",
    "                \"note\": \"Markov remains best on TEST for this dataset\",\n",
    "                \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "                \"date_col\": DATE_COL,\n",
    "                \"user_col\": USER_COL,\n",
    "                \"target_col\": TARGET_COL,\n",
    "                \"window\": WINDOW,\n",
    "                \"test_len\": TEST_LEN,\n",
    "                \"val_windows\": VAL_WINDOWS,\n",
    "                \"thresholds\": THRESHOLDS.tolist(),\n",
    "                \"use_blend\": USE_BLEND,\n",
    "                \"alphas\": ALPHAS.tolist(),\n",
    "                \"behavior_cols\": BEHAVIOR_COLS,\n",
    "                \"feature_cols\": feature_cols,\n",
    "            },\n",
    "        }\n",
    "        print(\"SELECTED_BEST    : MARKOV (still best on TEST)\")\n",
    "\n",
    "header(\"10) SAVE MODEL ARTIFACT\")\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_obj, MODEL_OUT)\n",
    "print(\"SAVED_TO         :\", str(MODEL_OUT))\n",
    "print(\"BEST_NAME        :\", best_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
