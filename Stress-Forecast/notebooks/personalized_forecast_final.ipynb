{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Semua nama kolom kini snake_case (mis. `user_id`, `stress_level`, `created_at`, `study_hour_per_day`).\n",
    "- Kolom `is_restored` adalah metadata input/restore dan **tidak** dipakai sebagai fitur model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bf8e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1) LOAD + FEATURE ENGINEERING (NO-LEAK)\n",
      "================================================================================\n",
      "DATA_PATH         : ..\\datasets\\global_dataset_pred.csv\n",
      "ROWS_FEAT         : 260\n",
      "USERS             : [1, 2, 3, 4, 5]\n",
      "DATE_RANGE        : 2025-11-24 -> 2026-01-14\n",
      "WINDOW            : 3\n",
      "TEST_LEN          : 12\n",
      "FEATURES_COUNT    : 13\n",
      "USE_USER_ID_FEATURE: False\n",
      "BINARY_DIST       : {1: 146, 0: 114}\n",
      "VAL_WINDOWS       : [(10, 20), (15, 25)]\n",
      "TUNE_THR_PER_USER : True\n",
      "\n",
      "================================================================================\n",
      "2) SPLIT PER USER (TIME-BASED)\n",
      "================================================================================\n",
      "TOTAL_TRAINPOOL   : 200\n",
      "TOTAL_TEST        : 60\n",
      "\n",
      "PER_USER_SPLIT:\n",
      "uid=1 | total=52 | train_pool=40 dist={0: 18, 1: 22} | test=12 dist={0: 3, 1: 9}\n",
      "uid=2 | total=52 | train_pool=40 dist={0: 16, 1: 24} | test=12 dist={0: 6, 1: 6}\n",
      "uid=3 | total=52 | train_pool=40 dist={0: 25, 1: 15} | test=12 dist={0: 7, 1: 5}\n",
      "uid=4 | total=52 | train_pool=40 dist={0: 15, 1: 25} | test=12 dist={0: 2, 1: 10}\n",
      "uid=5 | total=52 | train_pool=40 dist={0: 18, 1: 22} | test=12 dist={0: 4, 1: 8}\n",
      "\n",
      "================================================================================\n",
      "3) BASELINE L1: PERSISTENCE (PER USER)\n",
      "================================================================================\n",
      "TEST_POOLED_ACC   : 0.7166666666666667\n",
      "TEST_POOLED_F1    : 0.7671232876712328\n",
      "TEST_MACRO_ACC    : 0.7166666666666666\n",
      "TEST_MACRO_F1     : 0.7199787347155768\n",
      "\n",
      "PER-USER (Persistence) on TEST:\n",
      "uid=1 | n=12  | dist={0: 3, 1: 9} | acc=0.8333 | f1=0.8889\n",
      "uid=2 | n=12  | dist={0: 6, 1: 6} | acc=0.4167 | f1=0.3636\n",
      "uid=3 | n=12  | dist={0: 7, 1: 5} | acc=0.6667 | f1=0.6000\n",
      "uid=4 | n=12  | dist={0: 2, 1: 10} | acc=0.9167 | f1=0.9474\n",
      "uid=5 | n=12  | dist={0: 4, 1: 8} | acc=0.7500 | f1=0.8000\n",
      "\n",
      "================================================================================\n",
      "4) BASELINE L2: MARKOV PER USER (prev_high, dow) + THR TUNING\n",
      "================================================================================\n",
      "CV_FOLDS_TOTAL    : 10\n",
      "CV_POOLED_DIST    : {0: 15, 1: 85}\n",
      "BEST_THR_MARKOV   : 0.05\n",
      "CV_POOLED_F1      : 0.918918918918919\n",
      "TEST_POOLED_ACC   : 0.6333333333333333\n",
      "TEST_POOLED_F1    : 0.7755102040816326\n",
      "TEST_MACRO_ACC    : 0.6333333333333333\n",
      "TEST_MACRO_F1     : 0.764227145403616\n",
      "\n",
      "PER-USER (Markov) on TEST:\n",
      "uid=1 | n=12  | dist={0: 3, 1: 9} | acc=0.7500 | f1=0.8571 | thr=0.05\n",
      "uid=2 | n=12  | dist={0: 6, 1: 6} | acc=0.5000 | f1=0.6667 | thr=0.05\n",
      "uid=3 | n=12  | dist={0: 7, 1: 5} | acc=0.4167 | f1=0.5882 | thr=0.05\n",
      "uid=4 | n=12  | dist={0: 2, 1: 10} | acc=0.8333 | f1=0.9091 | thr=0.05\n",
      "uid=5 | n=12  | dist={0: 4, 1: 8} | acc=0.6667 | f1=0.8000 | thr=0.05\n",
      "\n",
      "================================================================================\n",
      "5) PREPROCESS (FOR ML)\n",
      "================================================================================\n",
      "CAT_COLS          : ['dow', 'is_weekend']\n",
      "NUM_COLS_COUNT    : 11\n",
      "\n",
      "================================================================================\n",
      "6) CANDIDATE MODELS\n",
      "================================================================================\n",
      "MODELS            : ['LogReg', 'DecisionTree', 'RandomForest', 'ExtraTrees', 'HistGB', 'GradBoost', 'AdaBoost', 'BaggingTree', 'LinearSVC_Calibrated_SAFE']\n",
      "THRESHOLDS_COUNT  : 19\n",
      "\n",
      "================================================================================\n",
      "7) TUNING UTILITIES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "8) PERSONALIZED ML: TRAIN + TUNE (NON-SVM)\n",
      "================================================================================\n",
      "MODEL              : LogReg\n",
      "  CV_SCORE        : 0.9123454790823212\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.7912087912087912\n",
      "  TEST_POOLED_ACC : 0.6833333333333333\n",
      "  TEST_MACRO_F1   : 0.7665800865800867\n",
      "  TEST_MACRO_ACC  : 0.6833333333333333\n",
      "  PARAMS          : {'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "MODEL              : DecisionTree\n",
      "  CV_SCORE        : 0.8986311933680355\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.7951807228915663\n",
      "  TEST_POOLED_ACC : 0.7166666666666667\n",
      "  TEST_MACRO_F1   : 0.7795763919602928\n",
      "  TEST_MACRO_ACC  : 0.7166666666666667\n",
      "  PARAMS          : {'clf__max_depth': 2, 'clf__min_samples_leaf': 8}\n",
      "MODEL              : RandomForest\n",
      "  CV_SCORE        : 0.9298000245368667\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.8\n",
      "  TEST_POOLED_ACC : 0.7333333333333333\n",
      "  TEST_MACRO_F1   : 0.7743815867654875\n",
      "  TEST_MACRO_ACC  : 0.7333333333333333\n",
      "  PARAMS          : {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "MODEL              : ExtraTrees\n",
      "  CV_SCORE        : 0.9241101241101243\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.8571428571428571\n",
      "  TEST_POOLED_ACC : 0.8\n",
      "  TEST_MACRO_F1   : 0.8263423710792133\n",
      "  TEST_MACRO_ACC  : 0.8\n",
      "  PARAMS          : {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__n_estimators': 200}\n",
      "MODEL              : HistGB\n",
      "  CV_SCORE        : 0.8986311933680355\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.7755102040816326\n",
      "  TEST_POOLED_ACC : 0.6333333333333333\n",
      "  TEST_MACRO_F1   : 0.764227145403616\n",
      "  TEST_MACRO_ACC  : 0.6333333333333333\n",
      "  PARAMS          : {'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "MODEL              : GradBoost\n",
      "  CV_SCORE        : 0.8948171948171948\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.8636363636363636\n",
      "  TEST_POOLED_ACC : 0.8\n",
      "  TEST_MACRO_F1   : 0.8505847953216374\n",
      "  TEST_MACRO_ACC  : 0.8\n",
      "  PARAMS          : {'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__n_estimators': 100}\n",
      "MODEL              : AdaBoost\n",
      "  CV_SCORE        : 0.9298000245368667\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.7912087912087912\n",
      "  TEST_POOLED_ACC : 0.6833333333333333\n",
      "  TEST_MACRO_F1   : 0.7665800865800867\n",
      "  TEST_MACRO_ACC  : 0.6833333333333333\n",
      "  PARAMS          : {'clf__learning_rate': 0.03, 'clf__n_estimators': 50}\n",
      "MODEL              : BaggingTree\n",
      "  CV_SCORE        : 0.9298000245368667\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.8674698795180723\n",
      "  TEST_POOLED_ACC : 0.8166666666666667\n",
      "  TEST_MACRO_F1   : 0.8372514619883041\n",
      "  TEST_MACRO_ACC  : 0.8166666666666667\n",
      "  PARAMS          : {'clf__estimator__max_depth': 2, 'clf__estimator__min_samples_leaf': 1, 'clf__n_estimators': 100}\n",
      "\n",
      "================================================================================\n",
      "9) SVM CALIBRATED SAFE (FIXED)\n",
      "================================================================================\n",
      "SVM_FEASIBLE_ALL_USERS: True\n",
      "SVM_FEASIBLE_DETAIL:\n",
      "uid=1 | min_class_count_trainpool=18\n",
      "uid=2 | min_class_count_trainpool=16\n",
      "uid=3 | min_class_count_trainpool=15\n",
      "uid=4 | min_class_count_trainpool=15\n",
      "uid=5 | min_class_count_trainpool=18\n",
      "MODEL              : LinearSVC_Calibrated_SAFE\n",
      "  CV_SCORE        : 0.9234759138649299\n",
      "  THRESHOLD       : per-user\n",
      "  TEST_POOLED_F1  : 0.7674418604651163\n",
      "  TEST_POOLED_ACC : 0.6666666666666666\n",
      "  TEST_MACRO_F1   : 0.6465800865800866\n",
      "  TEST_MACRO_ACC  : 0.6666666666666667\n",
      "  PARAMS          : {'C': 1.0, 'final_cv_by_user': {1: 3, 2: 3, 3: 3, 4: 3, 5: 3}}\n",
      "\n",
      "================================================================================\n",
      "10) LEADERBOARD + SELECT BEST\n",
      "================================================================================\n",
      "BASELINES:\n",
      "  Baseline-Persist | TEST pooled: acc=0.7167, f1=0.7671 | macro(user): acc=0.7167, f1=0.7200\n",
      "  Baseline-Markov  | CV pooled: f1=0.9189, thr=0.05 | TEST pooled: acc=0.6333, f1=0.7755 | macro(user): acc=0.6333, f1=0.7642\n",
      "\n",
      "CANDIDATES:\n",
      "  BaggingTree              | CV=0.9298 | thr=per-user | TEST pooled: acc=0.8167, f1=0.8675 | macro(user): acc=0.8167, f1=0.8373 | params={'clf__estimator__max_depth': 2, 'clf__estimator__min_samples_leaf': 1, 'clf__n_estimators': 100}\n",
      "  GradBoost                | CV=0.8948 | thr=per-user | TEST pooled: acc=0.8000, f1=0.8636 | macro(user): acc=0.8000, f1=0.8506 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__n_estimators': 100}\n",
      "  ExtraTrees               | CV=0.9241 | thr=per-user | TEST pooled: acc=0.8000, f1=0.8571 | macro(user): acc=0.8000, f1=0.8263 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__n_estimators': 200}\n",
      "  RandomForest             | CV=0.9298 | thr=per-user | TEST pooled: acc=0.7333, f1=0.8000 | macro(user): acc=0.7333, f1=0.7744 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "  DecisionTree             | CV=0.8986 | thr=per-user | TEST pooled: acc=0.7167, f1=0.7952 | macro(user): acc=0.7167, f1=0.7796 | params={'clf__max_depth': 2, 'clf__min_samples_leaf': 8}\n",
      "  LogReg                   | CV=0.9123 | thr=per-user | TEST pooled: acc=0.6833, f1=0.7912 | macro(user): acc=0.6833, f1=0.7666 | params={'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "  AdaBoost                 | CV=0.9298 | thr=per-user | TEST pooled: acc=0.6833, f1=0.7912 | macro(user): acc=0.6833, f1=0.7666 | params={'clf__learning_rate': 0.03, 'clf__n_estimators': 50}\n",
      "  HistGB                   | CV=0.8986 | thr=per-user | TEST pooled: acc=0.6333, f1=0.7755 | macro(user): acc=0.6333, f1=0.7642 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "  LinearSVC_Calibrated_SAFE | CV=0.9235 | thr=per-user | TEST pooled: acc=0.6667, f1=0.7674 | macro(user): acc=0.6667, f1=0.6466 | params={'C': 1.0, 'calibration_cv': 'adaptive<=3 (per user), {1: 3, 2: 3, 3: 3, 4: 3, 5: 3}'}\n",
      "\n",
      "SELECTED_BEST      : BaggingTree\n",
      "\n",
      "PER-USER (SELECTED_BEST=BaggingTree) on TEST:\n",
      "uid=1 | n=12  | dist={0: 3, 1: 9} | acc=0.9167 | f1=0.9474 | thr=0.05\n",
      "uid=2 | n=12  | dist={0: 6, 1: 6} | acc=0.6667 | f1=0.7500 | thr=0.05\n",
      "uid=3 | n=12  | dist={0: 7, 1: 5} | acc=0.6667 | f1=0.6000 | thr=0.50\n",
      "uid=4 | n=12  | dist={0: 2, 1: 10} | acc=1.0000 | f1=1.0000 | thr=0.05\n",
      "uid=5 | n=12  | dist={0: 4, 1: 8} | acc=0.8333 | f1=0.8889 | thr=0.05\n",
      "\n",
      "================================================================================\n",
      "11) SAVE ARTIFACT\n",
      "================================================================================\n",
      "SAVED_TO          : ..\\models\\personalized_forecast.joblib\n",
      "BEST_NAME         : BaggingTree\n"
     ]
    }
   ],
   "source": "# =====================================================================================\n# PERSONALIZED_FORECAST (TRUE Personalized, per-user model) - 1 CELL (SVM Calibration FIXED)\n#\n# Tujuan:\n# - TRUE PERSONALIZED: 1 model per user (latih terpisah per user).\n# - user_id TIDAK dipakai sebagai fitur (default), hanya untuk grouping & split.\n# - Target binary: y=1 jika stress_level_pred>=1, else 0.\n#\n# Baselines:\n# - L1 Persistence: y(t)=y(t-1)\n# - L2 Markov per-user: P(high_t | prev_high, dow) + threshold tuning via pooled per-user CV\n#\n# Model candidates (per-user):\n# - LogisticRegression\n# - DecisionTree\n# - RandomForest\n# - ExtraTrees\n# - HistGradientBoosting\n# - GradientBoosting\n# - AdaBoost\n# - BaggingTree\n# - LinearSVC + CalibratedClassifierCV (SAFE: cv adaptif per-fold, skip fold jika cv_k<2,\n#   dan skip kandidat SVM seluruhnya jika ada user tidak feasible / tidak ada fold valid)\n#\n# Split:\n# - per user time-based\n# - TEST = last TEST_LEN\n# - CV folds = windows di train_pool masing-masing user\n#\n# Output:\n# - ../models/personalized_forecast.joblib\n# =====================================================================================\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport joblib\n\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import ParameterGrid\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier, ExtraTreesClassifier,\n    HistGradientBoostingClassifier,\n    GradientBoostingClassifier, AdaBoostClassifier,\n    BaggingClassifier\n)\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\n\n\n# =========================\n# 0) CONFIG\n# =========================\nCANDIDATE_PATHS = [\n    Path(\"../datasets/global_dataset_pred.csv\"),\n]\nDATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\nif DATA_PATH is None:\n    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek CANDIDATE_PATHS / DATA_PATH.\")\n\nMODEL_OUT = Path(\"../models/personalized_forecast.joblib\")\n\nDATE_COL   = \"date\"\nUSER_COL   = \"user_id\"\nTARGET_COL = \"stress_level_pred\"  # 0..2\n\nWINDOW   = 3\nTEST_LEN = 12\n\nVAL_WINDOWS = [(10, 20), (15, 25)]\nTHRESHOLDS  = np.linspace(0.05, 0.95, 19)\n\nRANDOM_STATE = 26\n\n# TRUE personalized: default = False (no semi/global)\nUSE_USER_ID_FEATURE = False\n\n# threshold tuning:\n# - True  => tune threshold terpisah per user (lebih fleksibel, tapi harus fair: pakai CV user tsb)\n# - False => tune 1 threshold global (pooled across users) untuk semua user\nTUNE_THRESHOLD_PER_USER = True\n\n# Print detail per-user untuk baselines & model terpilih\nPRINT_PER_USER_DETAILS = True\n\n\n# =========================\n# Print helpers (rapih, no styling berlebihan)\n# =========================\ndef section(title: str):\n    print(\"\\n\" + \"=\" * 80)\n    print(title)\n    print(\"=\" * 80)\n\ndef kv(k, v):\n    print(f\"{k:<18}: {v}\")\n\ndef safe_class_counts(y):\n    y = np.asarray(y).astype(int)\n    return {0: int((y == 0).sum()), 1: int((y == 1).sum())}\n\ndef print_per_user_breakdown(title, per_user_records, thr_info=None):\n    print(f\"\\n{title}\")\n    for r in per_user_records:\n        uid = r[\"uid\"]\n        y = np.asarray(r[\"y\"]).astype(int)\n        pred = np.asarray(r[\"pred\"]).astype(int)\n        acc = float(accuracy_score(y, pred))\n        f1  = float(f1_score(y, pred, zero_division=0))\n        dist = safe_class_counts(y)\n\n        extra = \"\"\n        if thr_info is not None:\n            if isinstance(thr_info, dict) and uid in thr_info:\n                extra = f\" | thr={float(thr_info[uid]):.2f}\"\n            elif isinstance(thr_info, (float, int)):\n                extra = f\" | thr={float(thr_info):.2f}\"\n\n        print(f\"uid={uid} | n={len(y):<3} | dist={dist} | acc={acc:.4f} | f1={f1:.4f}{extra}\")\n\n\n# =========================\n# Helpers\n# =========================\ndef eval_bin(y_true, y_pred):\n    return {\n        \"acc\": float(accuracy_score(y_true, y_pred)),\n        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n    }\n\ndef tune_thr_from_proba(y_true, p_high, thresholds=THRESHOLDS):\n    best_thr, best_f1 = None, -1.0\n    for thr in thresholds:\n        pred = (p_high >= thr).astype(int)\n        f1 = float(f1_score(y_true, pred, zero_division=0))\n        if f1 > best_f1:\n            best_f1, best_thr = f1, thr\n    return float(best_thr), float(best_f1)\n\ndef per_user_macro_metrics(per_user_records):\n    accs, f1s = [], []\n    for r in per_user_records:\n        accs.append(accuracy_score(r[\"y\"], r[\"pred\"]))\n        f1s.append(f1_score(r[\"y\"], r[\"pred\"], zero_division=0))\n    return float(np.mean(accs)), float(np.mean(f1s))\n\ndef cv_folds_user(tp_df):\n    folds = []\n    for (v0, v1) in VAL_WINDOWS:\n        if len(tp_df) < v1:\n            continue\n        tr = tp_df.iloc[:v0].copy()\n        va = tp_df.iloc[v0:v1].copy()\n        folds.append((tr, va))\n    return folds\n\ndef min_class_count(y):\n    vc = pd.Series(np.asarray(y)).value_counts()\n    if len(vc) < 2:\n        return 0\n    return int(vc.min())\n\n\n# =========================\n# 1) LOAD + FEATURE ENGINEERING (no leak)\n# =========================\nsection(\"1) LOAD + FEATURE ENGINEERING (NO-LEAK)\")\n\ndf = pd.read_csv(DATA_PATH)\nif \"is_restored\" not in df.columns:\n    df[\"is_restored\"] = 0\ndf[\"is_restored\"] = df[\"is_restored\"].fillna(0).astype(int)\n\nif DATE_COL not in df.columns:\n    raise KeyError(f\"Kolom {DATE_COL} tidak ditemukan.\")\nif USER_COL not in df.columns:\n    raise KeyError(f\"Kolom {USER_COL} tidak ditemukan.\")\nif TARGET_COL not in df.columns:\n    raise KeyError(f\"Kolom {TARGET_COL} tidak ditemukan.\")\n\ndf[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"raise\")\ndf = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n\nif not df[TARGET_COL].dropna().between(0, 2).all():\n    raise ValueError(f\"{TARGET_COL} harus berada pada range 0..2\")\n\nrows = []\nfor uid, g in df.groupby(USER_COL):\n    g = g.sort_values(DATE_COL).reset_index(drop=True)\n\n    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n\n    for k in range(1, WINDOW + 1):\n        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n\n    sp_shift = g[TARGET_COL].shift(1)\n\n    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n\n    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n\n    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n    streak, cur = [], 0\n    for v in high:\n        cur = cur + 1 if v == 1 else 0\n        streak.append(cur)\n    g[\"streak_high\"] = streak\n\n    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n\n    rows.append(g)\n\nfeat = pd.concat(rows, ignore_index=True)\nfeat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n\nfeature_cols = (\n    [\"dow\", \"is_weekend\"]\n    + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)]\n    + [\n        \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n        \"count_high\", \"count_low\",\n        \"streak_high\", \"transitions\",\n    ]\n)\nif USE_USER_ID_FEATURE:\n    feature_cols = [USER_COL] + feature_cols\n\nfeat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\nusers = sorted(feat[USER_COL].unique().tolist())\n\nkv(\"DATA_PATH\", str(DATA_PATH))\nkv(\"ROWS_FEAT\", len(feat))\nkv(\"USERS\", users)\nkv(\"DATE_RANGE\", f\"{feat[DATE_COL].min().date()} -> {feat[DATE_COL].max().date()}\")\nkv(\"WINDOW\", WINDOW)\nkv(\"TEST_LEN\", TEST_LEN)\nkv(\"FEATURES_COUNT\", len(feature_cols))\nkv(\"USE_USER_ID_FEATURE\", USE_USER_ID_FEATURE)\nkv(\"BINARY_DIST\", feat[\"y_bin\"].value_counts().to_dict())\nkv(\"VAL_WINDOWS\", VAL_WINDOWS)\nkv(\"TUNE_THR_PER_USER\", TUNE_THRESHOLD_PER_USER)\n\n\n# =========================\n# 2) SPLIT per user (time-based)\n# =========================\nsection(\"2) SPLIT PER USER (TIME-BASED)\")\n\nper_user = {}\nsplit_rows = []\nfor uid in users:\n    g = feat[feat[USER_COL] == uid].sort_values(DATE_COL).reset_index(drop=True)\n    n = len(g)\n    test_start = n - TEST_LEN\n    if test_start <= 10:\n        raise ValueError(f\"User {uid}: data terlalu sedikit untuk split (n={n}, TEST_LEN={TEST_LEN}).\")\n\n    tp = g.iloc[:test_start].copy()\n    te = g.iloc[test_start:].copy()\n\n    per_user[uid] = {\"train_pool\": tp, \"test\": te}\n\n    split_rows.append({\n        \"uid\": uid,\n        \"n_total\": n,\n        \"n_train_pool\": len(tp),\n        \"n_test\": len(te),\n        \"train_pool_dist\": safe_class_counts(tp[\"y_bin\"].values),\n        \"test_dist\": safe_class_counts(te[\"y_bin\"].values),\n    })\n\nkv(\"TOTAL_TRAINPOOL\", sum(r[\"n_train_pool\"] for r in split_rows))\nkv(\"TOTAL_TEST\", sum(r[\"n_test\"] for r in split_rows))\nprint(\"\\nPER_USER_SPLIT:\")\nfor r in split_rows:\n    print(\n        f\"uid={r['uid']} | total={r['n_total']} | train_pool={r['n_train_pool']} dist={r['train_pool_dist']} \"\n        f\"| test={r['n_test']} dist={r['test_dist']}\"\n    )\n\n\n# =========================\n# 3) BASELINE L1: Persistence\n# =========================\nsection(\"3) BASELINE L1: PERSISTENCE (PER USER)\")\n\npersist_user_records = []\nall_true, all_pred = [], []\n\nfor uid in users:\n    te = per_user[uid][\"test\"]\n    y = te[\"y_bin\"].astype(int).values\n    pred = (te[\"lag_sp_1\"] >= 1).astype(int).values\n\n    persist_user_records.append({\"uid\": uid, \"y\": y, \"pred\": pred})\n    all_true.append(y)\n    all_pred.append(pred)\n\ny_all = np.concatenate(all_true)\npred_all = np.concatenate(all_pred)\n\npersist_pooled = eval_bin(y_all, pred_all)\npersist_macro_acc, persist_macro_f1 = per_user_macro_metrics(persist_user_records)\n\nkv(\"TEST_POOLED_ACC\", persist_pooled[\"acc\"])\nkv(\"TEST_POOLED_F1\", persist_pooled[\"f1\"])\nkv(\"TEST_MACRO_ACC\", persist_macro_acc)\nkv(\"TEST_MACRO_F1\", persist_macro_f1)\n\nif PRINT_PER_USER_DETAILS:\n    print_per_user_breakdown(\"PER-USER (Persistence) on TEST:\", persist_user_records)\n\n\n# =========================\n# 4) BASELINE L2: Markov USER(prev_high, dow)\n# =========================\nsection(\"4) BASELINE L2: MARKOV PER USER (prev_high, dow) + THR TUNING\")\n\ndef train_markov_one_user(df_train):\n    counts = np.zeros((2, 7, 2), dtype=int)  # prev(2) x dow(7) x y(2)\n    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n    dow  = (df_train[\"dow\"]).astype(int).values\n    yb   = (df_train[\"y_bin\"]).astype(int).values\n    for p, d, y in zip(prev, dow, yb):\n        counts[p, d, y] += 1\n    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace smoothing\n    return probs\n\ndef markov_proba_user(probs, df_eval):\n    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n    dow  = (df_eval[\"dow\"]).astype(int).values\n    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)], dtype=float)\n\n# pooled CV untuk threshold (konsisten & fair)\ncv_true, cv_phigh = [], []\ncv_fold_stats = []\n\nfor uid in users:\n    tp = per_user[uid][\"train_pool\"]\n    folds = cv_folds_user(tp)\n    for (tr_df, va_df) in folds:\n        probs = train_markov_one_user(tr_df)\n        p = markov_proba_user(probs, va_df)\n        cv_true.append(va_df[\"y_bin\"].astype(int).values)\n        cv_phigh.append(p)\n        cv_fold_stats.append({\"uid\": uid, \"tr_len\": len(tr_df), \"va_len\": len(va_df)})\n\nif len(cv_true) == 0:\n    raise ValueError(\"Tidak ada CV fold yang valid. Kurangi VAL_WINDOWS / TEST_LEN / WINDOW.\")\n\ncv_true = np.concatenate(cv_true)\ncv_phigh = np.concatenate(cv_phigh)\n\nthr_mk, cv_f1_mk = tune_thr_from_proba(cv_true, cv_phigh)\n\nmk_models = {}\nmarkov_user_records = []\nall_true, all_pred = [], []\n\nfor uid in users:\n    tp = per_user[uid][\"train_pool\"]\n    te = per_user[uid][\"test\"]\n\n    probs = train_markov_one_user(tp)\n    mk_models[uid] = probs\n\n    p = markov_proba_user(probs, te)\n    pred = (p >= thr_mk).astype(int)\n    y = te[\"y_bin\"].astype(int).values\n\n    markov_user_records.append({\"uid\": uid, \"y\": y, \"pred\": pred})\n    all_true.append(y)\n    all_pred.append(pred)\n\ny_all = np.concatenate(all_true)\npred_all = np.concatenate(all_pred)\n\nmarkov_pooled = eval_bin(y_all, pred_all)\nmarkov_macro_acc, markov_macro_f1 = per_user_macro_metrics(markov_user_records)\n\nkv(\"CV_FOLDS_TOTAL\", len(cv_fold_stats))\nkv(\"CV_POOLED_DIST\", safe_class_counts(cv_true))\nkv(\"BEST_THR_MARKOV\", thr_mk)\nkv(\"CV_POOLED_F1\", cv_f1_mk)\nkv(\"TEST_POOLED_ACC\", markov_pooled[\"acc\"])\nkv(\"TEST_POOLED_F1\", markov_pooled[\"f1\"])\nkv(\"TEST_MACRO_ACC\", markov_macro_acc)\nkv(\"TEST_MACRO_F1\", markov_macro_f1)\n\nif PRINT_PER_USER_DETAILS:\n    print_per_user_breakdown(\"PER-USER (Markov) on TEST:\", markov_user_records, thr_info=thr_mk)\n\n\n# =========================\n# 5) Preprocess (for ML)\n# =========================\nsection(\"5) PREPROCESS (FOR ML)\")\n\ncat_cols = [\"dow\", \"is_weekend\"]\nif USE_USER_ID_FEATURE:\n    cat_cols = [USER_COL] + cat_cols\n\nnum_cols = [c for c in feature_cols if c not in cat_cols]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n    ],\n    remainder=\"drop\",\n)\n\nkv(\"CAT_COLS\", cat_cols)\nkv(\"NUM_COLS_COUNT\", len(num_cols))\n\n\n# =========================\n# 6) Candidate models (non-SVM) + SVM SAFE config\n# =========================\nsection(\"6) CANDIDATE MODELS\")\n\n# Bagging compatibility (estimator vs base_estimator)\ntry:\n    bag_base = BaggingClassifier(\n        estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n        random_state=RANDOM_STATE,\n        n_jobs=1\n    )\n    BAG_ESTIMATOR_PARAM = \"clf__estimator__\"\nexcept TypeError:\n    bag_base = BaggingClassifier(\n        base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n        random_state=RANDOM_STATE,\n        n_jobs=1\n    )\n    BAG_ESTIMATOR_PARAM = \"clf__base_estimator__\"\n\nCANDIDATES = {\n    \"LogReg\": (\n        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n    ),\n    \"DecisionTree\": (\n        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n        {\"clf__max_depth\": [2, 3, 4, 6, None], \"clf__min_samples_leaf\": [1, 2, 4, 8]}\n    ),\n    \"RandomForest\": (\n        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]}\n    ),\n    \"ExtraTrees\": (\n        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]}\n    ),\n    \"HistGB\": (\n        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3],\n         \"clf__max_leaf_nodes\": [15, 31, 63]}\n    ),\n    \"GradBoost\": (\n        GradientBoostingClassifier(random_state=RANDOM_STATE),\n        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__n_estimators\": [100, 200, 400],\n         \"clf__max_depth\": [2, 3]}\n    ),\n    \"AdaBoost\": (\n        AdaBoostClassifier(random_state=RANDOM_STATE),\n        {\"clf__learning_rate\": [0.03, 0.05, 0.1, 0.3], \"clf__n_estimators\": [50, 100, 200, 400]}\n    ),\n    \"BaggingTree\": (\n        bag_base,\n        {\"clf__n_estimators\": [50, 100, 200],\n         f\"{BAG_ESTIMATOR_PARAM}max_depth\": [2, 3, 4, None],\n         f\"{BAG_ESTIMATOR_PARAM}min_samples_leaf\": [1, 2, 4]}\n    ),\n}\n\nSVM_NAME = \"LinearSVC_Calibrated_SAFE\"\nSVM_GRID = {\"C\": [0.03, 0.1, 0.3, 1.0, 3.0]}\n\nkv(\"MODELS\", list(CANDIDATES.keys()) + [SVM_NAME])\nkv(\"THRESHOLDS_COUNT\", len(THRESHOLDS))\n\n\n# =========================\n# 7) Tuning utilities (threshold global or per-user)\n# =========================\nsection(\"7) TUNING UTILITIES\")\n\ndef tune_global_thr_pooled_over_all_users(pipe, params):\n    y_list, p_list = [], []\n    for uid in users:\n        tp = per_user[uid][\"train_pool\"]\n        folds = cv_folds_user(tp)\n        for tr_df, va_df in folds:\n            ytr = tr_df[\"y_bin\"].astype(int).values\n            if len(np.unique(ytr)) < 2:\n                continue\n            pipe.set_params(**params)\n            pipe.fit(tr_df[feature_cols], ytr)\n            p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n            y_list.append(va_df[\"y_bin\"].astype(int).values)\n            p_list.append(p)\n\n    if len(y_list) == 0:\n        return None, None\n\n    y_all = np.concatenate(y_list)\n    p_all = np.concatenate(p_list)\n    thr, cv_f1 = tune_thr_from_proba(y_all, p_all)\n    return float(thr), float(cv_f1)\n\ndef tune_per_user_thr(pipe, params):\n    thr_by_user = {}\n    f1s = []\n    for uid in users:\n        tp = per_user[uid][\"train_pool\"]\n        folds = cv_folds_user(tp)\n        if len(folds) == 0:\n            return None, None\n\n        y_list, p_list = [], []\n        for tr_df, va_df in folds:\n            ytr = tr_df[\"y_bin\"].astype(int).values\n            if len(np.unique(ytr)) < 2:\n                continue\n            pipe.set_params(**params)\n            pipe.fit(tr_df[feature_cols], ytr)\n            p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n            y_list.append(va_df[\"y_bin\"].astype(int).values)\n            p_list.append(p)\n\n        if len(y_list) == 0:\n            return None, None\n\n        y_u = np.concatenate(y_list)\n        p_u = np.concatenate(p_list)\n        thr_u, f1_u = tune_thr_from_proba(y_u, p_u)\n        thr_by_user[uid] = float(thr_u)\n        f1s.append(float(f1_u))\n\n    return thr_by_user, float(np.mean(f1s))\n\ndef eval_personalized_models(models_by_user, thr_by_user_or_scalar):\n    per_user_records = []\n    all_true, all_pred = [], []\n\n    for uid in users:\n        te = per_user[uid][\"test\"]\n        y = te[\"y_bin\"].astype(int).values\n\n        pipe = models_by_user[uid]\n        p = pipe.predict_proba(te[feature_cols])[:, 1]\n        thr = thr_by_user_or_scalar[uid] if isinstance(thr_by_user_or_scalar, dict) else float(thr_by_user_or_scalar)\n        pred = (p >= thr).astype(int)\n\n        per_user_records.append({\"uid\": uid, \"y\": y, \"pred\": pred})\n        all_true.append(y)\n        all_pred.append(pred)\n\n    y_all = np.concatenate(all_true)\n    pred_all = np.concatenate(all_pred)\n\n    pooled = eval_bin(y_all, pred_all)\n    macro_acc, macro_f1 = per_user_macro_metrics(per_user_records)\n    macro = {\"acc\": float(macro_acc), \"f1\": float(macro_f1)}\n    return pooled, macro, per_user_records\n\n\n# =========================\n# 8) TRAIN + TUNE all non-SVM candidates\n# =========================\nsection(\"8) PERSONALIZED ML: TRAIN + TUNE (NON-SVM)\")\n\nrows = []\n\nfor name, (clf, grid) in CANDIDATES.items():\n    best = None\n\n    for params in ParameterGrid(grid):\n        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n\n        if TUNE_THRESHOLD_PER_USER:\n            thr_obj, cv_score = tune_per_user_thr(pipe, params)\n        else:\n            thr_obj, cv_score = tune_global_thr_pooled_over_all_users(pipe, params)\n\n        if thr_obj is None:\n            continue\n\n        if (best is None) or (cv_score > best[\"cv_score\"]):\n            best = {\"params\": dict(params), \"thr_obj\": thr_obj, \"cv_score\": float(cv_score)}\n\n    if best is None:\n        print(f\"SKIP_MODEL         : {name} (no valid params/folds)\")\n        continue\n\n    # train final per-user\n    models_by_user = {}\n    ok = True\n    for uid in users:\n        tp = per_user[uid][\"train_pool\"]\n        ytr = tp[\"y_bin\"].astype(int).values\n        if len(np.unique(ytr)) < 2:\n            ok = False\n            break\n\n        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n        pipe.set_params(**best[\"params\"])\n        pipe.fit(tp[feature_cols], ytr)\n        models_by_user[uid] = pipe\n\n    if not ok:\n        print(f\"SKIP_MODEL         : {name} (some user train_pool has single class)\")\n        continue\n\n    pooled, macro, user_records = eval_personalized_models(models_by_user, best[\"thr_obj\"])\n\n    rows.append({\n        \"model\": name,\n        \"cv_score\": float(best[\"cv_score\"]),\n        \"thr_obj\": best[\"thr_obj\"],\n        \"test_pooled_f1\": float(pooled[\"f1\"]),\n        \"test_pooled_acc\": float(pooled[\"acc\"]),\n        \"test_macro_f1\": float(macro[\"f1\"]),\n        \"test_macro_acc\": float(macro[\"acc\"]),\n        \"params\": dict(best[\"params\"]),\n        \"models_by_user\": models_by_user,\n        \"test_user_records\": user_records,\n    })\n\n    thr_desc = \"per-user\" if isinstance(best[\"thr_obj\"], dict) else f\"{best['thr_obj']:.2f}\"\n    print(f\"MODEL              : {name}\")\n    kv(\"  CV_SCORE\", best[\"cv_score\"])\n    kv(\"  THRESHOLD\", thr_desc)\n    kv(\"  TEST_POOLED_F1\", pooled[\"f1\"])\n    kv(\"  TEST_POOLED_ACC\", pooled[\"acc\"])\n    kv(\"  TEST_MACRO_F1\", macro[\"f1\"])\n    kv(\"  TEST_MACRO_ACC\", macro[\"acc\"])\n    kv(\"  PARAMS\", best[\"params\"])\n\n\n# =========================\n# 9) SVM Calibrated SAFE (FIXED)\n# =========================\nsection(\"9) SVM CALIBRATED SAFE (FIXED)\")\n\ndef make_calibrator(base, cv_k):\n    try:\n        return CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=cv_k)\n    except TypeError:\n        return CalibratedClassifierCV(base_estimator=base, method=\"sigmoid\", cv=cv_k)\n\ndef svm_fit_predict_proba(tr_X, tr_y, va_X, C, cv_max=3):\n    mcc = min_class_count(tr_y)\n    cv_k = int(min(cv_max, mcc))\n    if cv_k < 2:\n        return None, cv_k\n    base = LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE, C=float(C))\n    calib = make_calibrator(base, cv_k=cv_k)\n    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", calib)])\n    pipe.fit(tr_X, tr_y)\n    return pipe.predict_proba(va_X)[:, 1], cv_k\n\n# 1) Check feasibility for all users in final training\nsvm_feasible_all_users = True\nsvm_feasible_detail = []\nfor uid in users:\n    y_tp = per_user[uid][\"train_pool\"][\"y_bin\"].astype(int).values\n    mcc = min_class_count(y_tp)\n    svm_feasible_detail.append({\"uid\": uid, \"min_class_count_trainpool\": mcc})\n    if mcc < 2:\n        svm_feasible_all_users = False\n\nkv(\"SVM_FEASIBLE_ALL_USERS\", svm_feasible_all_users)\nprint(\"SVM_FEASIBLE_DETAIL:\")\nfor r in svm_feasible_detail:\n    print(f\"uid={r['uid']} | min_class_count_trainpool={r['min_class_count_trainpool']}\")\n\nif svm_feasible_all_users:\n    best = None\n\n    for C in SVM_GRID[\"C\"]:\n        if TUNE_THRESHOLD_PER_USER:\n            thr_by_user = {}\n            per_user_cv_scores = []\n            all_users_ok = True\n            users_valid = 0\n\n            for uid in users:\n                tp = per_user[uid][\"train_pool\"]\n                folds = cv_folds_user(tp)\n\n                y_list_u, p_list_u = [], []\n\n                for (tr_df, va_df) in folds:\n                    tr_y = tr_df[\"y_bin\"].astype(int).values\n                    p, cv_k = svm_fit_predict_proba(tr_df[feature_cols], tr_y, va_df[feature_cols], C=C, cv_max=3)\n                    if p is None:\n                        continue\n                    y_list_u.append(va_df[\"y_bin\"].astype(int).values)\n                    p_list_u.append(p)\n\n                if len(y_list_u) == 0:\n                    all_users_ok = False\n                    break\n\n                y_u = np.concatenate(y_list_u)\n                p_u = np.concatenate(p_list_u)\n                thr_u, f1_u = tune_thr_from_proba(y_u, p_u)\n                thr_by_user[uid] = float(thr_u)\n                per_user_cv_scores.append(float(f1_u))\n                users_valid += 1\n\n            if (not all_users_ok) or (users_valid < len(users)):\n                continue\n\n            cv_score = float(np.mean(per_user_cv_scores))\n            thr_obj = thr_by_user\n\n        else:\n            y_list, p_list = [], []\n            for uid in users:\n                tp = per_user[uid][\"train_pool\"]\n                folds = cv_folds_user(tp)\n                for (tr_df, va_df) in folds:\n                    tr_y = tr_df[\"y_bin\"].astype(int).values\n                    p, cv_k = svm_fit_predict_proba(tr_df[feature_cols], tr_y, va_df[feature_cols], C=C, cv_max=3)\n                    if p is None:\n                        continue\n                    y_list.append(va_df[\"y_bin\"].astype(int).values)\n                    p_list.append(p)\n\n            if len(y_list) == 0:\n                continue\n\n            y_all = np.concatenate(y_list)\n            p_all = np.concatenate(p_list)\n            thr_obj, cv_score = tune_thr_from_proba(y_all, p_all)\n\n        if (best is None) or (cv_score > best[\"cv_score\"]):\n            best = {\"C\": float(C), \"thr_obj\": thr_obj, \"cv_score\": float(cv_score)}\n\n    if best is None:\n        print(f\"SKIP_MODEL         : {SVM_NAME} (no valid C setting across all users/folds)\")\n    else:\n        # 2) Final training per user (cv adaptif dari train_pool)\n        models_by_user = {}\n        ok = True\n        final_cv_by_user = {}\n\n        for uid in users:\n            tp = per_user[uid][\"train_pool\"]\n            tr_y = tp[\"y_bin\"].astype(int).values\n            mcc = min_class_count(tr_y)\n            cv_k = int(min(3, mcc))\n            if cv_k < 2:\n                ok = False\n                break\n\n            base = LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE, C=float(best[\"C\"]))\n            calib = make_calibrator(base, cv_k=cv_k)\n            pipe = Pipeline([(\"prep\", preprocess), (\"clf\", calib)])\n            pipe.fit(tp[feature_cols], tr_y)\n\n            models_by_user[uid] = pipe\n            final_cv_by_user[uid] = cv_k\n\n        if not ok:\n            print(f\"SKIP_MODEL         : {SVM_NAME} (final training not feasible for all users)\")\n        else:\n            pooled, macro, user_records = eval_personalized_models(models_by_user, best[\"thr_obj\"])\n            rows.append({\n                \"model\": SVM_NAME,\n                \"cv_score\": float(best[\"cv_score\"]),\n                \"thr_obj\": best[\"thr_obj\"],\n                \"test_pooled_f1\": float(pooled[\"f1\"]),\n                \"test_pooled_acc\": float(pooled[\"acc\"]),\n                \"test_macro_f1\": float(macro[\"f1\"]),\n                \"test_macro_acc\": float(macro[\"acc\"]),\n                \"params\": {\"C\": float(best[\"C\"]), \"calibration_cv\": f\"adaptive<=3 (per user), {final_cv_by_user}\"},\n                \"models_by_user\": models_by_user,\n                \"test_user_records\": user_records,\n            })\n\n            thr_desc = \"per-user\" if isinstance(best[\"thr_obj\"], dict) else f\"{best['thr_obj']:.2f}\"\n            print(f\"MODEL              : {SVM_NAME}\")\n            kv(\"  CV_SCORE\", best[\"cv_score\"])\n            kv(\"  THRESHOLD\", thr_desc)\n            kv(\"  TEST_POOLED_F1\", pooled[\"f1\"])\n            kv(\"  TEST_POOLED_ACC\", pooled[\"acc\"])\n            kv(\"  TEST_MACRO_F1\", macro[\"f1\"])\n            kv(\"  TEST_MACRO_ACC\", macro[\"acc\"])\n            kv(\"  PARAMS\", {\"C\": best[\"C\"], \"final_cv_by_user\": final_cv_by_user})\nelse:\n    print(f\"SKIP_MODEL         : {SVM_NAME} (some user train_pool has single class)\")\n\n\n# =========================\n# 10) LEADERBOARD + SELECT BEST vs Markov\n# =========================\nsection(\"10) LEADERBOARD + SELECT BEST\")\n\nprint(\"BASELINES:\")\nprint(f\"  Baseline-Persist | TEST pooled: acc={persist_pooled['acc']:.4f}, f1={persist_pooled['f1']:.4f} | \"\n      f\"macro(user): acc={persist_macro_acc:.4f}, f1={persist_macro_f1:.4f}\")\nprint(f\"  Baseline-Markov  | CV pooled: f1={cv_f1_mk:.4f}, thr={thr_mk:.2f} | \"\n      f\"TEST pooled: acc={markov_pooled['acc']:.4f}, f1={markov_pooled['f1']:.4f} | \"\n      f\"macro(user): acc={markov_macro_acc:.4f}, f1={markov_macro_f1:.4f}\")\n\nrows_sorted = sorted(rows, key=lambda r: r[\"test_pooled_f1\"], reverse=True)\n\nprint(\"\\nCANDIDATES:\")\nif len(rows_sorted) == 0:\n    print(\"  (no ML candidates succeeded)\")\nelse:\n    for r in rows_sorted:\n        thr_desc = \"per-user\" if isinstance(r[\"thr_obj\"], dict) else f\"{r['thr_obj']:.2f}\"\n        print(\n            f\"  {r['model']:<24} | CV={r['cv_score']:.4f} | thr={thr_desc:<8} | \"\n            f\"TEST pooled: acc={r['test_pooled_acc']:.4f}, f1={r['test_pooled_f1']:.4f} | \"\n            f\"macro(user): acc={r['test_macro_acc']:.4f}, f1={r['test_macro_f1']:.4f} | params={r['params']}\"\n        )\n\nbest_name = \"MarkovUser\"\nbest_obj = {\"type\": \"markov_user\", \"thr\": float(thr_mk), \"probs_by_user\": mk_models}\nbest_test_pooled_f1 = float(markov_pooled[\"f1\"])\nbest_user_records = markov_user_records\nbest_thr_info = thr_mk\n\nif len(rows_sorted) > 0 and float(rows_sorted[0][\"test_pooled_f1\"]) > best_test_pooled_f1:\n    top = rows_sorted[0]\n    best_name = top[\"model\"]\n    best_obj = {\n        \"type\": \"personalized_sklearn\",\n        \"models_by_user\": top[\"models_by_user\"],\n        \"thr\": top[\"thr_obj\"],\n        \"meta\": {\"tune_threshold_per_user\": bool(TUNE_THRESHOLD_PER_USER)},\n    }\n    best_user_records = top[\"test_user_records\"]\n    best_thr_info = top[\"thr_obj\"]\n\nprint(\"\\nSELECTED_BEST      :\", best_name)\nif best_name == \"MarkovUser\":\n    print(\"SELECT_REASON      : Markov baseline remains best on TEST pooled F1 for this dataset.\")\n\nif PRINT_PER_USER_DETAILS:\n    print_per_user_breakdown(f\"PER-USER (SELECTED_BEST={best_name}) on TEST:\", best_user_records, thr_info=best_thr_info)\n\n\n# =========================\n# 11) SAVE ARTIFACT\n# =========================\nsection(\"11) SAVE ARTIFACT\")\n\nMODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\njoblib.dump(\n    {\n        \"best_name\": best_name,\n        \"artifact\": best_obj,\n        \"meta\": {\n            \"target\": \"y_bin = (stress_level_pred>=1)\",\n            \"date_col\": DATE_COL,\n            \"user_col\": USER_COL,\n            \"target_col\": TARGET_COL,\n            \"window\": WINDOW,\n            \"test_len\": TEST_LEN,\n            \"val_windows\": VAL_WINDOWS,\n            \"thresholds\": THRESHOLDS.tolist(),\n            \"users\": users,\n            \"baseline_l1\": \"persistence(per-user)\",\n            \"baseline_l2\": \"markov_user(prev_high, dow)\",\n            \"use_user_id_feature\": USE_USER_ID_FEATURE,\n            \"tune_threshold_per_user\": TUNE_THRESHOLD_PER_USER,\n            \"random_state\": RANDOM_STATE,\n            \"feature_cols\": feature_cols,\n        }\n    },\n    MODEL_OUT\n)\n\nkv(\"SAVED_TO\", str(MODEL_OUT))\nkv(\"BEST_NAME\", best_name)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}