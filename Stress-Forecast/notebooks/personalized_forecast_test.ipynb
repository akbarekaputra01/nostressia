{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Semua nama kolom kini snake_case (mis. `user_id`, `stress_level`, `created_at`, `study_hour_per_day`).\n",
    "- Kolom `is_restored` adalah metadata input/restore dan **tidak** dipakai sebagai fitur model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b359dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 240 | Users: 5\n",
      "Binary dist: {1: 126, 0: 114}\n",
      "\n",
      "=== SPLIT ===\n",
      "Users: [1, 2, 3, 4, 5] | TEST_LEN: 12\n",
      "Total TrainPool: 180 | Total Test: 60\n",
      "\n",
      "=== MARKOV USER (PERSONALIZED) ===\n",
      "Best thr (pooled CV): 0.3 | CV pooled F1: 0.8151\n",
      "TEST pooled: {'acc': 0.75, 'f1': 0.8235294117647058} | TEST macro(user-avg) f1: 0.8102\n",
      "\n",
      "=== PERSONALIZED ML (pooled CV tuning) ===\n",
      "\n",
      "=== PERSONALIZED LEADERBOARD (sorted by TEST pooled F1) ===\n",
      "MarkovUser | CV f1=0.8151 thr=0.30 | TEST pooled f1=0.8235 acc=0.7500 | TEST macro(user)=0.8102\n",
      "LogReg     | CV f1=0.7864 thr=0.05 | TEST pooled f1=0.8046 acc=0.7167 | TEST macro(user) f1=0.7960 | params={'clf__C': 0.1, 'clf__solver': 'liblinear'}\n",
      "DecisionTree | CV f1=0.8166 thr=0.05 | TEST pooled f1=0.7179 acc=0.6333 | TEST macro(user) f1=0.6792 | params={'clf__max_depth': 2, 'clf__min_samples_leaf': 4}\n",
      "ExtraTrees | CV f1=0.8188 thr=0.25 | TEST pooled f1=0.7143 acc=0.6667 | TEST macro(user) f1=0.6628 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 400}\n",
      "RandomForest | CV f1=0.8099 thr=0.40 | TEST pooled f1=0.5574 acc=0.5500 | TEST macro(user) f1=0.5016 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__n_estimators': 400}\n",
      "HistGB     | CV f1=0.8151 thr=0.55 | TEST pooled f1=0.5161 acc=0.5000 | TEST macro(user) f1=0.3152 | params={'clf__learning_rate': 0.05, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "\n",
      "✅ BEST PERSONALIZED (by TEST pooled F1): MarkovUser\n",
      "   TEST: {'f1': 0.8235, 'acc': 0.75}\n",
      "   CV  : {'pooled_f1': 0.8151, 'thr': 0.3}\n",
      "Saved: ..\\models\\personalized_forecast.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# PERSONALIZED_FORECAST (Binary, from stress_level_pred) - 1 CELL\n",
    "# - Train a separate model per user (personalized)\n",
    "# - Compare:\n",
    "#   1) Markov USER(prev_high, dow, user)\n",
    "#   2) Per-user ML: LogReg / DecisionTree / RandomForest / ExtraTrees / HistGB\n",
    "# - Threshold tuning via pooled time-CV windows (stable)\n",
    "# - TEST = last TEST_LEN days per user (time-based)\n",
    "# - Save best to ../models/personalized_forecast_best.joblib\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek path DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/personalized_forecast.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"user_id\"\n",
    "TARGET_COL = \"stress_level_pred\"\n",
    "\n",
    "WINDOW = 7\n",
    "TEST_LEN = 12\n",
    "\n",
    "# CV windows di dalam train_pool tiap user (index relatif)\n",
    "VAL_WINDOWS = [(8, 20), (12, 24), (16, 28)]\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# jika True: fitur user_id tidak dipakai (karena sudah per-user model)\n",
    "# biarkan False/True sama saja; per-user model biasanya tidak butuh user_id\n",
    "USE_USER_ID_FEATURE = False\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # stress lags\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    # rolling stats ending at t-1\n",
    "    g[\"sp_mean_7\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std_7\"]  = sp_shift.rolling(WINDOW).std()\n",
    "    g[\"sp_min_7\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max_7\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high_7\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low_7\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    # streak_high up to t-1\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions_7 ending at t-1\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions_7\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = []\n",
    "if USE_USER_ID_FEATURE:\n",
    "    feature_cols.append(USER_COL)\n",
    "feature_cols += [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean_7\", \"sp_std_7\", \"sp_min_7\", \"sp_max_7\",\n",
    "    \"count_high_7\", \"count_low_7\",\n",
    "    \"streak_high\", \"transitions_7\"\n",
    "]\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== DATASET ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[USER_COL].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# =========================\n",
    "# 2) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "users = sorted(feat[USER_COL].unique().tolist())\n",
    "\n",
    "per_user = {}\n",
    "for uid in users:\n",
    "    g = feat[feat[USER_COL] == uid].sort_values(DATE_COL).reset_index(drop=True)\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(f\"User {uid}: data terlalu sedikit untuk split + CV windows.\")\n",
    "    train_pool = g.iloc[:test_start].copy()\n",
    "    test_block = g.iloc[test_start:].copy()\n",
    "    per_user[uid] = {\"train_pool\": train_pool, \"test\": test_block}\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"Users:\", users, \"| TEST_LEN:\", TEST_LEN)\n",
    "print(\"Total TrainPool:\", sum(len(per_user[u][\"train_pool\"]) for u in users), \"| Total Test:\", sum(len(per_user[u][\"test\"]) for u in users))\n",
    "\n",
    "# CV splits per user (index relatif)\n",
    "def cv_splits_user(train_pool_df):\n",
    "    splits = []\n",
    "    for (v0, v1) in VAL_WINDOWS:\n",
    "        if len(train_pool_df) < v1:\n",
    "            continue\n",
    "        tr = train_pool_df.iloc[:v0]\n",
    "        va = train_pool_df.iloc[v0:v1]\n",
    "        splits.append((tr, va))\n",
    "    return splits\n",
    "\n",
    "# =========================\n",
    "# 3) MARKOV USER\n",
    "# =========================\n",
    "def train_markov_one_user(df_train):\n",
    "    # probs[prev(2), dow(7), y(2)]\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace\n",
    "    return probs\n",
    "\n",
    "def predict_markov_one_user_proba(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    p_high = np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "    return p_high\n",
    "\n",
    "# tune threshold pooled across users & folds\n",
    "p_true_all, p_high_all = [], []\n",
    "for uid in users:\n",
    "    tp = per_user[uid][\"train_pool\"]\n",
    "    for tr_df, va_df in cv_splits_user(tp):\n",
    "        probs = train_markov_one_user(tr_df)\n",
    "        p = predict_markov_one_user_proba(probs, va_df)\n",
    "        p_true_all.append(va_df[\"y_bin\"].values)\n",
    "        p_high_all.append(p)\n",
    "\n",
    "p_true_all = np.concatenate(p_true_all)\n",
    "p_high_all = np.concatenate(p_high_all)\n",
    "thr_markov, cv_f1_markov = tune_thr_from_proba(p_true_all, p_high_all)\n",
    "\n",
    "# train final markov per user, test\n",
    "markov_models = {}\n",
    "test_preds_all, test_true_all = [], []\n",
    "per_user_test_f1 = []\n",
    "\n",
    "for uid in users:\n",
    "    tp = per_user[uid][\"train_pool\"]\n",
    "    te = per_user[uid][\"test\"]\n",
    "\n",
    "    probs = train_markov_one_user(tp)\n",
    "    markov_models[uid] = probs\n",
    "\n",
    "    p_te = predict_markov_one_user_proba(probs, te)\n",
    "    pred_te = (p_te >= thr_markov).astype(int)\n",
    "\n",
    "    test_true_all.append(te[\"y_bin\"].values)\n",
    "    test_preds_all.append(pred_te)\n",
    "\n",
    "    per_user_test_f1.append(f1_score(te[\"y_bin\"], pred_te, zero_division=0))\n",
    "\n",
    "test_true_all = np.concatenate(test_true_all)\n",
    "test_preds_all = np.concatenate(test_preds_all)\n",
    "\n",
    "markov_test = eval_bin(test_true_all, test_preds_all)\n",
    "markov_test_macro = float(np.mean(per_user_test_f1))\n",
    "\n",
    "print(\"\\n=== MARKOV USER (PERSONALIZED) ===\")\n",
    "print(\"Best thr (pooled CV):\", thr_markov, \"| CV pooled F1:\", round(cv_f1_markov, 4))\n",
    "print(\"TEST pooled:\", markov_test, \"| TEST macro(user-avg) f1:\", round(markov_test_macro, 4))\n",
    "\n",
    "# =========================\n",
    "# 4) PERSONALIZED ML: per user model, tuned fairly (pooled CV)\n",
    "# =========================\n",
    "# preprocess for per-user (user_id not needed)\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    cat_cols = [USER_COL] + cat_cols\n",
    "\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def pooled_cv_best_params_and_thr_for_personalized(model_name, clf, grid):\n",
    "    \"\"\"\n",
    "    For each param:\n",
    "      - For each user:\n",
    "        - For each CV fold (time window):\n",
    "          train on user-fold train, predict proba on user-fold val\n",
    "      - Pool all user+fold probs, tune threshold, get pooled CV F1\n",
    "    \"\"\"\n",
    "    best = None\n",
    "\n",
    "    for params in ParameterGrid(grid):\n",
    "        p_true_all, p_high_all = [], []\n",
    "\n",
    "        for uid in users:\n",
    "            tp = per_user[uid][\"train_pool\"]\n",
    "            folds = cv_splits_user(tp)\n",
    "            if len(folds) == 0:\n",
    "                continue\n",
    "\n",
    "            for tr_df, va_df in folds:\n",
    "                pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "                pipe.set_params(**params)\n",
    "                pipe.fit(tr_df[feature_cols], tr_df[\"y_bin\"])\n",
    "                p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n",
    "\n",
    "                p_true_all.append(va_df[\"y_bin\"].values)\n",
    "                p_high_all.append(p)\n",
    "\n",
    "        y_all = np.concatenate(p_true_all)\n",
    "        p_all = np.concatenate(p_high_all)\n",
    "        thr, cv_f1 = tune_thr_from_proba(y_all, p_all)\n",
    "\n",
    "        if (best is None) or (cv_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"params\": params, \"thr\": thr, \"cv_f1\": float(cv_f1)}\n",
    "\n",
    "    return best\n",
    "\n",
    "ml_rows = []\n",
    "\n",
    "print(\"\\n=== PERSONALIZED ML (pooled CV tuning) ===\")\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    best = pooled_cv_best_params_and_thr_for_personalized(name, clf, grid)\n",
    "\n",
    "    # train final per-user models on full train_pool\n",
    "    per_user_models = {}\n",
    "    per_user_f1 = []\n",
    "    pooled_true, pooled_pred = [], []\n",
    "\n",
    "    for uid in users:\n",
    "        tp = per_user[uid][\"train_pool\"]\n",
    "        te = per_user[uid][\"test\"]\n",
    "\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "        pipe.set_params(**best[\"params\"])\n",
    "        pipe.fit(tp[feature_cols], tp[\"y_bin\"])\n",
    "\n",
    "        p = pipe.predict_proba(te[feature_cols])[:, 1]\n",
    "        pred = (p >= best[\"thr\"]).astype(int)\n",
    "\n",
    "        per_user_models[uid] = pipe\n",
    "        per_user_f1.append(f1_score(te[\"y_bin\"], pred, zero_division=0))\n",
    "        pooled_true.append(te[\"y_bin\"].values)\n",
    "        pooled_pred.append(pred)\n",
    "\n",
    "    pooled_true = np.concatenate(pooled_true)\n",
    "    pooled_pred = np.concatenate(pooled_pred)\n",
    "\n",
    "    test_metrics = eval_bin(pooled_true, pooled_pred)\n",
    "    test_macro = float(np.mean(per_user_f1))\n",
    "\n",
    "    ml_rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": float(best[\"cv_f1\"]),\n",
    "        \"thr\": float(best[\"thr\"]),\n",
    "        \"test_f1_pooled\": float(test_metrics[\"f1\"]),\n",
    "        \"test_acc_pooled\": float(test_metrics[\"acc\"]),\n",
    "        \"test_f1_macro_users\": float(test_macro),\n",
    "        \"best_params\": best[\"params\"],\n",
    "        \"models_by_user\": per_user_models,\n",
    "    })\n",
    "\n",
    "# leaderboard by TEST pooled F1\n",
    "ml_sorted = sorted(ml_rows, key=lambda r: r[\"test_f1_pooled\"], reverse=True)\n",
    "\n",
    "print(\"\\n=== PERSONALIZED LEADERBOARD (sorted by TEST pooled F1) ===\")\n",
    "print(\"MarkovUser | CV f1=%.4f thr=%.2f | TEST pooled f1=%.4f acc=%.4f | TEST macro(user)=%.4f\" %\n",
    "      (cv_f1_markov, thr_markov, markov_test[\"f1\"], markov_test[\"acc\"], markov_test_macro))\n",
    "\n",
    "for r in ml_sorted:\n",
    "    print(f\"{r['model']:<10} | CV f1={r['cv_f1']:.4f} thr={r['thr']:.2f} | \"\n",
    "          f\"TEST pooled f1={r['test_f1_pooled']:.4f} acc={r['test_acc_pooled']:.4f} | \"\n",
    "          f\"TEST macro(user) f1={r['test_f1_macro_users']:.4f} | params={r['best_params']}\")\n",
    "\n",
    "# pick best by TEST pooled F1 (for practicality)\n",
    "best_personal = {\"name\": \"MarkovUser\", \"thr\": thr_markov, \"cv_f1\": cv_f1_markov,\n",
    "                 \"test_f1\": markov_test[\"f1\"], \"test_acc\": markov_test[\"acc\"],\n",
    "                 \"artifact\": {\"type\": \"markov_user\", \"thr\": float(thr_markov), \"probs_by_user\": markov_models}}\n",
    "\n",
    "if len(ml_sorted) > 0 and ml_sorted[0][\"test_f1_pooled\"] > best_personal[\"test_f1\"]:\n",
    "    top = ml_sorted[0]\n",
    "    best_personal = {\n",
    "        \"name\": top[\"model\"],\n",
    "        \"thr\": float(top[\"thr\"]),\n",
    "        \"cv_f1\": float(top[\"cv_f1\"]),\n",
    "        \"test_f1\": float(top[\"test_f1_pooled\"]),\n",
    "        \"test_acc\": float(top[\"test_acc_pooled\"]),\n",
    "        \"artifact\": {\"type\": \"personalized_sklearn\", \"thr\": float(top[\"thr\"]), \"models_by_user\": top[\"models_by_user\"]}\n",
    "    }\n",
    "\n",
    "print(\"\\n✅ BEST PERSONALIZED (by TEST pooled F1):\", best_personal[\"name\"])\n",
    "print(\"   TEST:\", {\"f1\": round(best_personal[\"test_f1\"], 4), \"acc\": round(best_personal[\"test_acc\"], 4)})\n",
    "print(\"   CV  :\", {\"pooled_f1\": round(best_personal[\"cv_f1\"], 4), \"thr\": round(best_personal[\"thr\"], 2)})\n",
    "\n",
    "# Save\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"type\": best_personal[\"artifact\"][\"type\"],\n",
    "        \"thr\": float(best_personal[\"thr\"]),\n",
    "        \"artifact\": best_personal[\"artifact\"],\n",
    "        \"meta\": {\n",
    "            \"target\": \"y_bin = (stress_level_pred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"use_user_id_feature\": USE_USER_ID_FEATURE,\n",
    "            \"users\": users,\n",
    "        }\n",
    "    },\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(\"Saved:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e403ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Users: [1, 2, 3, 4, 5]\n",
      "Rows: 240 | Binary dist: {1: 126, 0: 114}\n",
      "WINDOW: 7 | TEST_LEN: 12 | USE_USER_ID_FEATURE: False\n",
      "\n",
      "=== SPLIT ===\n",
      "Total TrainPool: 180 | Total Test: 60\n",
      "\n",
      "=== BASELINE L1: Persistence (per user) ===\n",
      "TEST pooled: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328} | TEST macro(user-avg) f1: 0.72\n",
      "\n",
      "=== BASELINE L2: Markov USER(prev_high, dow, user) ===\n",
      "Best thr: 0.3 | CV pooled F1: 0.8151\n",
      "TEST pooled: {'acc': 0.75, 'f1': 0.8235294117647058} | TEST macro(user-avg) f1: 0.8102\n",
      "\n",
      "=== PERSONALIZED ML: TRAIN + TUNE (pooled CV, fair protocol) ===\n",
      "\n",
      "=== PERSONALIZED LEADERBOARD (sorted by TEST pooled F1) ===\n",
      "Baseline-Persist | TEST pooled f1=0.7671 acc=0.7167 | macro(user) f1=0.7200\n",
      "Baseline-Markov  | CV f1=0.8151 thr=0.30 | TEST pooled f1=0.8235 acc=0.7500 | macro(user) f1=0.8102\n",
      "LogReg     | CV f1=0.7864 thr=0.05 | TEST pooled f1=0.8046 acc=0.7167 | macro(user) f1=0.7960 | params={'clf__C': 0.1, 'clf__solver': 'liblinear'}\n",
      "DecisionTree | CV f1=0.8166 thr=0.05 | TEST pooled f1=0.7179 acc=0.6333 | macro(user) f1=0.6792 | params={'clf__max_depth': 2, 'clf__min_samples_leaf': 4}\n",
      "ExtraTrees | CV f1=0.8188 thr=0.25 | TEST pooled f1=0.7143 acc=0.6667 | macro(user) f1=0.6628 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 400}\n",
      "RandomForest | CV f1=0.8099 thr=0.40 | TEST pooled f1=0.5574 acc=0.5500 | macro(user) f1=0.5016 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__n_estimators': 400}\n",
      "HistGB     | CV f1=0.8151 thr=0.55 | TEST pooled f1=0.5161 acc=0.5000 | macro(user) f1=0.3152 | params={'clf__learning_rate': 0.05, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "\n",
      "✅ BEST PERSONALIZED (by TEST pooled F1): MarkovUser\n",
      "TEST: {'f1': 0.8235, 'acc': 0.75}\n",
      "Saved: ..\\models\\personalized_forecast_best.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# PERSONALIZED_FORECAST (Binary, from stress_level_pred) - 1 CELL (Consistent Baselines)\n",
    "#\n",
    "# Baseline Level 1 (paling dasar, untuk personalized juga):\n",
    "#   - Persistence per user: y(t)=y(t-1)\n",
    "#\n",
    "# Baseline Level 2 (probabilistik, personalized):\n",
    "#   - Markov USER: P(high_t | prev_high, dow, user) + threshold tuning (pooled time-CV)\n",
    "#\n",
    "# Models (PERSONALIZED = model terpisah per user):\n",
    "#   - LogReg / DecisionTree / RandomForest / ExtraTrees / HistGB (per user)\n",
    "#   - Semua pakai: time-based split per user (TEST=last TEST_LEN),\n",
    "#                  time-based CV windows per user (pooled across users),\n",
    "#                  threshold tuning yang sama.\n",
    "#\n",
    "# Target:\n",
    "#   y_bin = 1 if stress_level_pred(t) >= 1 else 0\n",
    "#\n",
    "# Data:\n",
    "#   /mnt/data/global_dataset_pred.csv (upload kamu) / atau ../datasets/global_dataset_pred.csv\n",
    "#\n",
    "# Save best personalized:\n",
    "#   ../models/personalized_forecast_best.joblib\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"/mnt/data/global_dataset_pred.csv\"),\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek path DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/personalized_forecast_best.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"user_id\"\n",
    "TARGET_COL = \"stress_level_pred\"\n",
    "\n",
    "WINDOW = 7\n",
    "TEST_LEN = 12\n",
    "\n",
    "# CV windows dalam train_pool tiap user (index relatif)\n",
    "VAL_WINDOWS = [(8, 20), (12, 24), (16, 28)]\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Personalized = per user model, jadi user_id biasanya TIDAK perlu jadi fitur.\n",
    "USE_USER_ID_FEATURE = False\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    g[\"sp_mean_7\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std_7\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min_7\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max_7\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high_7\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low_7\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    # streak_high (<= t-1)\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions_7\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions_7\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean_7\", \"sp_std_7\", \"sp_min_7\", \"sp_max_7\",\n",
    "    \"count_high_7\", \"count_low_7\",\n",
    "    \"streak_high\", \"transitions_7\"\n",
    "]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    feature_cols = [USER_COL] + feature_cols\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "users = sorted(feat[USER_COL].unique().tolist())\n",
    "\n",
    "print(\"=== DATASET ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Users:\", users)\n",
    "print(\"Rows:\", len(feat), \"| Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN, \"| USE_USER_ID_FEATURE:\", USE_USER_ID_FEATURE)\n",
    "\n",
    "# =========================\n",
    "# 2) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "per_user = {}\n",
    "for uid in users:\n",
    "    g = feat[feat[USER_COL] == uid].sort_values(DATE_COL).reset_index(drop=True)\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(f\"User {uid}: data terlalu sedikit untuk split + CV windows.\")\n",
    "    per_user[uid] = {\n",
    "        \"train_pool\": g.iloc[:test_start].copy(),\n",
    "        \"test\": g.iloc[test_start:].copy()\n",
    "    }\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"Total TrainPool:\", sum(len(per_user[u][\"train_pool\"]) for u in users),\n",
    "      \"| Total Test:\", sum(len(per_user[u][\"test\"]) for u in users))\n",
    "\n",
    "def cv_folds_user(tp_df):\n",
    "    folds = []\n",
    "    for (v0, v1) in VAL_WINDOWS:\n",
    "        if len(tp_df) < v1:\n",
    "            continue\n",
    "        tr = tp_df.iloc[:v0]\n",
    "        va = tp_df.iloc[v0:v1]\n",
    "        folds.append((tr, va))\n",
    "    return folds\n",
    "\n",
    "# =========================\n",
    "# 3) BASELINE L1: Persistence (per user)\n",
    "# =========================\n",
    "all_true, all_pred = [], []\n",
    "per_user_f1 = []\n",
    "\n",
    "for uid in users:\n",
    "    te = per_user[uid][\"test\"]\n",
    "    pred = (te[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    y = te[\"y_bin\"].astype(int).values\n",
    "\n",
    "    all_true.append(y)\n",
    "    all_pred.append(pred)\n",
    "    per_user_f1.append(f1_score(y, pred, zero_division=0))\n",
    "\n",
    "y_all = np.concatenate(all_true)\n",
    "p_all = np.concatenate(all_pred)\n",
    "\n",
    "persist_test = eval_bin(y_all, p_all)\n",
    "persist_macro = float(np.mean(per_user_f1))\n",
    "\n",
    "print(\"\\n=== BASELINE L1: Persistence (per user) ===\")\n",
    "print(\"TEST pooled:\", persist_test, \"| TEST macro(user-avg) f1:\", round(persist_macro, 4))\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINE L2: Markov USER(prev_high, dow, user) + thr tuning\n",
    "# =========================\n",
    "def train_markov_one_user(df_train):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace\n",
    "    return probs\n",
    "\n",
    "def markov_proba_user(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "\n",
    "# pooled CV for threshold (across users & folds)\n",
    "cv_true, cv_phigh = [], []\n",
    "for uid in users:\n",
    "    tp = per_user[uid][\"train_pool\"]\n",
    "    for tr_df, va_df in cv_folds_user(tp):\n",
    "        probs = train_markov_one_user(tr_df)\n",
    "        p = markov_proba_user(probs, va_df)\n",
    "        cv_true.append(va_df[\"y_bin\"].values)\n",
    "        cv_phigh.append(p)\n",
    "\n",
    "cv_true = np.concatenate(cv_true)\n",
    "cv_phigh = np.concatenate(cv_phigh)\n",
    "\n",
    "thr_mk, cv_f1_mk = tune_thr_from_proba(cv_true, cv_phigh)\n",
    "\n",
    "# train final per-user Markov on full train_pool -> test\n",
    "mk_models = {}\n",
    "all_true, all_pred = [], []\n",
    "per_user_f1 = []\n",
    "\n",
    "for uid in users:\n",
    "    tp = per_user[uid][\"train_pool\"]\n",
    "    te = per_user[uid][\"test\"]\n",
    "\n",
    "    probs = train_markov_one_user(tp)\n",
    "    mk_models[uid] = probs\n",
    "\n",
    "    p = markov_proba_user(probs, te)\n",
    "    pred = (p >= thr_mk).astype(int)\n",
    "\n",
    "    y = te[\"y_bin\"].values\n",
    "    all_true.append(y)\n",
    "    all_pred.append(pred)\n",
    "    per_user_f1.append(f1_score(y, pred, zero_division=0))\n",
    "\n",
    "y_all = np.concatenate(all_true)\n",
    "pred_all = np.concatenate(all_pred)\n",
    "\n",
    "markov_test = eval_bin(y_all, pred_all)\n",
    "markov_macro = float(np.mean(per_user_f1))\n",
    "\n",
    "print(\"\\n=== BASELINE L2: Markov USER(prev_high, dow, user) ===\")\n",
    "print(\"Best thr:\", thr_mk, \"| CV pooled F1:\", round(cv_f1_mk, 4))\n",
    "print(\"TEST pooled:\", markov_test, \"| TEST macro(user-avg) f1:\", round(markov_macro, 4))\n",
    "\n",
    "# =========================\n",
    "# 5) MODELS: per-user ML (fair protocol, pooled CV)\n",
    "# =========================\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    cat_cols = [USER_COL] + cat_cols\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def pooled_cv_best_params_and_thr_personalized(clf, grid):\n",
    "    best = None\n",
    "    for params in ParameterGrid(grid):\n",
    "        y_list, p_list = [], []\n",
    "\n",
    "        for uid in users:\n",
    "            tp = per_user[uid][\"train_pool\"]\n",
    "            folds = cv_folds_user(tp)\n",
    "            if len(folds) == 0:\n",
    "                continue\n",
    "            for tr_df, va_df in folds:\n",
    "                pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "                pipe.set_params(**params)\n",
    "                pipe.fit(tr_df[feature_cols], tr_df[\"y_bin\"].astype(int))\n",
    "\n",
    "                p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n",
    "                y_list.append(va_df[\"y_bin\"].values)\n",
    "                p_list.append(p)\n",
    "\n",
    "        y_all = np.concatenate(y_list)\n",
    "        p_all = np.concatenate(p_list)\n",
    "\n",
    "        thr, cv_f1 = tune_thr_from_proba(y_all, p_all)\n",
    "\n",
    "        if (best is None) or (cv_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"params\": params, \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "    return best\n",
    "\n",
    "rows = []\n",
    "print(\"\\n=== PERSONALIZED ML: TRAIN + TUNE (pooled CV, fair protocol) ===\")\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    best = pooled_cv_best_params_and_thr_personalized(clf, grid)\n",
    "\n",
    "    models_by_user = {}\n",
    "    per_user_test_f1 = []\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    for uid in users:\n",
    "        tp = per_user[uid][\"train_pool\"]\n",
    "        te = per_user[uid][\"test\"]\n",
    "\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "        pipe.set_params(**best[\"params\"])\n",
    "        pipe.fit(tp[feature_cols], tp[\"y_bin\"].astype(int))\n",
    "\n",
    "        p = pipe.predict_proba(te[feature_cols])[:, 1]\n",
    "        pred = (p >= best[\"thr\"]).astype(int)\n",
    "\n",
    "        models_by_user[uid] = pipe\n",
    "        per_user_test_f1.append(f1_score(te[\"y_bin\"].values, pred, zero_division=0))\n",
    "\n",
    "        all_true.append(te[\"y_bin\"].values)\n",
    "        all_pred.append(pred)\n",
    "\n",
    "    y_all = np.concatenate(all_true)\n",
    "    pred_all = np.concatenate(all_pred)\n",
    "\n",
    "    test_metrics = eval_bin(y_all, pred_all)\n",
    "    test_macro = float(np.mean(per_user_test_f1))\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": best[\"cv_f1\"],\n",
    "        \"thr\": best[\"thr\"],\n",
    "        \"test_f1_pooled\": test_metrics[\"f1\"],\n",
    "        \"test_acc_pooled\": test_metrics[\"acc\"],\n",
    "        \"test_f1_macro_users\": test_macro,\n",
    "        \"params\": best[\"params\"],\n",
    "        \"models_by_user\": models_by_user,\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 6) LEADERBOARD + SAVE BEST\n",
    "# =========================\n",
    "print(\"\\n=== PERSONALIZED LEADERBOARD (sorted by TEST pooled F1) ===\")\n",
    "print(f\"Baseline-Persist | TEST pooled f1={persist_test['f1']:.4f} acc={persist_test['acc']:.4f} | macro(user) f1={persist_macro:.4f}\")\n",
    "print(f\"Baseline-Markov  | CV f1={cv_f1_mk:.4f} thr={thr_mk:.2f} | TEST pooled f1={markov_test['f1']:.4f} acc={markov_test['acc']:.4f} | macro(user) f1={markov_macro:.4f}\")\n",
    "\n",
    "rows_sorted = sorted(rows, key=lambda r: r[\"test_f1_pooled\"], reverse=True)\n",
    "for r in rows_sorted:\n",
    "    print(f\"{r['model']:<10} | CV f1={r['cv_f1']:.4f} thr={r['thr']:.2f} | \"\n",
    "          f\"TEST pooled f1={r['test_f1_pooled']:.4f} acc={r['test_acc_pooled']:.4f} | \"\n",
    "          f\"macro(user) f1={r['test_f1_macro_users']:.4f} | params={r['params']}\")\n",
    "\n",
    "# choose best among: Markov vs best ML (by TEST pooled F1, for practicality)\n",
    "best_artifact = {\"type\": \"markov_user\", \"thr\": float(thr_mk), \"probs_by_user\": mk_models}\n",
    "best_name = \"MarkovUser\"\n",
    "best_test_f1 = float(markov_test[\"f1\"])\n",
    "best_test_acc = float(markov_test[\"acc\"])\n",
    "best_cv_f1 = float(cv_f1_mk)\n",
    "\n",
    "if len(rows_sorted) > 0 and rows_sorted[0][\"test_f1_pooled\"] > best_test_f1:\n",
    "    top = rows_sorted[0]\n",
    "    best_name = top[\"model\"]\n",
    "    best_test_f1 = float(top[\"test_f1_pooled\"])\n",
    "    best_test_acc = float(top[\"test_acc_pooled\"])\n",
    "    best_cv_f1 = float(top[\"cv_f1\"])\n",
    "    best_artifact = {\"type\": \"personalized_sklearn\", \"thr\": float(top[\"thr\"]), \"models_by_user\": top[\"models_by_user\"]}\n",
    "\n",
    "print(\"\\n✅ BEST PERSONALIZED (by TEST pooled F1):\", best_name)\n",
    "print(\"TEST:\", {\"f1\": round(best_test_f1, 4), \"acc\": round(best_test_acc, 4)})\n",
    "\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"best_name\": best_name,\n",
    "        \"artifact\": best_artifact,\n",
    "        \"meta\": {\n",
    "            \"target\": \"y_bin = (stress_level_pred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"users\": users,\n",
    "            \"baseline_l1\": \"persistence(per-user)\",\n",
    "            \"baseline_l2\": \"markov_user(prev_high, dow, user)\",\n",
    "        }\n",
    "    },\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(\"Saved:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc610bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAW ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 275 | Users: 5\n",
      "Behavior cols: ['extracurricular_hour_per_day', 'physical_activity_hour_per_day', 'sleep_hour_per_day', 'study_hour_per_day', 'social_hour_per_day']\n",
      "\n",
      "=== FEAT ===\n",
      "Users: [1, 2, 3, 4, 5] | Rows: 260\n",
      "Binary dist: {1: 146, 0: 114}\n",
      "WINDOW: 3 | TEST_LEN: 12\n",
      "\n",
      "=== SPLIT ===\n",
      "Total TrainPool: 200 | Total Test: 60\n",
      "\n",
      "=== TRUE PERSONALIZED TRAIN ===\n",
      "\n",
      "=== PERSONALIZED SUMMARY (per user) ===\n",
      "User 1: Persist f1=0.8889 | Markov f1=0.8571 | BEST=Blend-RandomForest f1=0.8889\n",
      "User 2: Persist f1=0.3636 | Markov f1=0.6667 | BEST=MarkovUser f1=0.6667\n",
      "User 3: Persist f1=0.6000 | Markov f1=0.9091 | BEST=MarkovUser f1=0.9091\n",
      "User 4: Persist f1=0.9474 | Markov f1=0.9091 | BEST=MarkovUser f1=0.9091\n",
      "User 5: Persist f1=0.8000 | Markov f1=1.0000 | BEST=MarkovUser f1=1.0000\n",
      "\n",
      "=== PERSONALIZED POOLED TEST (all users combined) ===\n",
      "{'acc': 0.8, 'f1': 0.8571428571428571}\n",
      "\n",
      "Saved: ..\\models\\personalized_forecast_true_personalized.joblib\n"
     ]
    }
   ],
   "source": [
    "# personalized_forecast_true_personalized.py\n",
    "# =====================================================================================\n",
    "# TRUE PERSONALIZED FORECAST (Binary) from stress_level_pred\n",
    "#\n",
    "# TRUE PERSONALIZED means:\n",
    "# - Model terpisah per user (train hanya dari data user tsb)\n",
    "# - Threshold per user (dituning di CV user tsb)\n",
    "# - (Optional) Blend per user: MarkovUser + MLUser\n",
    "#\n",
    "# Baselines per user:\n",
    "# - L1: Persistence\n",
    "# - L2: Markov USER P(high_t | prev_high, dow) + thr tuning per user\n",
    "#\n",
    "# Candidate ML per user:\n",
    "# - LogReg, RandomForest, ExtraTrees, HistGB\n",
    "#\n",
    "# Saves:\n",
    "# - ../models/personalized_forecast_true_personalized.joblib\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [Path(\"/mnt/data/global_dataset_pred.csv\"), Path(\"../datasets/global_dataset_pred.csv\")]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/personalized_forecast_true_personalized.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"user_id\"\n",
    "TARGET_COL = \"stress_level_pred\"\n",
    "\n",
    "WINDOW   = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "THRESHOLDS  = np.linspace(0.05, 0.95, 19)\n",
    "ALPHAS      = np.linspace(0.0, 1.0, 21)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "USE_BEHAVIOR_LAG1 = True\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "            \"f1\":  float(f1_score(y_true, y_pred, zero_division=0))}\n",
    "\n",
    "def tune_thr(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1.0\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "def build_user_cv_blocks(tp_df):\n",
    "    folds = []\n",
    "    for (v0, v1) in VAL_WINDOWS:\n",
    "        if len(tp_df) < v1:\n",
    "            continue\n",
    "        tr = tp_df.iloc[:v0].copy()\n",
    "        va = tp_df.iloc[v0:v1].copy()\n",
    "        folds.append((tr, va))\n",
    "    return folds\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "exclude = {DATE_COL, USER_COL, TARGET_COL}\n",
    "num_cols_all = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "hour_like = [c for c in num_cols_all if (\"hour\" in c.lower()) or (\"hours\" in c.lower())]\n",
    "known = [\"study_hour_per_day\",\"sleep_hour_per_day\",\"social_hour_per_day\",\"physical_activity_hour_per_day\",\"extracurricular_hour_per_day\"]\n",
    "for c in known:\n",
    "    if c in num_cols_all and c not in hour_like:\n",
    "        hour_like.append(c)\n",
    "BEHAVIOR_COLS = hour_like if USE_BEHAVIOR_LAG1 else []\n",
    "\n",
    "print(\"=== RAW ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(df), \"| Users:\", df[USER_COL].nunique())\n",
    "print(\"Behavior cols:\", BEHAVIOR_COLS)\n",
    "\n",
    "# =========================\n",
    "# Feature engineering per user\n",
    "# =========================\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    if len(BEHAVIOR_COLS) > 0:\n",
    "        for c in BEHAVIOR_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\",\"sp_std\",\"sp_min\",\"sp_max\",\"count_high\",\"count_low\",\"streak_high\",\"transitions\"\n",
    "]\n",
    "if len(BEHAVIOR_COLS) > 0:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in BEHAVIOR_COLS]\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "users = sorted(feat[USER_COL].unique().tolist())\n",
    "print(\"\\n=== FEAT ===\")\n",
    "print(\"Users:\", users, \"| Rows:\", len(feat))\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN)\n",
    "\n",
    "# =========================\n",
    "# Split per user: TEST = last TEST_LEN\n",
    "# =========================\n",
    "per_user = {}\n",
    "for uid in users:\n",
    "    g = feat[feat[USER_COL] == uid].sort_values(DATE_COL).reset_index(drop=True)\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(f\"User {uid}: data terlalu sedikit.\")\n",
    "    tp = g.iloc[:test_start].copy()\n",
    "    te = g.iloc[test_start:].copy()\n",
    "    per_user[uid] = {\"train_pool\": tp, \"test\": te, \"folds\": build_user_cv_blocks(tp)}\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"Total TrainPool:\", sum(len(per_user[u][\"train_pool\"]) for u in users),\n",
    "      \"| Total Test:\", sum(len(per_user[u][\"test\"]) for u in users))\n",
    "\n",
    "# =========================\n",
    "# Markov USER per user\n",
    "# =========================\n",
    "def train_markov_user(df_train_user):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train_user[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train_user[\"dow\"].astype(int).values\n",
    "    yb   = df_train_user[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)\n",
    "    return probs\n",
    "\n",
    "def markov_proba_user(probs, df_eval_user):\n",
    "    prev = (df_eval_user[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval_user[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "\n",
    "# =========================\n",
    "# Candidate ML (per user)\n",
    "# =========================\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Train TRUE PERSONALIZED: tune per-user (params + thr + alpha)\n",
    "# =========================\n",
    "print(\"\\n=== TRUE PERSONALIZED TRAIN ===\")\n",
    "\n",
    "models_by_user = {}\n",
    "report_rows = []\n",
    "\n",
    "for uid in users:\n",
    "    tp = per_user[uid][\"train_pool\"]\n",
    "    te = per_user[uid][\"test\"]\n",
    "    folds = per_user[uid][\"folds\"]\n",
    "    if len(folds) == 0:\n",
    "        raise ValueError(f\"User {uid}: folds kosong. Kecilkan VAL_WINDOWS / WINDOW / TEST_LEN.\")\n",
    "\n",
    "    # ---- Baseline L1: persistence\n",
    "    y_te = te[\"y_bin\"].astype(int).values\n",
    "    pred_persist = (te[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    base_persist = eval_bin(y_te, pred_persist)\n",
    "\n",
    "    # ---- Baseline L2: Markov user + thr per user (tune dari CV user tsb)\n",
    "    y_cv_all, p_mk_cv_all = [], []\n",
    "    for tr_df, va_df in folds:\n",
    "        probs = train_markov_user(tr_df)\n",
    "        p = markov_proba_user(probs, va_df)\n",
    "        y_cv_all.append(va_df[\"y_bin\"].astype(int).values)\n",
    "        p_mk_cv_all.append(p)\n",
    "\n",
    "    y_cv_all = np.concatenate(y_cv_all)\n",
    "    p_mk_cv_all = np.concatenate(p_mk_cv_all)\n",
    "    thr_mk, cv_f1_mk = tune_thr(y_cv_all, p_mk_cv_all)\n",
    "\n",
    "    probs_full = train_markov_user(tp)\n",
    "    p_mk_test  = markov_proba_user(probs_full, te)\n",
    "    pred_mk_test = (p_mk_test >= thr_mk).astype(int)\n",
    "    mk_metrics = eval_bin(y_te, pred_mk_test)\n",
    "\n",
    "    # ---- Tune ML per user + BLEND per user (alpha & thr dituning per user)\n",
    "    best_user = {\"name\": \"MarkovUser\", \"test_f1\": mk_metrics[\"f1\"], \"artifact\": {\"type\":\"markov_user\",\"probs\":probs_full,\"thr\":thr_mk}}\n",
    "\n",
    "    for mname, (clf, grid) in CANDIDATES.items():\n",
    "        # 1) pilih best params via CV (p_ml pooled across user folds)\n",
    "        best_params = None\n",
    "        best_cv_ml  = -1.0\n",
    "        best_thr_ml = 0.5\n",
    "\n",
    "        for params in ParameterGrid(grid):\n",
    "            y_all, p_all = [], []\n",
    "            for tr_df, va_df in folds:\n",
    "                pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "                pipe.set_params(**params)\n",
    "                pipe.fit(tr_df[feature_cols], tr_df[\"y_bin\"].astype(int).values)\n",
    "                p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n",
    "                y_all.append(va_df[\"y_bin\"].astype(int).values)\n",
    "                p_all.append(p)\n",
    "\n",
    "            y_all = np.concatenate(y_all)\n",
    "            p_all = np.concatenate(p_all)\n",
    "\n",
    "            thr, cv_f1 = tune_thr(y_all, p_all)\n",
    "            if cv_f1 > best_cv_ml:\n",
    "                best_cv_ml = cv_f1\n",
    "                best_thr_ml = thr\n",
    "                best_params = params\n",
    "\n",
    "        # 2) fit final ML on full train_pool\n",
    "        pipe_full = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "        pipe_full.set_params(**best_params)\n",
    "        pipe_full.fit(tp[feature_cols], tp[\"y_bin\"].astype(int).values)\n",
    "        p_ml_test = pipe_full.predict_proba(te[feature_cols])[:, 1]\n",
    "\n",
    "        # 3) tune BLEND alpha+thr per user (strict, based on CV folds)\n",
    "        # compute p_ml_cv using the best_params model on each fold\n",
    "        p_ml_cv_all = []\n",
    "        y_bl_cv_all = []\n",
    "        for tr_df, va_df in folds:\n",
    "            pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "            pipe.set_params(**best_params)\n",
    "            pipe.fit(tr_df[feature_cols], tr_df[\"y_bin\"].astype(int).values)\n",
    "            p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n",
    "            p_ml_cv_all.append(p)\n",
    "            y_bl_cv_all.append(va_df[\"y_bin\"].astype(int).values)\n",
    "\n",
    "        p_ml_cv_all = np.concatenate(p_ml_cv_all)\n",
    "        y_bl_cv_all = np.concatenate(y_bl_cv_all)\n",
    "\n",
    "        # Note: p_mk_cv_all sudah ada untuk user ini\n",
    "        best_blend = None\n",
    "        for a in ALPHAS:\n",
    "            p_bl = a * p_ml_cv_all + (1.0 - a) * p_mk_cv_all\n",
    "            thr, cv_f1 = tune_thr(y_bl_cv_all, p_bl)\n",
    "            if (best_blend is None) or (cv_f1 > best_blend[\"cv_f1\"]):\n",
    "                best_blend = {\"alpha\": float(a), \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "\n",
    "        p_bl_test = best_blend[\"alpha\"] * p_ml_test + (1.0 - best_blend[\"alpha\"]) * p_mk_test\n",
    "        pred_bl_test = (p_bl_test >= best_blend[\"thr\"]).astype(int)\n",
    "        blend_metrics = eval_bin(y_te, pred_bl_test)\n",
    "\n",
    "        # pilih terbaik untuk user ini\n",
    "        if blend_metrics[\"f1\"] > best_user[\"test_f1\"]:\n",
    "            best_user = {\n",
    "                \"name\": f\"Blend-{mname}\",\n",
    "                \"test_f1\": float(blend_metrics[\"f1\"]),\n",
    "                \"artifact\": {\n",
    "                    \"type\": \"true_personalized_blend\",\n",
    "                    \"alpha\": float(best_blend[\"alpha\"]),\n",
    "                    \"thr\": float(best_blend[\"thr\"]),\n",
    "                    \"markov\": {\"probs\": probs_full},\n",
    "                    \"ml\": {\"pipe\": pipe_full},\n",
    "                    \"meta\": {\"best_params\": best_params, \"cv_f1_blend\": best_blend[\"cv_f1\"], \"cv_f1_ml\": best_cv_ml}\n",
    "                }\n",
    "            }\n",
    "\n",
    "    models_by_user[uid] = {\n",
    "        \"baseline_persist\": base_persist,\n",
    "        \"baseline_markov\": {\"thr\": float(thr_mk), \"cv_f1\": float(cv_f1_mk), \"test\": mk_metrics},\n",
    "        \"best\": best_user\n",
    "    }\n",
    "\n",
    "    report_rows.append({\n",
    "        \"user\": uid,\n",
    "        \"persist_f1\": base_persist[\"f1\"],\n",
    "        \"markov_f1\": mk_metrics[\"f1\"],\n",
    "        \"best_name\": best_user[\"name\"],\n",
    "        \"best_f1\": best_user[\"test_f1\"],\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# Report\n",
    "# =========================\n",
    "print(\"\\n=== PERSONALIZED SUMMARY (per user) ===\")\n",
    "for r in report_rows:\n",
    "    print(f\"User {r['user']}: Persist f1={r['persist_f1']:.4f} | Markov f1={r['markov_f1']:.4f} | BEST={r['best_name']} f1={r['best_f1']:.4f}\")\n",
    "\n",
    "# pooled evaluation (gabung semua test user) untuk informasi tambahan\n",
    "all_y, all_pred = [], []\n",
    "for uid in users:\n",
    "    te = per_user[uid][\"test\"]\n",
    "    y = te[\"y_bin\"].astype(int).values\n",
    "    best = models_by_user[uid][\"best\"]\n",
    "\n",
    "    # reproduce prediction for pooled score\n",
    "    probs_full = models_by_user[uid][\"baseline_markov\"]\n",
    "    # but easiest: re-run using stored artifacts\n",
    "    if best[\"artifact\"][\"type\"] == \"markov_user\":\n",
    "        thr = best[\"artifact\"][\"thr\"]\n",
    "        probs = best[\"artifact\"][\"probs\"]\n",
    "        p = markov_proba_user(probs, te)\n",
    "        pred = (p >= thr).astype(int)\n",
    "    else:\n",
    "        a = best[\"artifact\"][\"alpha\"]\n",
    "        thr = best[\"artifact\"][\"thr\"]\n",
    "        probs = best[\"artifact\"][\"markov\"][\"probs\"]\n",
    "        pipe = best[\"artifact\"][\"ml\"][\"pipe\"]\n",
    "        p_mk = markov_proba_user(probs, te)\n",
    "        p_ml = pipe.predict_proba(te[feature_cols])[:, 1]\n",
    "        p_bl = a * p_ml + (1.0 - a) * p_mk\n",
    "        pred = (p_bl >= thr).astype(int)\n",
    "\n",
    "    all_y.append(y)\n",
    "    all_pred.append(pred)\n",
    "\n",
    "all_y = np.concatenate(all_y)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "pooled = eval_bin(all_y, all_pred)\n",
    "\n",
    "print(\"\\n=== PERSONALIZED POOLED TEST (all users combined) ===\")\n",
    "print(pooled)\n",
    "\n",
    "# =========================\n",
    "# Save\n",
    "# =========================\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"type\": \"true_personalized\",\n",
    "        \"users\": users,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"window\": WINDOW,\n",
    "        \"test_len\": TEST_LEN,\n",
    "        \"val_windows\": VAL_WINDOWS,\n",
    "        \"thresholds\": THRESHOLDS.tolist(),\n",
    "        \"behavior_cols\": BEHAVIOR_COLS,\n",
    "        \"models_by_user\": models_by_user,\n",
    "        \"meta\": {\"true_global\": False, \"true_personalized\": True, \"target\": \"y_bin=(stress_level_pred>=1)\"}\n",
    "    },\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(\"\\nSaved:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899fc5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Users: [1, 2, 3, 4, 5]\n",
      "Rows: 260 | Binary dist: {1: 146, 0: 114}\n",
      "WINDOW: 3 | TEST_LEN: 12 | USE_USER_ID_FEATURE: False\n",
      "CV windows: [(10, 20), (15, 25)] | Tune thr per user: True\n",
      "\n",
      "=== SPLIT ===\n",
      "Total TrainPool: 200 | Total Test: 60\n",
      "\n",
      "=== BASELINE L1: Persistence (per user) ===\n",
      "TEST pooled: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328} | TEST macro(user): {'acc': 0.7167, 'f1': 0.72}\n",
      "\n",
      "=== BASELINE L2: Markov USER(prev_high, dow) ===\n",
      "Best thr: 0.05 | CV pooled F1: 0.9189\n",
      "TEST pooled: {'acc': 0.6333333333333333, 'f1': 0.7755102040816326} | TEST macro(user): {'acc': 0.6333, 'f1': 0.7642}\n",
      "\n",
      "=== PERSONALIZED ML: TRAIN + TUNE (safe) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akbar\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\svm\\_base.py:1258: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERSONALIZED LEADERBOARD ===\n",
      "Baseline-Persist | TEST pooled f1=0.7671 acc=0.7167 | macro(user) f1=0.7200 acc=0.7167\n",
      "Baseline-Markov  | CV pooled f1=0.9189 thr=0.05 | TEST pooled f1=0.7755 acc=0.6333 | macro(user) f1=0.7642 acc=0.6333\n",
      "BaggingTree            | CV score=0.9241 thr=per-user | TEST pooled f1=0.8675 acc=0.8167 | macro(user) f1=0.8373 acc=0.8167 | params={'clf__estimator__max_depth': 2, 'clf__estimator__min_samples_leaf': 1, 'clf__n_estimators': 100}\n",
      "GradBoost              | CV score=0.8948 thr=per-user | TEST pooled f1=0.8636 acc=0.8000 | macro(user) f1=0.8506 acc=0.8000 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__n_estimators': 100}\n",
      "ExtraTrees             | CV score=0.9187 thr=per-user | TEST pooled f1=0.8571 acc=0.8000 | macro(user) f1=0.8263 acc=0.8000 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__n_estimators': 200}\n",
      "RandomForest           | CV score=0.9298 thr=per-user | TEST pooled f1=0.8000 acc=0.7333 | macro(user) f1=0.7744 acc=0.7333 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "DecisionTree           | CV score=0.8986 thr=per-user | TEST pooled f1=0.7952 acc=0.7167 | macro(user) f1=0.7796 acc=0.7167 | params={'clf__max_depth': 2, 'clf__min_samples_leaf': 8}\n",
      "LogReg                 | CV score=0.9123 thr=per-user | TEST pooled f1=0.7912 acc=0.6833 | macro(user) f1=0.7666 acc=0.6833 | params={'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "AdaBoost               | CV score=0.9298 thr=per-user | TEST pooled f1=0.7826 acc=0.6667 | macro(user) f1=0.7557 acc=0.6667 | params={'clf__learning_rate': 0.03, 'clf__n_estimators': 50}\n",
      "HistGB                 | CV score=0.8986 thr=per-user | TEST pooled f1=0.7755 acc=0.6333 | macro(user) f1=0.7642 acc=0.6333 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "LinearSVC_Calibrated_SAFE | CV score=0.9235 thr=per-user | TEST pooled f1=0.7674 acc=0.6667 | macro(user) f1=0.6466 acc=0.6667 | params={'C': 1.0, 'calibration_cv': 'adaptive<=3'}\n",
      "\n",
      "✅ SELECTED BEST PERSONALIZED: BaggingTree\n",
      "Saved: ..\\models\\personalized_forecast_best.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# PERSONALIZED_FORECAST (TRUE Personalized, per-user model) - 1 CELL (SVM Calibration FIXED)\n",
    "#\n",
    "# Fix utama:\n",
    "# ✅ SVM calibrated pakai cv adaptif per-fold: cv_k = min(3, min_class_count_in_train_fold)\n",
    "# ✅ Kalau cv_k < 2 -> skip fold\n",
    "# ✅ Kalau banyak fold invalid / ada user tidak bisa -> skip kandidat SVM seluruhnya (biar konsisten)\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier, AdaBoostClassifier,\n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"/mnt/data/global_dataset_pred.csv\"),\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek path DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/personalized_forecast_best.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"user_id\"\n",
    "TARGET_COL = \"stress_level_pred\"\n",
    "\n",
    "WINDOW = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "VAL_WINDOWS = [(10, 20), (15, 25)]\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "USE_USER_ID_FEATURE = False\n",
    "\n",
    "TUNE_THRESHOLD_PER_USER = True\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "def per_user_macro_metrics(per_user_records):\n",
    "    f1s, accs = [], []\n",
    "    for r in per_user_records:\n",
    "        f1s.append(f1_score(r[\"y\"], r[\"pred\"], zero_division=0))\n",
    "        accs.append(accuracy_score(r[\"y\"], r[\"pred\"]))\n",
    "    return float(np.mean(accs)), float(np.mean(f1s))\n",
    "\n",
    "def cv_folds_user(tp_df):\n",
    "    folds = []\n",
    "    for (v0, v1) in VAL_WINDOWS:\n",
    "        if len(tp_df) < v1:\n",
    "            continue\n",
    "        tr = tp_df.iloc[:v0]\n",
    "        va = tp_df.iloc[v0:v1]\n",
    "        folds.append((tr, va))\n",
    "    return folds\n",
    "\n",
    "def min_class_count(y):\n",
    "    vc = pd.Series(y).value_counts()\n",
    "    if len(vc) < 2:\n",
    "        return 0\n",
    "    return int(vc.min())\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n",
    "    \"count_high\", \"count_low\",\n",
    "    \"streak_high\", \"transitions\"\n",
    "]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    feature_cols = [USER_COL] + feature_cols\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "users = sorted(feat[USER_COL].unique().tolist())\n",
    "\n",
    "print(\"=== DATASET ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Users:\", users)\n",
    "print(\"Rows:\", len(feat), \"| Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN, \"| USE_USER_ID_FEATURE:\", USE_USER_ID_FEATURE)\n",
    "print(\"CV windows:\", VAL_WINDOWS, \"| Tune thr per user:\", TUNE_THRESHOLD_PER_USER)\n",
    "\n",
    "# =========================\n",
    "# 2) Split per user (time-based)\n",
    "# =========================\n",
    "per_user = {}\n",
    "for uid in users:\n",
    "    g = feat[feat[USER_COL] == uid].sort_values(DATE_COL).reset_index(drop=True)\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 10:\n",
    "        raise ValueError(f\"User {uid}: data terlalu sedikit untuk split (n={n}).\")\n",
    "    per_user[uid] = {\n",
    "        \"train_pool\": g.iloc[:test_start].copy(),\n",
    "        \"test\": g.iloc[test_start:].copy()\n",
    "    }\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"Total TrainPool:\", sum(len(per_user[u][\"train_pool\"]) for u in users),\n",
    "      \"| Total Test:\", sum(len(per_user[u][\"test\"]) for u in users))\n",
    "\n",
    "# =========================\n",
    "# 3) Baseline L1: Persistence\n",
    "# =========================\n",
    "per_user_records = []\n",
    "all_true, all_pred = [], []\n",
    "\n",
    "for uid in users:\n",
    "    te = per_user[uid][\"test\"]\n",
    "    pred = (te[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    y = te[\"y_bin\"].astype(int).values\n",
    "\n",
    "    per_user_records.append({\"uid\": uid, \"y\": y, \"pred\": pred})\n",
    "    all_true.append(y)\n",
    "    all_pred.append(pred)\n",
    "\n",
    "y_all = np.concatenate(all_true)\n",
    "pred_all = np.concatenate(all_pred)\n",
    "\n",
    "persist_pooled = eval_bin(y_all, pred_all)\n",
    "persist_macro_acc, persist_macro_f1 = per_user_macro_metrics(per_user_records)\n",
    "\n",
    "print(\"\\n=== BASELINE L1: Persistence (per user) ===\")\n",
    "print(\"TEST pooled:\", persist_pooled, \"| TEST macro(user):\", {\"acc\": round(persist_macro_acc,4), \"f1\": round(persist_macro_f1,4)})\n",
    "\n",
    "# =========================\n",
    "# 4) Baseline L2: Markov USER(prev_high, dow)\n",
    "# =========================\n",
    "def train_markov_one_user(df_train):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = (df_train[\"dow\"]).astype(int).values\n",
    "    yb   = (df_train[\"y_bin\"]).astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)\n",
    "    return probs\n",
    "\n",
    "def markov_proba_user(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = (df_eval[\"dow\"]).astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "\n",
    "cv_true, cv_phigh = [], []\n",
    "for uid in users:\n",
    "    tp = per_user[uid][\"train_pool\"]\n",
    "    folds = cv_folds_user(tp)\n",
    "    for tr_df, va_df in folds:\n",
    "        probs = train_markov_one_user(tr_df)\n",
    "        p = markov_proba_user(probs, va_df)\n",
    "        cv_true.append(va_df[\"y_bin\"].values)\n",
    "        cv_phigh.append(p)\n",
    "\n",
    "if len(cv_true) == 0:\n",
    "    raise ValueError(\"Tidak ada CV fold yang valid. Kurangi VAL_WINDOWS / TEST_LEN / WINDOW.\")\n",
    "\n",
    "cv_true = np.concatenate(cv_true)\n",
    "cv_phigh = np.concatenate(cv_phigh)\n",
    "\n",
    "thr_mk, cv_f1_mk = tune_thr_from_proba(cv_true, cv_phigh)\n",
    "\n",
    "mk_models = {}\n",
    "per_user_records = []\n",
    "all_true, all_pred = [], []\n",
    "for uid in users:\n",
    "    tp = per_user[uid][\"train_pool\"]\n",
    "    te = per_user[uid][\"test\"]\n",
    "\n",
    "    probs = train_markov_one_user(tp)\n",
    "    mk_models[uid] = probs\n",
    "\n",
    "    p = markov_proba_user(probs, te)\n",
    "    pred = (p >= thr_mk).astype(int)\n",
    "    y = te[\"y_bin\"].astype(int).values\n",
    "\n",
    "    per_user_records.append({\"uid\": uid, \"y\": y, \"pred\": pred})\n",
    "    all_true.append(y)\n",
    "    all_pred.append(pred)\n",
    "\n",
    "y_all = np.concatenate(all_true)\n",
    "pred_all = np.concatenate(all_pred)\n",
    "\n",
    "markov_pooled = eval_bin(y_all, pred_all)\n",
    "markov_macro_acc, markov_macro_f1 = per_user_macro_metrics(per_user_records)\n",
    "\n",
    "print(\"\\n=== BASELINE L2: Markov USER(prev_high, dow) ===\")\n",
    "print(\"Best thr:\", thr_mk, \"| CV pooled F1:\", round(cv_f1_mk, 4))\n",
    "print(\"TEST pooled:\", markov_pooled, \"| TEST macro(user):\", {\"acc\": round(markov_macro_acc,4), \"f1\": round(markov_macro_f1,4)})\n",
    "\n",
    "# =========================\n",
    "# 5) Preprocess (for ML)\n",
    "# =========================\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    cat_cols = [USER_COL] + cat_cols\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 6) Candidate models\n",
    "# =========================\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, 6, None], \"clf__min_samples_leaf\": [1, 2, 4, 8]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3],\n",
    "         \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "    \"GradBoost\": (\n",
    "        GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__n_estimators\": [100, 200, 400],\n",
    "         \"clf__max_depth\": [2, 3]}\n",
    "    ),\n",
    "    \"AdaBoost\": (\n",
    "        AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1, 0.3], \"clf__n_estimators\": [50, 100, 200, 400]}\n",
    "    ),\n",
    "    \"BaggingTree\": (\n",
    "        BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1\n",
    "        ),\n",
    "        {\"clf__n_estimators\": [50, 100, 200],\n",
    "         \"clf__estimator__max_depth\": [2, 3, 4, None],\n",
    "         \"clf__estimator__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "# SVM calibrated SAFE (per-fold adaptive cv)\n",
    "SVM_GRID = {\"C\": [0.03, 0.1, 0.3, 1.0, 3.0]}\n",
    "SVM_NAME = \"LinearSVC_Calibrated_SAFE\"\n",
    "\n",
    "# =========================\n",
    "# 7) Tuning utilities\n",
    "# =========================\n",
    "def tune_global_thr_pooled_over_all_users(pipe, params):\n",
    "    y_list, p_list = [], []\n",
    "    for uid in users:\n",
    "        tp = per_user[uid][\"train_pool\"]\n",
    "        folds = cv_folds_user(tp)\n",
    "        for tr_df, va_df in folds:\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(tr_df[feature_cols], tr_df[\"y_bin\"].astype(int))\n",
    "            p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n",
    "            y_list.append(va_df[\"y_bin\"].values)\n",
    "            p_list.append(p)\n",
    "    y_all = np.concatenate(y_list)\n",
    "    p_all = np.concatenate(p_list)\n",
    "    thr, cv_f1 = tune_thr_from_proba(y_all, p_all)\n",
    "    return float(thr), float(cv_f1)\n",
    "\n",
    "def tune_per_user_thr(pipe, params):\n",
    "    thr_by_user = {}\n",
    "    f1s = []\n",
    "    for uid in users:\n",
    "        tp = per_user[uid][\"train_pool\"]\n",
    "        folds = cv_folds_user(tp)\n",
    "        if len(folds) == 0:\n",
    "            continue\n",
    "        y_list, p_list = [], []\n",
    "        for tr_df, va_df in folds:\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(tr_df[feature_cols], tr_df[\"y_bin\"].astype(int))\n",
    "            p = pipe.predict_proba(va_df[feature_cols])[:, 1]\n",
    "            y_list.append(va_df[\"y_bin\"].values)\n",
    "            p_list.append(p)\n",
    "        y_u = np.concatenate(y_list)\n",
    "        p_u = np.concatenate(p_list)\n",
    "        thr_u, f1_u = tune_thr_from_proba(y_u, p_u)\n",
    "        thr_by_user[uid] = float(thr_u)\n",
    "        f1s.append(float(f1_u))\n",
    "    return thr_by_user, float(np.mean(f1s))\n",
    "\n",
    "def eval_personalized_models(models_by_user, thr_by_user_or_scalar):\n",
    "    per_user_records = []\n",
    "    all_true, all_pred = [], []\n",
    "    for uid in users:\n",
    "        te = per_user[uid][\"test\"]\n",
    "        y = te[\"y_bin\"].astype(int).values\n",
    "        pipe = models_by_user[uid]\n",
    "        p = pipe.predict_proba(te[feature_cols])[:, 1]\n",
    "        thr = thr_by_user_or_scalar[uid] if isinstance(thr_by_user_or_scalar, dict) else float(thr_by_user_or_scalar)\n",
    "        pred = (p >= thr).astype(int)\n",
    "\n",
    "        per_user_records.append({\"uid\": uid, \"y\": y, \"pred\": pred})\n",
    "        all_true.append(y)\n",
    "        all_pred.append(pred)\n",
    "\n",
    "    y_all = np.concatenate(all_true)\n",
    "    pred_all = np.concatenate(all_pred)\n",
    "    pooled = eval_bin(y_all, pred_all)\n",
    "    macro_acc, macro_f1 = per_user_macro_metrics(per_user_records)\n",
    "    return pooled, {\"acc\": macro_acc, \"f1\": macro_f1}\n",
    "\n",
    "# =========================\n",
    "# 8) Train + Tune all candidates (regular)\n",
    "# =========================\n",
    "print(\"\\n=== PERSONALIZED ML: TRAIN + TUNE (safe) ===\")\n",
    "rows = []\n",
    "\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    best = None\n",
    "\n",
    "    for params in ParameterGrid(grid):\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "\n",
    "        if TUNE_THRESHOLD_PER_USER:\n",
    "            thr_obj, cv_score = tune_per_user_thr(pipe, params)\n",
    "        else:\n",
    "            thr_obj, cv_score = tune_global_thr_pooled_over_all_users(pipe, params)\n",
    "\n",
    "        if (best is None) or (cv_score > best[\"cv_score\"]):\n",
    "            best = {\"params\": params, \"thr_obj\": thr_obj, \"cv_score\": cv_score}\n",
    "\n",
    "    models_by_user = {}\n",
    "    for uid in users:\n",
    "        tp = per_user[uid][\"train_pool\"]\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "        pipe.set_params(**best[\"params\"])\n",
    "        pipe.fit(tp[feature_cols], tp[\"y_bin\"].astype(int))\n",
    "        models_by_user[uid] = pipe\n",
    "\n",
    "    pooled, macro = eval_personalized_models(models_by_user, best[\"thr_obj\"])\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_score\": float(best[\"cv_score\"]),\n",
    "        \"thr_obj\": best[\"thr_obj\"],\n",
    "        \"test_pooled_f1\": float(pooled[\"f1\"]),\n",
    "        \"test_pooled_acc\": float(pooled[\"acc\"]),\n",
    "        \"test_macro_f1\": float(macro[\"f1\"]),\n",
    "        \"test_macro_acc\": float(macro[\"acc\"]),\n",
    "        \"params\": best[\"params\"],\n",
    "        \"models_by_user\": models_by_user,\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 9) SVM Calibrated SAFE (FIXED)\n",
    "#    - per fold: cv_k = min(3, min_class_count(tr_y))\n",
    "#    - if cv_k < 2 -> skip fold\n",
    "#    - if too many folds skipped OR final training impossible for any user -> skip entire candidate\n",
    "# =========================\n",
    "def svm_fit_predict_proba(tr_X, tr_y, va_X, C, cv_max=3):\n",
    "    mcc = min_class_count(tr_y)\n",
    "    cv_k = min(cv_max, mcc)\n",
    "    if cv_k < 2:\n",
    "        return None  # signal: cannot fit on this fold\n",
    "    base = LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE, C=C)\n",
    "    calib = CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=cv_k)\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", calib)])\n",
    "    pipe.fit(tr_X, tr_y)\n",
    "    return pipe.predict_proba(va_X)[:, 1]\n",
    "\n",
    "# check final-train feasibility for all users (train_pool)\n",
    "svm_possible_all_users = True\n",
    "for uid in users:\n",
    "    y_tp = per_user[uid][\"train_pool\"][\"y_bin\"].astype(int).values\n",
    "    if min_class_count(y_tp) < 2:\n",
    "        svm_possible_all_users = False\n",
    "        break\n",
    "\n",
    "if svm_possible_all_users:\n",
    "    best = None\n",
    "\n",
    "    for C in SVM_GRID[\"C\"]:\n",
    "        # collect CV preds (pooled) OR per-user tuning\n",
    "        if TUNE_THRESHOLD_PER_USER:\n",
    "            thr_by_user = {}\n",
    "            f1s = []\n",
    "            valid_users = 0\n",
    "\n",
    "            for uid in users:\n",
    "                tp = per_user[uid][\"train_pool\"]\n",
    "                folds = cv_folds_user(tp)\n",
    "                y_list_u, p_list_u = [], []\n",
    "\n",
    "                for tr_df, va_df in folds:\n",
    "                    tr_y = tr_df[\"y_bin\"].astype(int).values\n",
    "                    p = svm_fit_predict_proba(\n",
    "                        tr_df[feature_cols], tr_y,\n",
    "                        va_df[feature_cols],\n",
    "                        C=C,\n",
    "                        cv_max=3\n",
    "                    )\n",
    "                    if p is None:\n",
    "                        continue  # skip fold safely\n",
    "                    y_list_u.append(va_df[\"y_bin\"].astype(int).values)\n",
    "                    p_list_u.append(p)\n",
    "\n",
    "                if len(y_list_u) == 0:\n",
    "                    # user ini tidak punya fold valid -> SVM kandidat dianggap tidak stabil\n",
    "                    thr_by_user = None\n",
    "                    break\n",
    "\n",
    "                y_u = np.concatenate(y_list_u)\n",
    "                p_u = np.concatenate(p_list_u)\n",
    "                thr_u, f1_u = tune_thr_from_proba(y_u, p_u)\n",
    "                thr_by_user[uid] = float(thr_u)\n",
    "                f1s.append(float(f1_u))\n",
    "                valid_users += 1\n",
    "\n",
    "            if thr_by_user is None or valid_users < len(users):\n",
    "                continue  # C ini tidak valid (karena ada user tanpa fold valid)\n",
    "\n",
    "            cv_score = float(np.mean(f1s))\n",
    "            thr_obj = thr_by_user\n",
    "\n",
    "        else:\n",
    "            y_list, p_list = [], []\n",
    "            for uid in users:\n",
    "                tp = per_user[uid][\"train_pool\"]\n",
    "                folds = cv_folds_user(tp)\n",
    "                for tr_df, va_df in folds:\n",
    "                    tr_y = tr_df[\"y_bin\"].astype(int).values\n",
    "                    p = svm_fit_predict_proba(\n",
    "                        tr_df[feature_cols], tr_y,\n",
    "                        va_df[feature_cols],\n",
    "                        C=C,\n",
    "                        cv_max=3\n",
    "                    )\n",
    "                    if p is None:\n",
    "                        continue\n",
    "                    y_list.append(va_df[\"y_bin\"].astype(int).values)\n",
    "                    p_list.append(p)\n",
    "\n",
    "            if len(y_list) == 0:\n",
    "                continue\n",
    "\n",
    "            y_all = np.concatenate(y_list)\n",
    "            p_all = np.concatenate(p_list)\n",
    "            thr_obj, cv_score = tune_thr_from_proba(y_all, p_all)\n",
    "\n",
    "        if (best is None) or (cv_score > best[\"cv_score\"]):\n",
    "            best = {\"C\": C, \"thr_obj\": thr_obj, \"cv_score\": float(cv_score)}\n",
    "\n",
    "    if best is not None:\n",
    "        # final training per user (cv adaptif dari train_pool)\n",
    "        models_by_user = {}\n",
    "        ok = True\n",
    "        for uid in users:\n",
    "            tp = per_user[uid][\"train_pool\"]\n",
    "            tr_y = tp[\"y_bin\"].astype(int).values\n",
    "            mcc = min_class_count(tr_y)\n",
    "            cv_k = min(3, mcc)\n",
    "            if cv_k < 2:\n",
    "                ok = False\n",
    "                break\n",
    "            base = LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE, C=best[\"C\"])\n",
    "            calib = CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=cv_k)\n",
    "            pipe = Pipeline([(\"prep\", preprocess), (\"clf\", calib)])\n",
    "            pipe.fit(tp[feature_cols], tr_y)\n",
    "            models_by_user[uid] = pipe\n",
    "\n",
    "        if ok:\n",
    "            pooled, macro = eval_personalized_models(models_by_user, best[\"thr_obj\"])\n",
    "            rows.append({\n",
    "                \"model\": SVM_NAME,\n",
    "                \"cv_score\": float(best[\"cv_score\"]),\n",
    "                \"thr_obj\": best[\"thr_obj\"],\n",
    "                \"test_pooled_f1\": float(pooled[\"f1\"]),\n",
    "                \"test_pooled_acc\": float(pooled[\"acc\"]),\n",
    "                \"test_macro_f1\": float(macro[\"f1\"]),\n",
    "                \"test_macro_acc\": float(macro[\"acc\"]),\n",
    "                \"params\": {\"C\": best[\"C\"], \"calibration_cv\": \"adaptive<=3\"},\n",
    "                \"models_by_user\": models_by_user,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"\\n[INFO] Skip {SVM_NAME}: final training tidak feasible untuk semua user (kelas minoritas terlalu sedikit).\")\n",
    "    else:\n",
    "        print(f\"\\n[INFO] Skip {SVM_NAME}: tidak ada setting C yang valid untuk semua user/fold (karena fold kelas minoritas kecil).\")\n",
    "else:\n",
    "    print(f\"\\n[INFO] Skip {SVM_NAME}: ada user train_pool tidak punya dua kelas, SVM calibrated tidak mungkin.\")\n",
    "\n",
    "# =========================\n",
    "# 10) Leaderboard + choose best vs Markov\n",
    "# =========================\n",
    "print(\"\\n=== PERSONALIZED LEADERBOARD ===\")\n",
    "print(f\"Baseline-Persist | TEST pooled f1={persist_pooled['f1']:.4f} acc={persist_pooled['acc']:.4f} | macro(user) f1={persist_macro_f1:.4f} acc={persist_macro_acc:.4f}\")\n",
    "print(f\"Baseline-Markov  | CV pooled f1={cv_f1_mk:.4f} thr={thr_mk:.2f} | TEST pooled f1={markov_pooled['f1']:.4f} acc={markov_pooled['acc']:.4f} | macro(user) f1={markov_macro_f1:.4f} acc={markov_macro_acc:.4f}\")\n",
    "\n",
    "rows_sorted = sorted(rows, key=lambda r: r[\"test_pooled_f1\"], reverse=True)\n",
    "for r in rows_sorted:\n",
    "    thr_desc = \"per-user\" if isinstance(r[\"thr_obj\"], dict) else f\"{r['thr_obj']:.2f}\"\n",
    "    print(f\"{r['model']:<22} | CV score={r['cv_score']:.4f} thr={thr_desc:<8} | \"\n",
    "          f\"TEST pooled f1={r['test_pooled_f1']:.4f} acc={r['test_pooled_acc']:.4f} | \"\n",
    "          f\"macro(user) f1={r['test_macro_f1']:.4f} acc={r['test_macro_acc']:.4f} | params={r['params']}\")\n",
    "\n",
    "best_name = \"MarkovUser\"\n",
    "best_obj = {\"type\": \"markov_user\", \"thr\": float(thr_mk), \"probs_by_user\": mk_models}\n",
    "best_test_pooled_f1 = float(markov_pooled[\"f1\"])\n",
    "\n",
    "if len(rows_sorted) > 0 and rows_sorted[0][\"test_pooled_f1\"] > best_test_pooled_f1:\n",
    "    top = rows_sorted[0]\n",
    "    best_name = top[\"model\"]\n",
    "    best_obj = {\n",
    "        \"type\": \"personalized_sklearn\",\n",
    "        \"models_by_user\": top[\"models_by_user\"],\n",
    "        \"thr\": top[\"thr_obj\"],\n",
    "        \"meta\": {\"tune_threshold_per_user\": TUNE_THRESHOLD_PER_USER}\n",
    "    }\n",
    "\n",
    "print(\"\\n✅ SELECTED BEST PERSONALIZED:\", best_name)\n",
    "if best_name == \"MarkovUser\":\n",
    "    print(\"Reason: baseline Markov masih best pada TEST pooled F1 (data per-user kecil -> Markov sering lebih stabil).\")\n",
    "\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"best_name\": best_name,\n",
    "        \"artifact\": best_obj,\n",
    "        \"meta\": {\n",
    "            \"target\": \"y_bin = (stress_level_pred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"users\": users,\n",
    "            \"baseline_l1\": \"persistence(per-user)\",\n",
    "            \"baseline_l2\": \"markov_user(prev_high, dow)\",\n",
    "            \"use_user_id_feature\": USE_USER_ID_FEATURE,\n",
    "            \"tune_threshold_per_user\": TUNE_THRESHOLD_PER_USER,\n",
    "        }\n",
    "    },\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(\"Saved:\", MODEL_OUT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}