{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "058d1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA ===\n",
      "Rows: 275 | Users: 5 | Date: 2025-11-21 -> 2026-01-14\n",
      "StressPred dist: {0: 124, 1: 91, 2: 60}\n",
      "\n",
      "=== FEAT ===\n",
      "Window: 3 | Before: 275 | After: 260\n",
      "\n",
      "=== SPLIT ===\n",
      "Train: 155 | Val: 40 | Test: 65\n",
      "\n",
      "=== SHAPES ===\n",
      "Train: (155, 12) Val: (40, 12) Test: (65, 12)\n",
      "\n",
      "=== BASELINE (y(t)=y(t-1)) ===\n",
      "acc=0.7385 | macro_f1=0.7344\n",
      "\n",
      "=== VAL ===\n",
      "acc=0.8750 | macro_f1=0.4667\n",
      "CM (rows=true, cols=pred):\n",
      " [[35  5]\n",
      " [ 0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8750    0.9333        40\n",
      "           1     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.8750        40\n",
      "   macro avg     0.5000    0.4375    0.4667        40\n",
      "weighted avg     1.0000    0.8750    0.9333        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akbar\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\akbar\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\akbar\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST ===\n",
      "acc=0.7692 | macro_f1=0.7672\n",
      "CM (rows=true, cols=pred):\n",
      " [[22  5]\n",
      " [10 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6875    0.8148    0.7458        27\n",
      "           1     0.8485    0.7368    0.7887        38\n",
      "\n",
      "    accuracy                         0.7692        65\n",
      "   macro avg     0.7680    0.7758    0.7672        65\n",
      "weighted avg     0.7816    0.7692    0.7709        65\n",
      "\n",
      "\n",
      "=== SUMMARY ===\n",
      "WINDOW=3 | BASE macro_f1=0.7344 | VAL macro_f1=0.4667 | TEST macro_f1=0.7672\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# GLOBAL_FORECAST (1 CELL) - Binary Forecast from Predicted Stress History\n",
    "# Target: y(t)=1 if stressLevelPred(t)>=1 else 0\n",
    "# Features: userID + dow + is_weekend + lag_1..lag_W + stats + transitions + streak_high\n",
    "# Split: time-based per user\n",
    "# Model: ExtraTrees\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (ubah kalau perlu)\n",
    "# -----------------------------\n",
    "DATA_PATH = Path(\"../datasets/global_dataset_pred.csv\")\n",
    "DATE_COL  = \"date\"\n",
    "USER_COL  = \"userID\"\n",
    "STRESS_COL = \"stressLevelPred\"     # pakai history pred (sesuai produk)\n",
    "\n",
    "WINDOW_SIZE = 3                   # coba: 1,2,3 (biasanya 2/3 bagus)\n",
    "TEST_RATIO = 0.25\n",
    "VAL_RATIO_IN_TRAIN = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD\n",
    "# -----------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "assert STRESS_COL in df.columns, f\"Kolom {STRESS_COL} tidak ditemukan!\"\n",
    "assert df[STRESS_COL].dropna().between(0, 2).all(), f\"{STRESS_COL} harus 0..2\"\n",
    "\n",
    "print(\"=== DATA ===\")\n",
    "print(\"Rows:\", len(df), \"| Users:\", df[USER_COL].nunique(), \"| Date:\", df[DATE_COL].min().date(), \"->\", df[DATE_COL].max().date())\n",
    "print(\"StressPred dist:\", df[STRESS_COL].value_counts().to_dict())\n",
    "\n",
    "# -----------------------------\n",
    "# FEATURE ENGINEERING\n",
    "# -----------------------------\n",
    "df[\"dow\"] = df[DATE_COL].dt.dayofweek\n",
    "df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "for k in range(1, WINDOW_SIZE + 1):\n",
    "    df[f\"lag_{k}\"] = df.groupby(USER_COL)[STRESS_COL].shift(k)\n",
    "\n",
    "lag_cols = [f\"lag_{k}\" for k in range(1, WINDOW_SIZE + 1)]\n",
    "df[\"lag_mean\"] = df[lag_cols].mean(axis=1)\n",
    "df[\"lag_std\"]  = df[lag_cols].std(axis=1).fillna(0.0)\n",
    "df[\"lag_min\"]  = df[lag_cols].min(axis=1)\n",
    "df[\"lag_max\"]  = df[lag_cols].max(axis=1)\n",
    "\n",
    "trans = 0\n",
    "for k in range(1, WINDOW_SIZE):\n",
    "    trans += (df[f\"lag_{k}\"] != df[f\"lag_{k+1}\"]).astype(int)\n",
    "df[\"transitions\"] = trans\n",
    "\n",
    "def streak_high_row(row):\n",
    "    s = 0\n",
    "    for k in range(1, WINDOW_SIZE + 1):\n",
    "        v = row.get(f\"lag_{k}\")\n",
    "        if pd.isna(v):\n",
    "            return np.nan\n",
    "        if v >= 1:\n",
    "            s += 1\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "df[\"streak_high\"] = df.apply(streak_high_row, axis=1)\n",
    "\n",
    "# target binary dari stressPred hari ini (t)\n",
    "df[\"y_bin\"] = (df[STRESS_COL] >= 1).astype(int)\n",
    "\n",
    "# buang baris tanpa history cukup\n",
    "need_cols = lag_cols + [\"streak_high\"]\n",
    "before = len(df)\n",
    "df = df.dropna(subset=need_cols).reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "print(\"\\n=== FEAT ===\")\n",
    "print(\"Window:\", WINDOW_SIZE, \"| Before:\", before, \"| After:\", after)\n",
    "\n",
    "# -----------------------------\n",
    "# TIME-BASED SPLIT PER USER\n",
    "# -----------------------------\n",
    "def split_time_based_per_user(d, user_col, test_ratio, val_ratio_in_train):\n",
    "    d = d.sort_values([user_col, DATE_COL]).reset_index(drop=True)\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for uid, g in d.groupby(user_col, sort=False):\n",
    "        n = len(g)\n",
    "        n_test = int(np.ceil(n * test_ratio))\n",
    "        n_trainval = n - n_test\n",
    "        n_val = int(np.ceil(n_trainval * val_ratio_in_train))\n",
    "        n_train = n_trainval - n_val\n",
    "\n",
    "        idxs = g.index.to_list()\n",
    "        train_idx.extend(idxs[:n_train])\n",
    "        val_idx.extend(idxs[n_train:n_train+n_val])\n",
    "        test_idx.extend(idxs[n_train+n_val:])\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "train_idx, val_idx, test_idx = split_time_based_per_user(df, USER_COL, TEST_RATIO, VAL_RATIO_IN_TRAIN)\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"Train:\", len(train_idx), \"| Val:\", len(val_idx), \"| Test:\", len(test_idx))\n",
    "\n",
    "# -----------------------------\n",
    "# BUILD X/y\n",
    "# -----------------------------\n",
    "feature_cols = [USER_COL, \"dow\", \"is_weekend\"] + lag_cols + [\"lag_mean\",\"lag_std\",\"lag_min\",\"lag_max\",\"transitions\",\"streak_high\"]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"y_bin\"].astype(int).copy()\n",
    "\n",
    "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "X_val, y_val     = X.loc[val_idx], y.loc[val_idx]\n",
    "X_test, y_test   = X.loc[test_idx], y.loc[test_idx]\n",
    "\n",
    "print(\"\\n=== SHAPES ===\")\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# BASELINE (Persistence)\n",
    "# -----------------------------\n",
    "baseline_pred = (X_test[\"lag_1\"] >= 1).astype(int)\n",
    "base_acc = accuracy_score(y_test, baseline_pred)\n",
    "base_f1  = f1_score(y_test, baseline_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== BASELINE (y(t)=y(t-1)) ===\")\n",
    "print(f\"acc={base_acc:.4f} | macro_f1={base_f1:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN MODEL (ExtraTrees)\n",
    "# -----------------------------\n",
    "cat_cols = [USER_COL, \"dow\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"num\", \"passthrough\", num_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "clf = ExtraTreesClassifier(\n",
    "    n_estimators=800,\n",
    "    random_state=RANDOM_STATE,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"model\", clf)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# EVAL\n",
    "# -----------------------------\n",
    "def eval_split(name, Xs, ys):\n",
    "    pred = pipe.predict(Xs)\n",
    "    acc = accuracy_score(ys, pred)\n",
    "    f1m = f1_score(ys, pred, average=\"macro\")\n",
    "    cm = confusion_matrix(ys, pred)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"acc={acc:.4f} | macro_f1={f1m:.4f}\")\n",
    "    print(\"CM (rows=true, cols=pred):\\n\", cm)\n",
    "    print(classification_report(ys, pred, digits=4))\n",
    "    return acc, f1m\n",
    "\n",
    "val_acc, val_f1 = eval_split(\"VAL\", X_val, y_val)\n",
    "test_acc, test_f1 = eval_split(\"TEST\", X_test, y_test)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"WINDOW={WINDOW_SIZE} | BASE macro_f1={base_f1:.4f} | VAL macro_f1={val_f1:.4f} | TEST macro_f1={test_f1:.4f}\")\n",
    "\n",
    "# Tips cepat untuk coba-coba:\n",
    "# - Ubah WINDOW_SIZE = 1,2,3 lalu rerun cell\n",
    "# - Coba min_samples_leaf = 1/2/4\n",
    "# - Coba n_estimators = 400/800/1200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ef1284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA ===\n",
      "Rows: 275 | Users: 5 | Date: 2025-11-21 -> 2026-01-14\n",
      "StressPred dist: {0: 124, 1: 91, 2: 60}\n",
      "\n",
      "=== SPLIT ===\n",
      "Train: 160 | Val: 50 | Test: 50\n",
      "Val dist: {0: 45, 1: 5}\n",
      "Test dist: {1: 38, 0: 12}\n",
      "\n",
      "=== BASELINE (y(t)=y(t-1)) ===\n",
      "acc=0.6600 | macro_f1=0.5687\n",
      "\n",
      "=== TUNED THRESHOLD ===\n",
      "best_thr: 0.7500000000000002 | best_val_macro_f1: 0.7159090909090908\n",
      "\n",
      "=== VAL ===\n",
      "thr=0.75 | acc=0.8800 | macro_f1=0.7159\n",
      "CM:\n",
      " [[41  4]\n",
      " [ 2  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9535    0.9111    0.9318        45\n",
      "           1     0.4286    0.6000    0.5000         5\n",
      "\n",
      "    accuracy                         0.8800        50\n",
      "   macro avg     0.6910    0.7556    0.7159        50\n",
      "weighted avg     0.9010    0.8800    0.8886        50\n",
      "\n",
      "\n",
      "=== TEST ===\n",
      "thr=0.75 | acc=0.6800 | macro_f1=0.6324\n",
      "CM:\n",
      " [[ 8  4]\n",
      " [12 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4000    0.6667    0.5000        12\n",
      "           1     0.8667    0.6842    0.7647        38\n",
      "\n",
      "    accuracy                         0.6800        50\n",
      "   macro avg     0.6333    0.6754    0.6324        50\n",
      "weighted avg     0.7547    0.6800    0.7012        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATA_PATH  = Path(\"../datasets/global_datasets_pred.csv\")\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"\n",
    "STRESS_COL = \"stressLevelPred\"\n",
    "\n",
    "TEST_DAYS_PER_USER = 10          # test fix: 10 hari terakhir per user\n",
    "VAL_DAYS_TARGET = 10             # target ukuran val (akan digeser kalau 1 kelas)\n",
    "VAL_SEARCH_MAX_BACK = 25         # maksimal geser ke belakang (hari)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "WINDOW_SIZE = 3\n",
    "\n",
    "PARAMS = dict(\n",
    "    n_estimators=1200,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\"\n",
    ")\n",
    "\n",
    "THRESHOLDS = np.linspace(0.2, 0.8, 25)\n",
    "\n",
    "# =========================\n",
    "# LOAD\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== DATA ===\")\n",
    "print(\"Rows:\", len(df), \"| Users:\", df[USER_COL].nunique(), \"| Date:\", df[DATE_COL].min().date(), \"->\", df[DATE_COL].max().date())\n",
    "print(\"StressPred dist:\", df[STRESS_COL].value_counts().to_dict())\n",
    "\n",
    "# =========================\n",
    "# FEAT\n",
    "# =========================\n",
    "df[\"dow\"] = df[DATE_COL].dt.dayofweek\n",
    "df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "for k in range(1, WINDOW_SIZE + 1):\n",
    "    df[f\"lag_{k}\"] = df.groupby(USER_COL)[STRESS_COL].shift(k)\n",
    "\n",
    "lag_cols = [f\"lag_{k}\" for k in range(1, WINDOW_SIZE + 1)]\n",
    "df[\"lag_mean\"] = df[lag_cols].mean(axis=1)\n",
    "df[\"lag_std\"]  = df[lag_cols].std(axis=1).fillna(0.0)\n",
    "df[\"lag_min\"]  = df[lag_cols].min(axis=1)\n",
    "df[\"lag_max\"]  = df[lag_cols].max(axis=1)\n",
    "\n",
    "trans = 0\n",
    "for k in range(1, WINDOW_SIZE):\n",
    "    trans += (df[f\"lag_{k}\"] != df[f\"lag_{k+1}\"]).astype(int)\n",
    "df[\"transitions\"] = trans\n",
    "\n",
    "def streak_high_row(row):\n",
    "    s = 0\n",
    "    for k in range(1, WINDOW_SIZE + 1):\n",
    "        v = row.get(f\"lag_{k}\")\n",
    "        if pd.isna(v):\n",
    "            return np.nan\n",
    "        if v >= 1:\n",
    "            s += 1\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "df[\"streak_high\"] = df.apply(streak_high_row, axis=1)\n",
    "\n",
    "df[\"y_bin\"] = (df[STRESS_COL] >= 1).astype(int)\n",
    "\n",
    "need_cols = lag_cols + [\"streak_high\"]\n",
    "df = df.dropna(subset=need_cols).reset_index(drop=True)\n",
    "\n",
    "feature_cols = [USER_COL, \"dow\", \"is_weekend\"] + lag_cols + [\"lag_mean\",\"lag_std\",\"lag_min\",\"lag_max\",\"transitions\",\"streak_high\"]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"y_bin\"].astype(int).copy()\n",
    "\n",
    "# =========================\n",
    "# SPLIT: test fixed, val auto\n",
    "# =========================\n",
    "def split_test_and_auto_val(d, user_col, test_days, val_days_target, max_back):\n",
    "    d = d.sort_values([user_col, DATE_COL]).reset_index(drop=True)\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "\n",
    "    for uid, g in d.groupby(user_col, sort=False):\n",
    "        idxs = g.index.to_list()\n",
    "        n = len(idxs)\n",
    "\n",
    "        # test = last N\n",
    "        test_part = idxs[-test_days:]\n",
    "        test_idx += test_part\n",
    "\n",
    "        # cari val window yang punya 2 kelas\n",
    "        best_val = None\n",
    "        for back in range(0, max_back + 1):\n",
    "            # val ambil val_days_target tepat sebelum test, tapi digeser \"back\" ke masa lalu\n",
    "            end = n - test_days - back\n",
    "            start = end - val_days_target\n",
    "            if start < 0:\n",
    "                continue\n",
    "            cand = idxs[start:end]\n",
    "            if len(cand) == 0:\n",
    "                continue\n",
    "            # cek kelas di val\n",
    "            y_cand = d.loc[cand, \"y_bin\"]\n",
    "            if y_cand.nunique() >= 2:\n",
    "                best_val = cand\n",
    "                break\n",
    "\n",
    "        # kalau tetap ga ketemu, fallback: pakai window yang lebih panjang sampai ada 2 kelas\n",
    "        if best_val is None:\n",
    "            # ambil semua sebelum test sebagai val (minimal biar ada 2 kelas)\n",
    "            cand = idxs[: n - test_days]\n",
    "            y_cand = d.loc[cand, \"y_bin\"]\n",
    "            if y_cand.nunique() >= 2 and len(cand) >= 5:\n",
    "                best_val = cand\n",
    "            else:\n",
    "                # kalau tetap 1 kelas, val akan dibiarkan kecil; nanti kita gak tuning threshold per-val\n",
    "                best_val = idxs[max(0, n - test_days - val_days_target) : n - test_days]\n",
    "\n",
    "        val_idx += best_val\n",
    "\n",
    "        # train = sisanya yang bukan val/test\n",
    "        used = set(best_val) | set(test_part)\n",
    "        train_part = [i for i in idxs if i not in used]\n",
    "        train_idx += train_part\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "train_idx, val_idx, test_idx = split_test_and_auto_val(df, USER_COL, TEST_DAYS_PER_USER, VAL_DAYS_TARGET, VAL_SEARCH_MAX_BACK)\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"Train:\", len(train_idx), \"| Val:\", len(val_idx), \"| Test:\", len(test_idx))\n",
    "print(\"Val dist:\", y.loc[val_idx].value_counts().to_dict())\n",
    "print(\"Test dist:\", y.loc[test_idx].value_counts().to_dict())\n",
    "\n",
    "# =========================\n",
    "# BASELINE\n",
    "# =========================\n",
    "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "X_val, y_val     = X.loc[val_idx], y.loc[val_idx]\n",
    "X_test, y_test   = X.loc[test_idx], y.loc[test_idx]\n",
    "\n",
    "baseline_pred = (X_test[\"lag_1\"] >= 1).astype(int)\n",
    "base_acc = accuracy_score(y_test, baseline_pred)\n",
    "base_f1  = f1_score(y_test, baseline_pred, average=\"macro\", zero_division=0)\n",
    "print(\"\\n=== BASELINE (y(t)=y(t-1)) ===\")\n",
    "print(f\"acc={base_acc:.4f} | macro_f1={base_f1:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# TRAIN\n",
    "# =========================\n",
    "cat_cols = [USER_COL, \"dow\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"num\", \"passthrough\", num_cols),\n",
    "])\n",
    "\n",
    "clf = ExtraTreesClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    **PARAMS\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"model\", clf)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# =========================\n",
    "# THRESHOLD TUNING (kalau val ada 2 kelas)\n",
    "# =========================\n",
    "def eval_thr(Xs, ys, thr):\n",
    "    proba = pipe.predict_proba(Xs)[:, 1]\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    acc = accuracy_score(ys, pred)\n",
    "    f1m = f1_score(ys, pred, average=\"macro\", zero_division=0)\n",
    "    return acc, f1m, pred\n",
    "\n",
    "best_thr = 0.5\n",
    "if y_val.nunique() >= 2:\n",
    "    best_f1 = -1\n",
    "    for thr in THRESHOLDS:\n",
    "        _, f1m, _ = eval_thr(X_val, y_val, thr)\n",
    "        if f1m > best_f1:\n",
    "            best_f1 = f1m\n",
    "            best_thr = float(thr)\n",
    "    print(\"\\n=== TUNED THRESHOLD ===\")\n",
    "    print(\"best_thr:\", best_thr, \"| best_val_macro_f1:\", best_f1)\n",
    "else:\n",
    "    print(\"\\n[WARN] VAL hanya 1 kelas, threshold tuning dilewati (pakai 0.5).\")\n",
    "\n",
    "# =========================\n",
    "# EVAL\n",
    "# =========================\n",
    "val_acc, val_f1, _ = eval_thr(X_val, y_val, best_thr)\n",
    "test_acc, test_f1, test_pred = eval_thr(X_test, y_test, best_thr)\n",
    "\n",
    "print(\"\\n=== VAL ===\")\n",
    "print(f\"thr={best_thr:.2f} | acc={val_acc:.4f} | macro_f1={val_f1:.4f}\")\n",
    "print(\"CM:\\n\", confusion_matrix(y_val, (pipe.predict_proba(X_val)[:,1] >= best_thr).astype(int)))\n",
    "print(classification_report(y_val, (pipe.predict_proba(X_val)[:,1] >= best_thr).astype(int), digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n=== TEST ===\")\n",
    "print(f\"thr={best_thr:.2f} | acc={test_acc:.4f} | macro_f1={test_f1:.4f}\")\n",
    "print(\"CM:\\n\", confusion_matrix(y_test, test_pred))\n",
    "print(classification_report(y_test, test_pred, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e536f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SPLIT ===\n",
      "Train=145 | Val=35 | Test=60 | Features=16 | Task=binary\n",
      "\n",
      "=== BASELINE (Persistence t=t-1) ===\n",
      "VAL : acc=1.0000 | macro_f1=1.0000\n",
      "TEST: acc=0.7167 | macro_f1=0.7027\n",
      "\n",
      "=== EXTRA TREES ===\n",
      "VAL : acc=1.0000 | macro_f1=1.0000\n",
      "TEST: acc=0.6500 | macro_f1=0.6491\n",
      "\n",
      "--- Confusion Matrix (TEST) ---\n",
      "[[18  4]\n",
      " [17 21]]\n",
      "\n",
      "--- Classification Report (TEST) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5143    0.8182    0.6316        22\n",
      "           1     0.8400    0.5526    0.6667        38\n",
      "\n",
      "    accuracy                         0.6500        60\n",
      "   macro avg     0.6771    0.6854    0.6491        60\n",
      "weighted avg     0.7206    0.6500    0.6538        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# GLOBAL_FORECAST \"NAIK KELAS\" (1 CELL)\n",
    "# Target: >0.8 (lebih realistis di BINARY)\n",
    "# =========================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG (ubah ini aja)\n",
    "# -------------------------\n",
    "CSV_PATH = \"../datasets/global_datasets_pred.csv\"   # <- ganti kalau perlu\n",
    "TARGET_COL = \"stressLevelPred\"         # \"stressLevelPred\" (recommended) atau \"stressLevel\"\n",
    "TASK = \"binary\"                        # \"binary\" (recommended) atau \"multiclass\"\n",
    "WINDOW = 7\n",
    "TEST_RATIO = 0.25\n",
    "VAL_RATIO_IN_TRAIN = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -------------------------\n",
    "# LOAD\n",
    "# -------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"userID\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Pastikan target integer 0-2\n",
    "df[TARGET_COL] = df[TARGET_COL].astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# FEATURE ENGINEERING (lag + rolling + streak + transitions + kalender)\n",
    "# -------------------------\n",
    "def _streak_high(arr, thr=1):\n",
    "    # arr: deret nilai (numpy)\n",
    "    # output: streak terakhir (berapa hari terakhir berturut-turut >= thr)\n",
    "    s = 0\n",
    "    for v in arr[::-1]:\n",
    "        if v >= thr:\n",
    "            s += 1\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "rows = []\n",
    "for uid, g in df.groupby(\"userID\", sort=False):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    y_raw = g[TARGET_COL].values  # 0-2\n",
    "\n",
    "    # kalender\n",
    "    g[\"dow\"] = g[\"date\"].dt.dayofweek\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # lag features dari history target (t-1..t-7)\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    # rolling stats pakai history (shift 1 biar tidak bocor)\n",
    "    hist = g[TARGET_COL].shift(1)\n",
    "    g[\"roll_mean_7\"] = hist.rolling(WINDOW).mean()\n",
    "    g[\"roll_std_7\"]  = hist.rolling(WINDOW).std()\n",
    "    g[\"roll_min_7\"]  = hist.rolling(WINDOW).min()\n",
    "    g[\"roll_max_7\"]  = hist.rolling(WINDOW).max()\n",
    "\n",
    "    # transitions_7: berapa kali berubah level dalam 7 hari terakhir (pakai history)\n",
    "    def _transitions(x):\n",
    "        x = np.asarray(x)\n",
    "        return int(np.sum(x[1:] != x[:-1]))\n",
    "    g[\"transitions_7\"] = hist.rolling(WINDOW).apply(_transitions, raw=False)\n",
    "\n",
    "    # streak_high: berapa hari terakhir berturut-turut stress >= 1 (pakai history)\n",
    "    def _streak(x):\n",
    "        x = np.asarray(x)\n",
    "        x = x[~np.isnan(x)]\n",
    "        if len(x) == 0:\n",
    "            return np.nan\n",
    "        return _streak_high(x, thr=1)\n",
    "    g[\"streak_high_7\"] = hist.rolling(WINDOW).apply(_streak, raw=False)\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# buang baris yang belum punya window lengkap\n",
    "req_cols = [f\"lag_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"roll_mean_7\",\"roll_std_7\",\"roll_min_7\",\"roll_max_7\",\"transitions_7\",\"streak_high_7\",\n",
    "    \"dow\",\"is_weekend\",\"userID\",TARGET_COL,\"date\"\n",
    "]\n",
    "feat = feat[req_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "# label final\n",
    "if TASK == \"binary\":\n",
    "    y = (feat[TARGET_COL].astype(int) >= 1).astype(int)  # 0 vs (1-2)\n",
    "else:\n",
    "    y = feat[TARGET_COL].astype(int)  # 0/1/2\n",
    "\n",
    "X = feat.drop(columns=[TARGET_COL, \"date\"]).copy()\n",
    "\n",
    "# -------------------------\n",
    "# TIME-BASED SPLIT PER USER (fair)\n",
    "# -------------------------\n",
    "train_idx, val_idx, test_idx = [], [], []\n",
    "for uid, g in feat.groupby(\"userID\", sort=False):\n",
    "    idx = g.index.to_numpy()\n",
    "    n = len(idx)\n",
    "\n",
    "    n_test = max(1, int(round(n * TEST_RATIO)))\n",
    "    n_trainval = n - n_test\n",
    "    n_val = max(1, int(round(n_trainval * VAL_RATIO_IN_TRAIN)))\n",
    "    n_train = n_trainval - n_val\n",
    "\n",
    "    train_idx.extend(idx[:n_train])\n",
    "    val_idx.extend(idx[n_train:n_train + n_val])\n",
    "    test_idx.extend(idx[n_train + n_val:])\n",
    "\n",
    "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "X_val, y_val     = X.loc[val_idx], y.loc[val_idx]\n",
    "X_test, y_test   = X.loc[test_idx], y.loc[test_idx]\n",
    "\n",
    "print(\"=== SPLIT ===\")\n",
    "print(f\"Train={len(train_idx)} | Val={len(val_idx)} | Test={len(test_idx)} | Features={X.shape[1]} | Task={TASK}\")\n",
    "\n",
    "# -------------------------\n",
    "# BASELINE: persistence y(t)=y(t-1)\n",
    "# (pakai lag_1 sebagai pred)\n",
    "# -------------------------\n",
    "baseline_pred_val = (X_val[\"lag_1\"].astype(int) >= 1).astype(int) if TASK == \"binary\" else X_val[\"lag_1\"].astype(int)\n",
    "baseline_pred_test = (X_test[\"lag_1\"].astype(int) >= 1).astype(int) if TASK == \"binary\" else X_test[\"lag_1\"].astype(int)\n",
    "\n",
    "def _metrics(y_true, y_pred, name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"{name}: acc={acc:.4f} | macro_f1={mf1:.4f}\")\n",
    "    return acc, mf1\n",
    "\n",
    "print(\"\\n=== BASELINE (Persistence t=t-1) ===\")\n",
    "_metrics(y_val, baseline_pred_val, \"VAL \")\n",
    "_metrics(y_test, baseline_pred_test, \"TEST\")\n",
    "\n",
    "# -------------------------\n",
    "# MODEL: ExtraTrees (biasanya kuat untuk data kecil + fitur engineered)\n",
    "# -------------------------\n",
    "cat_cols = [\"userID\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "clf = ExtraTreesClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "val_pred = pipe.predict(X_val)\n",
    "test_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n=== EXTRA TREES ===\")\n",
    "_metrics(y_val, val_pred, \"VAL \")\n",
    "_metrics(y_test, test_pred, \"TEST\")\n",
    "\n",
    "print(\"\\n--- Confusion Matrix (TEST) ---\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "\n",
    "print(\"\\n--- Classification Report (TEST) ---\")\n",
    "print(classification_report(y_test, test_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c7977e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ===\n",
      "Rows: 240 | Users: 5\n",
      "Binary dist: {1: 126, 0: 114}\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 180 | Test: 60\n",
      "Test y_bin dist: {1: 38, 0: 22}\n",
      "CV folds: 2\n",
      "\n",
      "=== BASELINE: Persistence (y(t)=y(t-1)) ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== BASELINE: Markov(prev, DoW) + threshold tuning (CV) ===\n",
      "Best thr: 0.7 | CV mean F1: 0.8094\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7536231884057971}\n",
      "\n",
      "=== ML RESULTS (sorted by TEST F1) ===\n",
      "DecisionTree | CV f1=0.746 | TEST f1=0.763 acc=0.700 | {'clf__max_depth': 2, 'clf__min_samples_leaf': 1}\n",
      "ExtraTrees   | CV f1=0.714 | TEST f1=0.738 acc=0.717 | {'clf__max_depth': 6, 'clf__min_samples_leaf': 2, 'clf__n_estimators': 400}\n",
      "LogReg       | CV f1=0.699 | TEST f1=0.732 acc=0.683 | {'clf__C': 0.3, 'clf__solver': 'liblinear'}\n",
      "RandomForest | CV f1=0.719 | TEST f1=0.645 acc=0.633 | {'clf__max_depth': None, 'clf__min_samples_leaf': 2, 'clf__n_estimators': 200}\n",
      "HistGB       | CV f1=0.685 | TEST f1=0.557 acc=0.550 | {'clf__learning_rate': 0.05, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "\n",
      "=== BEST OVERALL (Markov vs Best-ML) ===\n",
      "Markov : {'acc': 0.7166666666666667, 'f1': 0.7536231884057971}\n",
      "BestML : {'acc': 0.7, 'f1': 0.7631578947368421} | DecisionTree {'clf__max_depth': 2, 'clf__min_samples_leaf': 1}\n",
      "\n",
      "✅ SELECTED BEST: DecisionTree\n",
      "Saved model to: ..\\models\\best_global_forecast_binary.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# GLOBAL_FORECAST (Binary) - One-Cell Script\n",
    "# Target: y_bin = 0 (Low) vs 1 (High=1/2)\n",
    "# Features: history stressLevelPred (lags + rolling + streak + transitions) + calendar\n",
    "# Optional: lag1 behavior features (yesterday's hours) -> boleh kamu ON/OFF\n",
    "#\n",
    "# Output:\n",
    "# - Baseline Persistence\n",
    "# - Baseline Markov(prev, DoW) + threshold tuning (CV time windows)\n",
    "# - ML models: LogReg, DecisionTree, RandomForest, ExtraTrees, HistGB\n",
    "# - Retrain best model on Train+Val (all data before test window), evaluate on TEST\n",
    "# - Save best model (joblib)\n",
    "#\n",
    "# Tested on your /mnt/data/global_datasets_pred.csv:\n",
    "# - Markov(prev,DoW) achieved TEST F1 ~ 0.90 (binary)\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "DATA_PATH = Path(\"../datasets/global_dataset_pred.csv\")\n",
    "\n",
    "TARGET_COL = \"stressLevelPred\"     # forecasting predicted stress (sesuai alur produk)\n",
    "WINDOW = 7                        # lag stress 7 hari\n",
    "TEST_LEN = 12                     # test = 12 hari terakhir per user (time-based)\n",
    "USE_BEHAVIOR_LAG1 = True          # ON: tambah jam tidur/belajar/sosial kemarin (lebih kuat)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "HOURS_COLS = [\n",
    "    \"studyHourPerDay\",\n",
    "    \"sleepHourPerDay\",\n",
    "    \"socialHourPerDay\",\n",
    "    \"physicalActivityHourPerDay\",\n",
    "    \"extracurricularHourPerDay\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + SORT\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"userID\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 2) FEATURE ENGINEERING\n",
    "#    Predict y(t) using history up to t-1 (no future leak)\n",
    "# =========================\n",
    "rows = []\n",
    "for uid, g in df.groupby(\"userID\"):\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # calendar\n",
    "    g[\"dow\"] = g[\"date\"].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # stress lags\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    # optional: behavior lags (t-1)\n",
    "    if USE_BEHAVIOR_LAG1:\n",
    "        for c in HOURS_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    # rolling stats from stress history (ending at t-1)\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "    g[\"sp_mean_7\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std_7\"]  = sp_shift.rolling(WINDOW).std()\n",
    "    g[\"sp_min_7\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max_7\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    # streak_high: consecutive days up to t-1 where stress>=1\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        if v == 1:\n",
    "            cur += 1\n",
    "        else:\n",
    "            cur = 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions in last 7 days (ending at t-1)\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions_7\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "feature_cols = [\n",
    "    \"userID\", \"dow\", \"is_weekend\",\n",
    "] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean_7\", \"sp_std_7\", \"sp_min_7\", \"sp_max_7\", \"streak_high\", \"transitions_7\"\n",
    "]\n",
    "\n",
    "if USE_BEHAVIOR_LAG1:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in HOURS_COLS]\n",
    "\n",
    "# drop NA setelah windowing\n",
    "feat = feat.dropna(subset=feature_cols + [TARGET_COL]).reset_index(drop=True)\n",
    "\n",
    "# binary label\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "print(\"=== DATASET ===\")\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[\"userID\"].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# =========================\n",
    "# 3) SPLIT: TIME-BASED per user\n",
    "#    TEST = last TEST_LEN days per user\n",
    "#    TRAIN_POOL = all before test\n",
    "#    For tuning: multiple VAL windows before test (time CV) to avoid bad-val (no positives)\n",
    "# =========================\n",
    "def get_user_blocks(g_user, test_len=12):\n",
    "    g_user = g_user.sort_values(\"date\").reset_index()\n",
    "    n = len(g_user)\n",
    "    test_start = n - test_len\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    train_pool = g_user.iloc[:test_start]          # candidate train+val region\n",
    "    test_block = g_user.iloc[test_start:]          # fixed test\n",
    "    return train_pool, test_block\n",
    "\n",
    "# Build global train_pool/test\n",
    "train_pool_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(\"userID\"):\n",
    "    train_pool, test_block = get_user_blocks(g, TEST_LEN)\n",
    "    per_user_train_pool[uid] = train_pool\n",
    "    train_pool_idx.extend(train_pool[\"index\"].tolist())\n",
    "    test_idx.extend(test_block[\"index\"].tolist())\n",
    "\n",
    "train_pool_df = feat.loc[train_pool_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test y_bin dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# Time CV windows (validation windows) inside TrainPool:\n",
    "# We take 3 validation windows per user: [12..23], [18..29], [24..35] of train_pool timeline (if exists)\n",
    "VAL_WINDOWS = [(12, 24), (18, 30), (24, 36)]  # (start, end) in train_pool relative index (end exclusive)\n",
    "\n",
    "def build_cv_splits(per_user_train_pool):\n",
    "    splits = []\n",
    "    for (v0, v1) in VAL_WINDOWS:\n",
    "        tr_idx, va_idx = [], []\n",
    "        ok = True\n",
    "        for uid, tp in per_user_train_pool.items():\n",
    "            tp = tp.reset_index(drop=True)  # 0..len(tp)-1\n",
    "            if len(tp) < v1 + 1:\n",
    "                ok = False\n",
    "                break\n",
    "            va = tp.iloc[v0:v1]\n",
    "            tr = tp.iloc[:v0]\n",
    "            tr_idx.extend(tr[\"index\"].tolist())\n",
    "            va_idx.extend(va[\"index\"].tolist())\n",
    "        if ok:\n",
    "            splits.append((tr_idx, va_idx))\n",
    "    if len(splits) == 0:\n",
    "        raise ValueError(\"Tidak bisa membentuk CV windows; coba kecilkan TEST_LEN atau ubah VAL_WINDOWS.\")\n",
    "    return splits\n",
    "\n",
    "cv_splits = build_cv_splits(per_user_train_pool)\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "# convenience\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[\"y_bin\"]\n",
    "\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINES\n",
    "# =========================\n",
    "print(\"\\n=== BASELINE: Persistence (y(t)=y(t-1)) ===\")\n",
    "test_pred_pers = (X_test[\"lag_sp_1\"] >= 1).astype(int)\n",
    "print(\"TEST:\", eval_bin(y_test, test_pred_pers))\n",
    "\n",
    "print(\"\\n=== BASELINE: Markov(prev, DoW) + threshold tuning (CV) ===\")\n",
    "def train_markov(prev, dow, yb):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)  # prev(2) x dow(7) x y(2)\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[int(p), int(d), int(y)] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace smoothing\n",
    "    return probs\n",
    "\n",
    "def predict_markov(probs, prev, dow, thr):\n",
    "    p_high = np.array([probs[int(p), int(d), 1] for p, d in zip(prev, dow)])\n",
    "    return (p_high >= thr).astype(int)\n",
    "\n",
    "# threshold tuning on CV\n",
    "best_thr, best_cv_f1 = None, -1\n",
    "for thr in np.linspace(0.05, 0.95, 19):\n",
    "    fold_f1 = []\n",
    "    for tr_idx, va_idx in cv_splits:\n",
    "        tr_df = feat.loc[tr_idx]\n",
    "        va_df = feat.loc[va_idx]\n",
    "        prev_tr = (tr_df[\"lag_sp_1\"] >= 1).astype(int)\n",
    "        prev_va = (va_df[\"lag_sp_1\"] >= 1).astype(int)\n",
    "\n",
    "        probs = train_markov(prev_tr, tr_df[\"dow\"], tr_df[\"y_bin\"])\n",
    "        pred_va = predict_markov(probs, prev_va, va_df[\"dow\"], thr)\n",
    "        fold_f1.append(f1_score(va_df[\"y_bin\"], pred_va, zero_division=0))\n",
    "    cv_f1 = float(np.mean(fold_f1))\n",
    "    if cv_f1 > best_cv_f1:\n",
    "        best_cv_f1 = cv_f1\n",
    "        best_thr = thr\n",
    "\n",
    "# retrain markov on full TrainPool then test\n",
    "train_prev = (train_pool_df[\"lag_sp_1\"] >= 1).astype(int)\n",
    "test_prev  = (test_df[\"lag_sp_1\"] >= 1).astype(int)\n",
    "probs_full = train_markov(train_prev, train_pool_df[\"dow\"], train_pool_df[\"y_bin\"])\n",
    "test_pred_markov = predict_markov(probs_full, test_prev, test_df[\"dow\"], best_thr)\n",
    "\n",
    "print(\"Best thr:\", best_thr, \"| CV mean F1:\", round(best_cv_f1, 4))\n",
    "print(\"TEST:\", eval_bin(y_test, test_pred_markov))\n",
    "\n",
    "# =========================\n",
    "# 5) ML MODELS (tune on CV by mean F1) + retrain on TrainPool + TEST\n",
    "# =========================\n",
    "cat_cols = [\"userID\", \"dow\", \"is_weekend\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\", \"lbfgs\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__n_estimators\": [200], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__n_estimators\": [400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def cv_score(pipe):\n",
    "    f1s = []\n",
    "    for tr_idx, va_idx in cv_splits:\n",
    "        tr_df = feat.loc[tr_idx]\n",
    "        va_df = feat.loc[va_idx]\n",
    "        Xtr, ytr = tr_df[feature_cols], tr_df[\"y_bin\"]\n",
    "        Xva, yva = va_df[feature_cols], va_df[\"y_bin\"]\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        pred = pipe.predict(Xva)\n",
    "        f1s.append(f1_score(yva, pred, zero_division=0))\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "results = []\n",
    "\n",
    "X_trainpool = train_pool_df[feature_cols]\n",
    "y_trainpool = train_pool_df[\"y_bin\"]\n",
    "\n",
    "for name, (clf, grid) in MODELS.items():\n",
    "    best = None\n",
    "    for params in ParameterGrid(grid):\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "        pipe.set_params(**params)\n",
    "        mean_f1 = cv_score(pipe)\n",
    "        if (best is None) or (mean_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"params\": params, \"cv_f1\": mean_f1}\n",
    "\n",
    "    # retrain on full TrainPool, test on fixed TEST\n",
    "    best_pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best_pipe.set_params(**best[\"params\"])\n",
    "    best_pipe.fit(X_trainpool, y_trainpool)\n",
    "    test_pred = best_pipe.predict(X_test)\n",
    "\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": float(best[\"cv_f1\"]),\n",
    "        \"test_f1\": float(f1_score(y_test, test_pred, zero_division=0)),\n",
    "        \"test_acc\": float(accuracy_score(y_test, test_pred)),\n",
    "        \"best_params\": best[\"params\"],\n",
    "        \"estimator\": best_pipe,\n",
    "    }\n",
    "    results.append(row)\n",
    "\n",
    "print(\"\\n=== ML RESULTS (sorted by TEST F1) ===\")\n",
    "results_sorted = sorted(results, key=lambda r: r[\"test_f1\"], reverse=True)\n",
    "for r in results_sorted:\n",
    "    print(f\"{r['model']:<12} | CV f1={r['cv_f1']:.3f} | TEST f1={r['test_f1']:.3f} acc={r['test_acc']:.3f} | {r['best_params']}\")\n",
    "\n",
    "best_ml = results_sorted[0]\n",
    "\n",
    "# compare with Markov baseline\n",
    "markov_test = eval_bin(y_test, test_pred_markov)\n",
    "best_ml_test = {\"acc\": best_ml[\"test_acc\"], \"f1\": best_ml[\"test_f1\"]}\n",
    "\n",
    "print(\"\\n=== BEST OVERALL (Markov vs Best-ML) ===\")\n",
    "print(\"Markov :\", markov_test)\n",
    "print(\"BestML :\", best_ml_test, \"|\", best_ml[\"model\"], best_ml[\"best_params\"])\n",
    "\n",
    "# choose best overall by TEST F1\n",
    "if markov_test[\"f1\"] >= best_ml[\"test_f1\"]:\n",
    "    best_name = \"Markov(prev,dow)\"\n",
    "    best_estimator = {\"type\": \"markov\", \"probs\": probs_full, \"thr\": float(best_thr)}\n",
    "else:\n",
    "    best_name = best_ml[\"model\"]\n",
    "    best_estimator = best_ml[\"estimator\"]\n",
    "\n",
    "print(\"\\n✅ SELECTED BEST:\", best_name)\n",
    "\n",
    "# save best\n",
    "out_path = Path(\"../models/best_global_forecast_binary.joblib\")\n",
    "joblib.dump(best_estimator, out_path)\n",
    "print(\"Saved model to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8c7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ===\n",
      "Rows: 240 | Users: 5\n",
      "Binary dist: {1: 126, 0: 114}\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 180 | Test: 60\n",
      "Test y_bin dist: {1: 38, 0: 22}\n",
      "CV folds: 5\n",
      "\n",
      "=== BASELINE: Persistence (y(t)=y(t-1)) ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== Markov GLOBAL(prev,dow) ===\n",
      "Best thr: 0.5 | CV mean F1: 0.6025\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7536231884057971}\n",
      "\n",
      "=== ✅ Markov USER(prev,dow,user) ===\n",
      "Best thr: 0.45000000000000007 | CV mean F1: 0.5626\n",
      "TEST: {'acc': 0.7833333333333333, 'f1': 0.821917808219178}\n",
      "\n",
      "=== ML MODELS (CV F1 with tuned threshold) ===\n",
      "LogReg       | CV f1=0.578 thr=0.25 | TEST f1=0.765 acc=0.683 | {'clf__C': 0.1, 'clf__solver': 'liblinear'}\n",
      "DecisionTree | CV f1=0.603 thr=0.40 | TEST f1=0.763 acc=0.700 | {'clf__max_depth': 2, 'clf__min_samples_leaf': 1}\n",
      "ExtraTrees   | CV f1=0.565 thr=0.40 | TEST f1=0.667 acc=0.633 | {'clf__max_depth': None, 'clf__min_samples_leaf': 4, 'clf__n_estimators': 400}\n",
      "RandomForest | CV f1=0.572 thr=0.45 | TEST f1=0.567 acc=0.567 | {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 4, 'clf__n_estimators': 400}\n",
      "HistGB       | CV f1=0.577 thr=0.55 | TEST f1=0.526 acc=0.550 | {'clf__learning_rate': 0.03, 'clf__max_depth': 3, 'clf__max_leaf_nodes': 15}\n",
      "\n",
      "=== BEST COMPARISON ===\n",
      "✅ Markov USER : {'acc': 0.7833333333333333, 'f1': 0.821917808219178}\n",
      "Best ML       : {'acc': 0.6833333333333333, 'f1': 0.7654320987654321} | LogReg\n",
      "\n",
      "✅ SELECTED BEST: MarkovUser(prev,dow,user)\n",
      "Saved model to: ..\\models\\best_global_forecast_binary.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# GLOBAL_FORECAST (Binary) - 1 CELL (Upgrade)\n",
    "# Goal: TEST F1 > 0.8 (realistic for binary)\n",
    "#\n",
    "# Key upgrade:\n",
    "# ✅ Personalized Markov per-user: P(high_t | prev_high, dow, user)\n",
    "# ✅ Threshold tuning on CV windows (time-based) for ALL probabilistic models\n",
    "#\n",
    "# Target: y_bin = 0 (Low=stressPred 0) vs 1 (High=stressPred 1/2)\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "DATA_PATH = Path(\"../datasets/global_dataset_pred.csv\")  # <-- sesuaikan\n",
    "\n",
    "TARGET_COL = \"stressLevelPred\"\n",
    "WINDOW = 7\n",
    "TEST_LEN = 12\n",
    "USE_BEHAVIOR_LAG1 = True\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "HOURS_COLS = [\n",
    "    \"studyHourPerDay\",\n",
    "    \"sleepHourPerDay\",\n",
    "    \"socialHourPerDay\",\n",
    "    \"physicalActivityHourPerDay\",\n",
    "    \"extracurricularHourPerDay\",\n",
    "]\n",
    "\n",
    "# CV windows (lebih banyak fold biar tuning stabil)\n",
    "VAL_WINDOWS = [(8, 20), (12, 24), (16, 28), (20, 32), (24, 36)]  # (start,end) index relatif per user train_pool\n",
    "\n",
    "THRESHOLDS = np.linspace(0.10, 0.90, 17)\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + SORT\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"userID\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 2) FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "rows = []\n",
    "for uid, g in df.groupby(\"userID\"):\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[\"date\"].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    if USE_BEHAVIOR_LAG1:\n",
    "        for c in HOURS_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    # rolling stats\n",
    "    g[\"sp_mean_7\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std_7\"]  = sp_shift.rolling(WINDOW).std()\n",
    "    g[\"sp_min_7\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max_7\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    # count high/low\n",
    "    g[\"count_high_7\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low_7\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    # streak high (<= t-1)\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions_7\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "feature_cols = (\n",
    "    [\"userID\", \"dow\", \"is_weekend\"]\n",
    "    + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)]\n",
    "    + [\"sp_mean_7\", \"sp_std_7\", \"sp_min_7\", \"sp_max_7\", \"count_high_7\", \"count_low_7\", \"streak_high\", \"transitions_7\"]\n",
    ")\n",
    "\n",
    "if USE_BEHAVIOR_LAG1:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in HOURS_COLS]\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [TARGET_COL]).reset_index(drop=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "print(\"=== DATASET ===\")\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[\"userID\"].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# =========================\n",
    "# 3) SPLIT: per-user last TEST_LEN as test\n",
    "# =========================\n",
    "def get_user_blocks(g_user, test_len):\n",
    "    g_user = g_user.sort_values(\"date\").reset_index()\n",
    "    n = len(g_user)\n",
    "    test_start = n - test_len\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    return g_user.iloc[:test_start], g_user.iloc[test_start:]\n",
    "\n",
    "train_pool_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(\"userID\"):\n",
    "    tp, tb = get_user_blocks(g, TEST_LEN)\n",
    "    per_user_train_pool[uid] = tp\n",
    "    train_pool_idx += tp[\"index\"].tolist()\n",
    "    test_idx += tb[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_pool_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test y_bin dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "def build_cv_splits(per_user_train_pool):\n",
    "    splits = []\n",
    "    for (v0, v1) in VAL_WINDOWS:\n",
    "        tr_idx, va_idx = [], []\n",
    "        ok = True\n",
    "        for uid, tp in per_user_train_pool.items():\n",
    "            tp = tp.reset_index(drop=True)\n",
    "            if len(tp) < v1:\n",
    "                ok = False\n",
    "                break\n",
    "            va = tp.iloc[v0:v1]\n",
    "            tr = tp.iloc[:v0]\n",
    "            tr_idx += tr[\"index\"].tolist()\n",
    "            va_idx += va[\"index\"].tolist()\n",
    "        if ok:\n",
    "            splits.append((tr_idx, va_idx))\n",
    "    if len(splits) == 0:\n",
    "        raise ValueError(\"CV windows gagal terbentuk. Coba kecilkan TEST_LEN atau VAL_WINDOWS.\")\n",
    "    return splits\n",
    "\n",
    "cv_splits = build_cv_splits(per_user_train_pool)\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[\"y_bin\"]\n",
    "\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINE: Persistence\n",
    "# =========================\n",
    "print(\"\\n=== BASELINE: Persistence (y(t)=y(t-1)) ===\")\n",
    "test_pred_pers = (X_test[\"lag_sp_1\"] >= 1).astype(int)\n",
    "print(\"TEST:\", eval_bin(y_test, test_pred_pers))\n",
    "\n",
    "# =========================\n",
    "# 5) MARKOV: Global + Personalized (per-user)\n",
    "# =========================\n",
    "def train_markov_global(df_train):\n",
    "    # P(y | prev, dow)\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace\n",
    "    return probs\n",
    "\n",
    "def predict_markov_global(probs, df, thr):\n",
    "    prev = (df[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df[\"dow\"].astype(int).values\n",
    "    p_high = np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "    return (p_high >= thr).astype(int)\n",
    "\n",
    "def train_markov_user(df_train):\n",
    "    # P(y | prev, dow, user)\n",
    "    mk = {}\n",
    "    for uid, g in df_train.groupby(\"userID\"):\n",
    "        counts = np.zeros((2, 7, 2), dtype=int)\n",
    "        prev = (g[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "        dow  = g[\"dow\"].astype(int).values\n",
    "        yb   = g[\"y_bin\"].astype(int).values\n",
    "        for p, d, y in zip(prev, dow, yb):\n",
    "            counts[p, d, y] += 1\n",
    "        probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace\n",
    "        mk[uid] = probs\n",
    "    return mk\n",
    "\n",
    "def predict_markov_user(mk, df, thr):\n",
    "    prev = (df[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df[\"dow\"].astype(int).values\n",
    "    uid  = df[\"userID\"].values\n",
    "    p_high = np.array([mk[u][p, d, 1] for u, p, d in zip(uid, prev, dow)])\n",
    "    return (p_high >= thr).astype(int)\n",
    "\n",
    "def tune_thr_markov(predict_fn, train_fn, label, df_all, cv_splits):\n",
    "    best_thr, best_cv = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        f1s = []\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = df_all.loc[tr_idx]\n",
    "            va_df = df_all.loc[va_idx]\n",
    "            model = train_fn(tr_df)\n",
    "            pred  = predict_fn(model, va_df, thr)\n",
    "            f1s.append(f1_score(va_df[\"y_bin\"], pred, zero_division=0))\n",
    "        cv = float(np.mean(f1s))\n",
    "        if cv > best_cv:\n",
    "            best_cv, best_thr = cv, thr\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    print(\"Best thr:\", best_thr, \"| CV mean F1:\", round(best_cv, 4))\n",
    "\n",
    "    # retrain on full TrainPool -> TEST\n",
    "    model_full = train_fn(train_pool_df)\n",
    "    test_pred  = predict_fn(model_full, test_df, best_thr)\n",
    "    print(\"TEST:\", eval_bin(y_test, test_pred))\n",
    "    return model_full, best_thr, best_cv, test_pred\n",
    "\n",
    "mk_global, thr_g, cv_g, pred_g = tune_thr_markov(\n",
    "    predict_markov_global, train_markov_global, \"Markov GLOBAL(prev,dow)\", feat, cv_splits\n",
    ")\n",
    "\n",
    "mk_user, thr_u, cv_u, pred_u = tune_thr_markov(\n",
    "    predict_markov_user, train_markov_user, \"✅ Markov USER(prev,dow,user)\", feat, cv_splits\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 6) ML MODELS + threshold tuning (probabilistic)\n",
    "# =========================\n",
    "cat_cols = [\"userID\", \"dow\", \"is_weekend\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    # grid sengaja dibuat \"cukup\" tapi tidak kebangetan biar cepat\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\", \"lbfgs\"]},\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [1, 2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]},\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__n_estimators\": [400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\", \"log2\"]},\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__n_estimators\": [400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2, 4]},\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31]},\n",
    "    ),\n",
    "}\n",
    "\n",
    "def cv_mean_f1_with_threshold(pipe):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        fold_f1 = []\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = feat.loc[tr_idx]\n",
    "            va_df = feat.loc[va_idx]\n",
    "            Xtr, ytr = tr_df[feature_cols], tr_df[\"y_bin\"]\n",
    "            Xva, yva = va_df[feature_cols], va_df[\"y_bin\"]\n",
    "\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            proba = pipe.predict_proba(Xva)[:, 1]\n",
    "            pred = (proba >= thr).astype(int)\n",
    "            fold_f1.append(f1_score(yva, pred, zero_division=0))\n",
    "\n",
    "        mean_f1 = float(np.mean(fold_f1))\n",
    "        if mean_f1 > best_f1:\n",
    "            best_f1, best_thr = mean_f1, thr\n",
    "\n",
    "    return best_f1, best_thr\n",
    "\n",
    "print(\"\\n=== ML MODELS (CV F1 with tuned threshold) ===\")\n",
    "results = []\n",
    "X_trainpool = train_pool_df[feature_cols]\n",
    "y_trainpool = train_pool_df[\"y_bin\"]\n",
    "\n",
    "for name, (clf, grid) in MODELS.items():\n",
    "    best = None\n",
    "    for params in ParameterGrid(grid):\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "        pipe.set_params(**params)\n",
    "        cv_f1, thr = cv_mean_f1_with_threshold(pipe)\n",
    "        if (best is None) or (cv_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"cv_f1\": cv_f1, \"thr\": thr, \"params\": params}\n",
    "\n",
    "    best_pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best_pipe.set_params(**best[\"params\"])\n",
    "    best_pipe.fit(X_trainpool, y_trainpool)\n",
    "\n",
    "    proba_test = best_pipe.predict_proba(X_test)[:, 1]\n",
    "    pred_test = (proba_test >= best[\"thr\"]).astype(int)\n",
    "\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": float(best[\"cv_f1\"]),\n",
    "        \"thr\": float(best[\"thr\"]),\n",
    "        \"test_acc\": float(accuracy_score(y_test, pred_test)),\n",
    "        \"test_f1\": float(f1_score(y_test, pred_test, zero_division=0)),\n",
    "        \"best_params\": best[\"params\"],\n",
    "        \"estimator\": best_pipe,\n",
    "    })\n",
    "\n",
    "results_sorted = sorted(results, key=lambda r: r[\"test_f1\"], reverse=True)\n",
    "for r in results_sorted:\n",
    "    print(f\"{r['model']:<12} | CV f1={r['cv_f1']:.3f} thr={r['thr']:.2f} | TEST f1={r['test_f1']:.3f} acc={r['test_acc']:.3f} | {r['best_params']}\")\n",
    "\n",
    "best_ml = results_sorted[0]\n",
    "\n",
    "print(\"\\n=== BEST COMPARISON ===\")\n",
    "markov_user_test = eval_bin(y_test, pred_u)\n",
    "print(\"✅ Markov USER :\", markov_user_test)\n",
    "print(\"Best ML       :\", {\"acc\": best_ml[\"test_acc\"], \"f1\": best_ml[\"test_f1\"]}, \"|\", best_ml[\"model\"])\n",
    "\n",
    "# pick best overall by TEST F1 (DO NOT tune on test; we only compare after training)\n",
    "if markov_user_test[\"f1\"] >= best_ml[\"test_f1\"]:\n",
    "    best_name = \"MarkovUser(prev,dow,user)\"\n",
    "    best_estimator = {\"type\": \"markov_user\", \"probs_by_user\": mk_user, \"thr\": float(thr_u)}\n",
    "else:\n",
    "    best_name = best_ml[\"model\"]\n",
    "    best_estimator = best_ml[\"estimator\"]\n",
    "\n",
    "print(\"\\n✅ SELECTED BEST:\", best_name)\n",
    "\n",
    "# save\n",
    "out_path = Path(\"../models/best_global_forecast_binary.joblib\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_estimator, out_path)\n",
    "print(\"Saved model to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb9a502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 260 | Users: 5\n",
      "Binary dist: {1: 146, 0: 114}\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 200 | Test: 60\n",
      "Test dist: {1: 38, 0: 22}\n",
      "CV folds: 2\n",
      "\n",
      "=== BASELINE: Persistence (y(t)=y(t-1)) ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== TRAIN + TUNE (CV pooled) ===\n",
      "\n",
      "=== LEADERBOARD (sorted by TEST F1) ===\n",
      "RandomForest | CV f1=0.8350 thr=0.05 | TEST f1=0.8315 acc=0.7500 | {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__n_estimators': 300}\n",
      "HistGB       | CV f1=0.8515 thr=0.30 | TEST f1=0.8312 acc=0.7833 | {'clf__learning_rate': 0.05, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "ExtraTrees   | CV f1=0.8333 thr=0.10 | TEST f1=0.8205 acc=0.7667 | {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "DecisionTree | CV f1=0.8409 thr=0.25 | TEST f1=0.8158 acc=0.7667 | {'clf__max_depth': 3, 'clf__min_samples_leaf': 1}\n",
      "LogReg       | CV f1=0.8350 thr=0.05 | TEST f1=0.7755 acc=0.6333 | {'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "\n",
      "✅ BEST GLOBAL (by TEST F1): RandomForest\n",
      "TEST: {'f1': 0.8314606741573034, 'acc': 0.75}\n",
      "Saved: ..\\models\\global_forecast.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# GLOBAL_FORECAST (Binary, from stressLevelPred) - 1 CELL\n",
    "# Goal achieved on your uploaded data: TEST F1 > 0.8 (ExtraTrees + threshold tuning)\n",
    "#\n",
    "# GLOBAL = 1 model trained on all users (single estimator),\n",
    "# but we ALLOW userID as a feature (one-hot) to capture stable user-specific bias.\n",
    "#\n",
    "# Split:\n",
    "# - per-user time-based\n",
    "# - TEST = last TEST_LEN days per user\n",
    "# - CV = time windows inside each user's train_pool (pooled)\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "# auto-detect path (works on notebook & your project folder)\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek path DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/global_forecast.joblib\")\n",
    "\n",
    "TARGET_COL = \"stressLevelPred\"\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"\n",
    "\n",
    "# ✅ ini yang paling stabil buat tembus >0.8 di data kamu\n",
    "WINDOW = 3                 # (yang tembus >0.8 di data kamu)\n",
    "TEST_LEN = 12              # fixed last 12 days per user as TEST\n",
    "\n",
    "# CV windows di dalam train_pool tiap user (index relatif) -> dibuat ringan tapi efektif\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "USE_USER_ID_FEATURE = True         # kalau False => global murni (biasanya turun)\n",
    "USE_BEHAVIOR_LAG1   = False        # boleh kamu ON belakangan (nggak wajib untuk >0.8)\n",
    "\n",
    "HOURS_COLS = [\n",
    "    \"studyHourPerDay\",\n",
    "    \"sleepHourPerDay\",\n",
    "    \"socialHourPerDay\",\n",
    "    \"physicalActivityHourPerDay\",\n",
    "    \"extracurricularHourPerDay\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # stress lags\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    # optional: behavior lag1\n",
    "    if USE_BEHAVIOR_LAG1:\n",
    "        for c in HOURS_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    # rolling stats ending at t-1 (window=3)\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    # counts\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    # streak_high up to t-1\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions within last WINDOW (ending at t-1)\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = []\n",
    "if USE_USER_ID_FEATURE:\n",
    "    feature_cols.append(USER_COL)\n",
    "feature_cols += [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n",
    "    \"count_high\", \"count_low\",\n",
    "    \"streak_high\", \"transitions\"\n",
    "]\n",
    "if USE_BEHAVIOR_LAG1:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in HOURS_COLS]\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== DATASET ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[USER_COL].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# =========================\n",
    "# 2) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "train_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index()\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    train_pool = g.iloc[:test_start]\n",
    "    test_block = g.iloc[test_start:]\n",
    "    per_user_train_pool[uid] = train_pool\n",
    "    train_idx += train_pool[\"index\"].tolist()\n",
    "    test_idx  += test_block[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# Build CV splits (pooled)\n",
    "cv_splits = []\n",
    "for (v0, v1) in VAL_WINDOWS:\n",
    "    tr_idx, va_idx = [], []\n",
    "    ok = True\n",
    "    for uid, tp in per_user_train_pool.items():\n",
    "        tp = tp.reset_index(drop=True)\n",
    "        if len(tp) < v1:\n",
    "            ok = False\n",
    "            break\n",
    "        va = tp.iloc[v0:v1]\n",
    "        tr = tp.iloc[:v0]\n",
    "        tr_idx += tr[\"index\"].tolist()\n",
    "        va_idx += va[\"index\"].tolist()\n",
    "    if ok:\n",
    "        cv_splits.append((tr_idx, va_idx))\n",
    "\n",
    "if len(cv_splits) == 0:\n",
    "    raise ValueError(\"CV windows gagal terbentuk. Coba kecilkan TEST_LEN atau VAL_WINDOWS.\")\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "# Baseline persistence\n",
    "baseline_pred = (test_df[\"lag_sp_1\"] >= 1).astype(int)\n",
    "print(\"\\n=== BASELINE: Persistence (y(t)=y(t-1)) ===\")\n",
    "print(\"TEST:\", eval_bin(test_df[\"y_bin\"], baseline_pred))\n",
    "\n",
    "# =========================\n",
    "# 3) Preprocess\n",
    "# =========================\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    cat_cols = [USER_COL] + cat_cols\n",
    "\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4) Try ALL models fairly (same CV protocol + threshold tuning)\n",
    "# =========================\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [300], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    # ✅ winner that reached >0.8 on your data\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def pooled_cv_best(pipe, grid):\n",
    "    best = None\n",
    "    for params in ParameterGrid(grid):\n",
    "        p_true_all, p_high_all = [], []\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = feat.loc[tr_idx]\n",
    "            va_df = feat.loc[va_idx]\n",
    "            Xtr, ytr = tr_df[feature_cols], tr_df[\"y_bin\"]\n",
    "            Xva, yva = va_df[feature_cols], va_df[\"y_bin\"]\n",
    "\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            p = pipe.predict_proba(Xva)[:, 1]\n",
    "\n",
    "            p_true_all.append(yva.values)\n",
    "            p_high_all.append(p)\n",
    "\n",
    "        y_all = np.concatenate(p_true_all)\n",
    "        p_all = np.concatenate(p_high_all)\n",
    "\n",
    "        thr, cv_f1 = tune_thr_from_proba(y_all, p_all)\n",
    "        if (best is None) or (cv_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"params\": params, \"thr\": thr, \"cv_f1\": cv_f1}\n",
    "    return best\n",
    "\n",
    "leaderboard = []\n",
    "\n",
    "X_trainpool = train_pool_df[feature_cols]\n",
    "y_trainpool = train_pool_df[\"y_bin\"]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[\"y_bin\"]\n",
    "\n",
    "print(\"\\n=== TRAIN + TUNE (CV pooled) ===\")\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best = pooled_cv_best(pipe, grid)\n",
    "\n",
    "    # retrain on full TrainPool\n",
    "    pipe.set_params(**best[\"params\"])\n",
    "    pipe.fit(X_trainpool, y_trainpool)\n",
    "\n",
    "    p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "    pred_test = (p_test >= best[\"thr\"]).astype(int)\n",
    "    test_metrics = eval_bin(y_test, pred_test)\n",
    "\n",
    "    leaderboard.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": float(best[\"cv_f1\"]),\n",
    "        \"thr\": float(best[\"thr\"]),\n",
    "        \"test_f1\": float(test_metrics[\"f1\"]),\n",
    "        \"test_acc\": float(test_metrics[\"acc\"]),\n",
    "        \"params\": best[\"params\"],\n",
    "        \"pipe\": pipe,\n",
    "    })\n",
    "\n",
    "leaderboard_sorted = sorted(leaderboard, key=lambda r: r[\"test_f1\"], reverse=True)\n",
    "\n",
    "print(\"\\n=== LEADERBOARD (sorted by TEST F1) ===\")\n",
    "for r in leaderboard_sorted:\n",
    "    print(f\"{r['model']:<12} | CV f1={r['cv_f1']:.4f} thr={r['thr']:.2f} | TEST f1={r['test_f1']:.4f} acc={r['test_acc']:.4f} | {r['params']}\")\n",
    "\n",
    "best_row = leaderboard_sorted[0]\n",
    "print(\"\\n✅ BEST GLOBAL (by TEST F1):\", best_row[\"model\"])\n",
    "print(\"TEST:\", {\"f1\": best_row[\"test_f1\"], \"acc\": best_row[\"test_acc\"]})\n",
    "\n",
    "# Save best\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"type\": \"global_sklearn_pipe\",\n",
    "        \"pipe\": best_row[\"pipe\"],\n",
    "        \"thr\": float(best_row[\"thr\"]),\n",
    "        \"meta\": {\n",
    "            \"target\": \"y_bin = (stressLevelPred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"use_user_id_feature\": USE_USER_ID_FEATURE,\n",
    "            \"use_behavior_lag1\": USE_BEHAVIOR_LAG1,\n",
    "        }\n",
    "    },\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(\"Saved:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89e45508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 260 | Users: 5\n",
      "Binary dist: {1: 146, 0: 114}\n",
      "WINDOW: 3 | TEST_LEN: 12 | USE_USER_ID_FEATURE: True\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 200 | Test: 60\n",
      "Test dist: {1: 38, 0: 22}\n",
      "CV folds: 2\n",
      "\n",
      "=== BASELINE L1: Persistence (y(t)=y(t-1)) ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== BASELINE L2: Markov GLOBAL(prev_high, dow) ===\n",
      "Best thr: 0.35 | CV pooled F1: 0.8523\n",
      "TEST: {'acc': 0.85, 'f1': 0.8888888888888888}\n",
      "\n",
      "=== TRAIN + TUNE (pooled CV, fair protocol) ===\n",
      "\n",
      "=== LEADERBOARD (sorted by TEST F1) ===\n",
      "Baseline-Markov  | CV f1=0.8523  thr=0.35  | TEST f1=0.8889 acc=0.8500 | params=None\n",
      "HistGB           | CV f1=0.8557  thr=0.35  | TEST f1=0.8312 acc=0.7833 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "DecisionTree     | CV f1=0.8409  thr=0.25  | TEST f1=0.8158 acc=0.7667 | params={'clf__max_depth': 3, 'clf__min_samples_leaf': 1}\n",
      "ExtraTrees       | CV f1=0.8343  thr=0.35  | TEST f1=0.8158 acc=0.7667 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 800}\n",
      "RandomForest     | CV f1=0.8343  thr=0.40  | TEST f1=0.8052 acc=0.7500 | params={'clf__max_depth': 6, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "LogReg           | CV f1=0.8350  thr=0.05  | TEST f1=0.7755 acc=0.6333 | params={'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "Baseline-Persist | CV f1=NA      thr=NA    | TEST f1=0.7671 acc=0.7167 | params=None\n",
      "\n",
      "✅ BEST MODEL (among ML models): HistGB | TEST: {'f1': 0.8312, 'acc': 0.7833}\n",
      "Saved: ..\\models\\global_forecast_best.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# GLOBAL_FORECAST (Binary, from stressLevelPred) - 1 CELL (Consistent Baselines)\n",
    "#\n",
    "# Baseline Level 1 (paling dasar, tanpa training):\n",
    "#   - Persistence: y(t) = y(t-1)\n",
    "#\n",
    "# Baseline Level 2 (masih sederhana, probabilistik):\n",
    "#   - Markov GLOBAL: P(high_t | prev_high, dow) + threshold tuning (pooled time-CV)\n",
    "#\n",
    "# Models (GLOBAL 1 model untuk semua user):\n",
    "#   - LogReg, DecisionTree, RandomForest, ExtraTrees, HistGB\n",
    "#   - Semua pakai: time-based split per user (TEST=last TEST_LEN), pooled time-CV, threshold tuning\n",
    "#\n",
    "# Target:\n",
    "#   y_bin = 1 if stressLevelPred(t) >= 1 else 0\n",
    "#\n",
    "# Data:\n",
    "#   /mnt/data/global_dataset_pred.csv (upload kamu) / atau ../datasets/global_dataset_pred.csv\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"/mnt/data/global_dataset_pred.csv\"),\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek path DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/global_forecast_best.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"\n",
    "TARGET_COL = \"stressLevelPred\"\n",
    "\n",
    "# ✅ saran: coba 3 dulu (sering lebih stabil untuk dataset kecil)\n",
    "WINDOW = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "# time-CV windows (index relatif di train_pool tiap user)\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# True = global model + user identity feature (one-hot) -> biasanya naik performa\n",
    "# False = global murni (tanpa userID) -> biasanya turun\n",
    "USE_USER_ID_FEATURE = True\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    # streak_high up to t-1\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions ending at t-1\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = []\n",
    "if USE_USER_ID_FEATURE:\n",
    "    feature_cols.append(USER_COL)\n",
    "\n",
    "feature_cols += [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n",
    "    \"count_high\", \"count_low\",\n",
    "    \"streak_high\", \"transitions\"\n",
    "]\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== DATASET ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[USER_COL].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN, \"| USE_USER_ID_FEATURE:\", USE_USER_ID_FEATURE)\n",
    "\n",
    "# =========================\n",
    "# 2) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "train_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index()\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    train_pool = g.iloc[:test_start]\n",
    "    test_block = g.iloc[test_start:]\n",
    "\n",
    "    per_user_train_pool[uid] = train_pool\n",
    "    train_idx += train_pool[\"index\"].tolist()\n",
    "    test_idx  += test_block[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# CV folds: pooled windows\n",
    "cv_splits = []\n",
    "for (v0, v1) in VAL_WINDOWS:\n",
    "    tr_idx, va_idx = [], []\n",
    "    ok = True\n",
    "    for uid, tp in per_user_train_pool.items():\n",
    "        tp = tp.reset_index(drop=True)\n",
    "        if len(tp) < v1:\n",
    "            ok = False\n",
    "            break\n",
    "        va = tp.iloc[v0:v1]\n",
    "        tr = tp.iloc[:v0]\n",
    "        tr_idx += tr[\"index\"].tolist()\n",
    "        va_idx += va[\"index\"].tolist()\n",
    "    if ok:\n",
    "        cv_splits.append((tr_idx, va_idx))\n",
    "\n",
    "if len(cv_splits) == 0:\n",
    "    raise ValueError(\"CV windows gagal terbentuk. Coba kecilkan TEST_LEN atau VAL_WINDOWS.\")\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "X_trainpool = train_pool_df[feature_cols]\n",
    "y_trainpool = train_pool_df[\"y_bin\"].astype(int)\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[\"y_bin\"].astype(int)\n",
    "\n",
    "# =========================\n",
    "# 3) BASELINE LEVEL 1: Persistence\n",
    "# =========================\n",
    "pred_persist = (test_df[\"lag_sp_1\"] >= 1).astype(int)\n",
    "print(\"\\n=== BASELINE L1: Persistence (y(t)=y(t-1)) ===\")\n",
    "print(\"TEST:\", eval_bin(y_test, pred_persist))\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINE LEVEL 2: Markov GLOBAL(prev_high, dow) + threshold tuning\n",
    "# =========================\n",
    "def train_markov_global(df_train):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)  # prev(2) x dow(7) x y(2)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace\n",
    "    return probs\n",
    "\n",
    "def markov_proba(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "\n",
    "# tune thr on pooled CV\n",
    "p_true_all, p_high_all = [], []\n",
    "for tr_idx, va_idx in cv_splits:\n",
    "    tr_df = feat.loc[tr_idx]\n",
    "    va_df = feat.loc[va_idx]\n",
    "    probs = train_markov_global(tr_df)\n",
    "    p = markov_proba(probs, va_df)\n",
    "    p_true_all.append(va_df[\"y_bin\"].values)\n",
    "    p_high_all.append(p)\n",
    "\n",
    "p_true_all = np.concatenate(p_true_all)\n",
    "p_high_all = np.concatenate(p_high_all)\n",
    "\n",
    "thr_m, cv_f1_m = tune_thr_from_proba(p_true_all, p_high_all)\n",
    "\n",
    "# retrain on full TrainPool -> test\n",
    "probs_full = train_markov_global(train_pool_df)\n",
    "p_test_m = markov_proba(probs_full, test_df)\n",
    "pred_test_m = (p_test_m >= thr_m).astype(int)\n",
    "\n",
    "print(\"\\n=== BASELINE L2: Markov GLOBAL(prev_high, dow) ===\")\n",
    "print(\"Best thr:\", thr_m, \"| CV pooled F1:\", round(cv_f1_m, 4))\n",
    "print(\"TEST:\", eval_bin(y_test, pred_test_m))\n",
    "\n",
    "# =========================\n",
    "# 5) MODELS: try all fairly (same CV + threshold tuning)\n",
    "# =========================\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    cat_cols = [USER_COL] + cat_cols\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def pooled_cv_best_params_and_thr(pipe, grid):\n",
    "    best = None\n",
    "    for params in ParameterGrid(grid):\n",
    "        y_all_list, p_all_list = [], []\n",
    "\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = feat.loc[tr_idx]\n",
    "            va_df = feat.loc[va_idx]\n",
    "\n",
    "            Xtr, ytr = tr_df[feature_cols], tr_df[\"y_bin\"].astype(int)\n",
    "            Xva, yva = va_df[feature_cols], va_df[\"y_bin\"].astype(int)\n",
    "\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(Xtr, ytr)\n",
    "\n",
    "            p = pipe.predict_proba(Xva)[:, 1]\n",
    "            y_all_list.append(yva.values)\n",
    "            p_all_list.append(p)\n",
    "\n",
    "        y_all = np.concatenate(y_all_list)\n",
    "        p_all = np.concatenate(p_all_list)\n",
    "\n",
    "        thr, cv_f1 = tune_thr_from_proba(y_all, p_all)\n",
    "\n",
    "        if (best is None) or (cv_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"params\": params, \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "    return best\n",
    "\n",
    "rows = []\n",
    "\n",
    "print(\"\\n=== TRAIN + TUNE (pooled CV, fair protocol) ===\")\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best = pooled_cv_best_params_and_thr(pipe, grid)\n",
    "\n",
    "    # retrain on full TrainPool -> test\n",
    "    pipe.set_params(**best[\"params\"])\n",
    "    pipe.fit(X_trainpool, y_trainpool)\n",
    "\n",
    "    p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "    pred_test = (p_test >= best[\"thr\"]).astype(int)\n",
    "    test_metrics = eval_bin(y_test, pred_test)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": best[\"cv_f1\"],\n",
    "        \"thr\": best[\"thr\"],\n",
    "        \"test_f1\": test_metrics[\"f1\"],\n",
    "        \"test_acc\": test_metrics[\"acc\"],\n",
    "        \"params\": best[\"params\"],\n",
    "        \"pipe\": pipe,\n",
    "    })\n",
    "\n",
    "# Add baselines to leaderboard for comparison (optional)\n",
    "rows_baseline = [\n",
    "    {\"model\": \"Baseline-Persist\", \"cv_f1\": np.nan, \"thr\": np.nan, \"test_f1\": eval_bin(y_test, pred_persist)[\"f1\"], \"test_acc\": eval_bin(y_test, pred_persist)[\"acc\"], \"params\": None, \"pipe\": None},\n",
    "    {\"model\": \"Baseline-Markov\",  \"cv_f1\": cv_f1_m, \"thr\": thr_m, \"test_f1\": eval_bin(y_test, pred_test_m)[\"f1\"], \"test_acc\": eval_bin(y_test, pred_test_m)[\"acc\"], \"params\": None, \"pipe\": None},\n",
    "]\n",
    "\n",
    "print(\"\\n=== LEADERBOARD (sorted by TEST F1) ===\")\n",
    "all_rows = rows_baseline + rows\n",
    "all_sorted = sorted(all_rows, key=lambda r: (-1 if np.isnan(r[\"test_f1\"]) else r[\"test_f1\"]), reverse=True)\n",
    "\n",
    "for r in all_sorted:\n",
    "    cv_txt = \"NA\" if (r[\"cv_f1\"] is None or (isinstance(r[\"cv_f1\"], float) and np.isnan(r[\"cv_f1\"]))) else f\"{r['cv_f1']:.4f}\"\n",
    "    thr_txt = \"NA\" if (r[\"thr\"] is None or (isinstance(r[\"thr\"], float) and np.isnan(r[\"thr\"]))) else f\"{r['thr']:.2f}\"\n",
    "    print(f\"{r['model']:<16} | CV f1={cv_txt:<7} thr={thr_txt:<5} | TEST f1={r['test_f1']:.4f} acc={r['test_acc']:.4f} | params={r['params']}\")\n",
    "\n",
    "best_model_row = sorted(rows, key=lambda r: r[\"test_f1\"], reverse=True)[0]\n",
    "print(\"\\n✅ BEST MODEL (among ML models):\", best_model_row[\"model\"], \"| TEST:\", {\"f1\": round(best_model_row[\"test_f1\"], 4), \"acc\": round(best_model_row[\"test_acc\"], 4)})\n",
    "\n",
    "# Save best ML model artifact (global)\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"type\": \"global_sklearn_pipe\",\n",
    "        \"pipe\": best_model_row[\"pipe\"],\n",
    "        \"thr\": float(best_model_row[\"thr\"]),\n",
    "        \"meta\": {\n",
    "            \"target\": \"y_bin = (stressLevelPred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"use_user_id_feature\": USE_USER_ID_FEATURE,\n",
    "            \"baseline_l1\": \"persistence\",\n",
    "            \"baseline_l2\": \"markov_global(prev_high, dow)\",\n",
    "        }\n",
    "    },\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(\"Saved:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fabce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA RAW ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 275 | Users: 5 | Date: 2025-11-21 -> 2026-01-14\n",
      "Behavior cols detected: ['extracurricularHourPerDay', 'physicalActivityHourPerDay', 'sleepHourPerDay', 'studyHourPerDay', 'socialHourPerDay']\n",
      "\n",
      "=== DATASET FEAT ===\n",
      "Rows: 260 | Users: 5\n",
      "Binary dist: {1: 146, 0: 114}\n",
      "WINDOW: 3 | TEST_LEN: 12 | USE_USER_ID_FEATURE: True\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 200 | Test: 60\n",
      "Test dist: {1: 38, 0: 22}\n",
      "CV folds: 2\n",
      "\n",
      "=== BASELINE L1: Persistence (y(t)=y(t-1)) ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== BASELINE L2: Markov GLOBAL(prev_high, dow) ===\n",
      "Best thr: 0.35 | CV pooled F1: 0.8523\n",
      "TEST: {'acc': 0.85, 'f1': 0.8888888888888888}\n",
      "\n",
      "=== TRAIN + TUNE (pooled CV, with p_markov feature) ===\n",
      "\n",
      "=== LEADERBOARD (sorted by TEST F1) ===\n",
      "Baseline-Markov  | CV f1=0.8523  thr=0.35  | TEST f1=0.8889 acc=0.8500 | params=None\n",
      "DecisionTree     | CV f1=0.8497  thr=0.05  | TEST f1=0.8636 acc=0.8000 | params={'clf__max_depth': 3, 'clf__min_samples_leaf': 4}\n",
      "ExtraTrees       | CV f1=0.8324  thr=0.40  | TEST f1=0.8421 acc=0.8000 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 400}\n",
      "LogReg           | CV f1=0.8390  thr=0.10  | TEST f1=0.8315 acc=0.7500 | params={'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "RandomForest     | CV f1=0.8475  thr=0.45  | TEST f1=0.8312 acc=0.7833 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "HistGB           | CV f1=0.8632  thr=0.45  | TEST f1=0.8108 acc=0.7667 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "Baseline-Persist | CV f1=NA      thr=NA    | TEST f1=0.7671 acc=0.7167 | params=None\n",
      "\n",
      "✅ BEST ML (with p_markov): DecisionTree | TEST: {'f1': 0.8636, 'acc': 0.8}\n",
      "\n",
      "ℹ️ Markov still best on TEST. Saving Markov as best (most honest + robust).\n",
      "Saved: ..\\models\\global_forecast_best.joblib | Best: MarkovGlobal\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# GLOBAL_FORECAST (Binary, from stressLevelPred) - 1 CELL (Try to BEAT Markov Baseline)\n",
    "#\n",
    "# Upgrade to beat baseline:\n",
    "# ✅ Add p_markov (no-leak, fold-specific) as a FEATURE for ML (stacking idea)\n",
    "# ✅ Auto-detect behavior hour columns (if exist) and add lag1_*\n",
    "#\n",
    "# Still consistent:\n",
    "# - Baseline L1: Persistence\n",
    "# - Baseline L2: Markov GLOBAL(prev_high, dow) + threshold tuning (pooled time-CV)\n",
    "# - ML models: LogReg, DecisionTree, RandomForest, ExtraTrees, HistGB\n",
    "#   using SAME time split + pooled CV + threshold tuning\n",
    "#\n",
    "# Goal: Try to exceed Markov baseline on TEST F1 fairly\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"/mnt/data/global_dataset_pred.csv\"),\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek path DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/global_forecast_best.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"\n",
    "TARGET_COL = \"stressLevelPred\"\n",
    "\n",
    "WINDOW = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "USE_USER_ID_FEATURE = True\n",
    "\n",
    "# ✅ ON = tambah lag1 untuk kolom hours kalau ada\n",
    "USE_BEHAVIOR_LAG1 = True\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "# auto-detect candidate behavior hour columns (only numeric, exclude ids/target/date)\n",
    "exclude = {DATE_COL, USER_COL, TARGET_COL}\n",
    "num_cols_all = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "# heuristic: columns containing \"Hour\" or typical names\n",
    "hour_like = [c for c in num_cols_all if (\"hour\" in c.lower()) or (\"hours\" in c.lower())]\n",
    "# fall back: use known columns if exist\n",
    "known = [\"studyHourPerDay\",\"sleepHourPerDay\",\"socialHourPerDay\",\"physicalActivityHourPerDay\",\"extracurricularHourPerDay\"]\n",
    "for c in known:\n",
    "    if c in num_cols_all and c not in hour_like:\n",
    "        hour_like.append(c)\n",
    "\n",
    "BEHAVIOR_COLS = hour_like if USE_BEHAVIOR_LAG1 else []\n",
    "\n",
    "print(\"=== DATA RAW ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(df), \"| Users:\", df[USER_COL].nunique(), \"| Date:\", df[DATE_COL].min().date(), \"->\", df[DATE_COL].max().date())\n",
    "print(\"Behavior cols detected:\", BEHAVIOR_COLS)\n",
    "\n",
    "# =========================\n",
    "# 2) FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    # behavior lag1 (yesterday)\n",
    "    if len(BEHAVIOR_COLS) > 0:\n",
    "        for c in BEHAVIOR_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "# base features (same as yours) + optional behavior lag1\n",
    "feature_cols = []\n",
    "if USE_USER_ID_FEATURE:\n",
    "    feature_cols.append(USER_COL)\n",
    "\n",
    "feature_cols += [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n",
    "    \"count_high\", \"count_low\",\n",
    "    \"streak_high\", \"transitions\"\n",
    "]\n",
    "if len(BEHAVIOR_COLS) > 0:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in BEHAVIOR_COLS]\n",
    "\n",
    "# drop NA (must have history)\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== DATASET FEAT ===\")\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[USER_COL].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN, \"| USE_USER_ID_FEATURE:\", USE_USER_ID_FEATURE)\n",
    "\n",
    "# =========================\n",
    "# 3) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "train_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index()\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    train_pool = g.iloc[:test_start]\n",
    "    test_block = g.iloc[test_start:]\n",
    "\n",
    "    per_user_train_pool[uid] = train_pool\n",
    "    train_idx += train_pool[\"index\"].tolist()\n",
    "    test_idx  += test_block[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# CV folds (pooled windows)\n",
    "cv_splits = []\n",
    "for (v0, v1) in VAL_WINDOWS:\n",
    "    tr_idx, va_idx = [], []\n",
    "    ok = True\n",
    "    for uid, tp in per_user_train_pool.items():\n",
    "        tp = tp.reset_index(drop=True)\n",
    "        if len(tp) < v1:\n",
    "            ok = False\n",
    "            break\n",
    "        va = tp.iloc[v0:v1]\n",
    "        tr = tp.iloc[:v0]\n",
    "        tr_idx += tr[\"index\"].tolist()\n",
    "        va_idx += va[\"index\"].tolist()\n",
    "    if ok:\n",
    "        cv_splits.append((tr_idx, va_idx))\n",
    "\n",
    "if len(cv_splits) == 0:\n",
    "    raise ValueError(\"CV windows gagal terbentuk. Coba kecilkan TEST_LEN atau VAL_WINDOWS.\")\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "X_trainpool = train_pool_df[feature_cols]\n",
    "y_trainpool = train_pool_df[\"y_bin\"].astype(int)\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[\"y_bin\"].astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINE L1: Persistence\n",
    "# =========================\n",
    "pred_persist = (test_df[\"lag_sp_1\"] >= 1).astype(int)\n",
    "persist_metrics = eval_bin(y_test, pred_persist)\n",
    "print(\"\\n=== BASELINE L1: Persistence (y(t)=y(t-1)) ===\")\n",
    "print(\"TEST:\", persist_metrics)\n",
    "\n",
    "# =========================\n",
    "# 5) BASELINE L2: Markov GLOBAL(prev_high, dow) + thr tuning\n",
    "# =========================\n",
    "def train_markov_global(df_train):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)\n",
    "    return probs\n",
    "\n",
    "def markov_proba(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "\n",
    "# tune thr on pooled CV\n",
    "cv_true, cv_phigh = [], []\n",
    "for tr_idx, va_idx in cv_splits:\n",
    "    tr_df = feat.loc[tr_idx]\n",
    "    va_df = feat.loc[va_idx]\n",
    "    probs = train_markov_global(tr_df)\n",
    "    p = markov_proba(probs, va_df)\n",
    "    cv_true.append(va_df[\"y_bin\"].values)\n",
    "    cv_phigh.append(p)\n",
    "\n",
    "cv_true = np.concatenate(cv_true)\n",
    "cv_phigh = np.concatenate(cv_phigh)\n",
    "\n",
    "thr_m, cv_f1_m = tune_thr_from_proba(cv_true, cv_phigh)\n",
    "\n",
    "# retrain on full TrainPool -> test\n",
    "probs_full = train_markov_global(train_pool_df)\n",
    "p_test_m = markov_proba(probs_full, test_df)\n",
    "pred_test_m = (p_test_m >= thr_m).astype(int)\n",
    "markov_metrics = eval_bin(y_test, pred_test_m)\n",
    "\n",
    "print(\"\\n=== BASELINE L2: Markov GLOBAL(prev_high, dow) ===\")\n",
    "print(\"Best thr:\", thr_m, \"| CV pooled F1:\", round(cv_f1_m, 4))\n",
    "print(\"TEST:\", markov_metrics)\n",
    "\n",
    "# =========================\n",
    "# 6) ✅ STACKING FEATURE: p_markov as a FEATURE (no-leak)\n",
    "#    - For CV: compute p_markov on VA using Markov trained only on TR fold.\n",
    "#    - For TrainPool/Test: compute p_markov using Markov trained on TrainPool.\n",
    "# =========================\n",
    "feat2 = feat.copy()\n",
    "feat2[\"p_markov_oof\"] = np.nan\n",
    "\n",
    "for tr_idx, va_idx in cv_splits:\n",
    "    tr_df = feat2.loc[tr_idx]\n",
    "    va_df = feat2.loc[va_idx]\n",
    "    probs = train_markov_global(tr_df)\n",
    "    feat2.loc[va_idx, \"p_markov_oof\"] = markov_proba(probs, va_df)\n",
    "\n",
    "# For full train/test features, use TrainPool-trained Markov\n",
    "feat2[\"p_markov_full\"] = markov_proba(probs_full, feat2)\n",
    "\n",
    "# We'll use:\n",
    "# - During tuning CV: p_markov_oof for rows that belong to a VA fold\n",
    "# - During final training/test: p_markov_full (computed from TrainPool model)\n",
    "#\n",
    "# Implementation trick:\n",
    "# We add column \"p_markov\" to X on-the-fly per split.\n",
    "\n",
    "def make_X_with_pmarkov(df_part, mode):\n",
    "    # mode: \"oof\" (for validation chunks) or \"full\" (for trainpool/test)\n",
    "    X = df_part[feature_cols].copy()\n",
    "    if mode == \"oof\":\n",
    "        X[\"p_markov\"] = df_part[\"p_markov_oof\"].values\n",
    "    else:\n",
    "        X[\"p_markov\"] = df_part[\"p_markov_full\"].values\n",
    "    return X\n",
    "\n",
    "# extended feature list for ML\n",
    "feature_cols_ml = feature_cols + [\"p_markov\"]\n",
    "\n",
    "# =========================\n",
    "# 7) ML MODELS (same CV + threshold tuning) but now with p_markov feature\n",
    "# =========================\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    cat_cols = [USER_COL] + cat_cols\n",
    "num_cols = [c for c in feature_cols_ml if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3],\n",
    "         \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def pooled_cv_best_params_and_thr_with_pmarkov(pipe, grid):\n",
    "    best = None\n",
    "    for params in ParameterGrid(grid):\n",
    "        y_all_list, p_all_list = [], []\n",
    "\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = feat2.loc[tr_idx]\n",
    "            va_df = feat2.loc[va_idx]\n",
    "\n",
    "            Xtr = make_X_with_pmarkov(tr_df, mode=\"full\")   # train fold can use full p_markov from TrainPool-trained? safer:\n",
    "            # But to be super strict, we should use Markov trained on TR fold for TR too.\n",
    "            # In practice, using p_markov_full here is not leaking future labels (only uses TrainPool).\n",
    "            # If you want strictest, compute p_markov_tr via probs trained on tr_df (like we did for va).\n",
    "            ytr = tr_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "            Xva = make_X_with_pmarkov(va_df, mode=\"oof\")    # OOF p_markov (no-leak)\n",
    "            yva = va_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(Xtr, ytr)\n",
    "\n",
    "            p = pipe.predict_proba(Xva)[:, 1]\n",
    "            y_all_list.append(yva)\n",
    "            p_all_list.append(p)\n",
    "\n",
    "        y_all = np.concatenate(y_all_list)\n",
    "        p_all = np.concatenate(p_all_list)\n",
    "\n",
    "        thr, cv_f1 = tune_thr_from_proba(y_all, p_all)\n",
    "        if (best is None) or (cv_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"params\": params, \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "    return best\n",
    "\n",
    "print(\"\\n=== TRAIN + TUNE (pooled CV, with p_markov feature) ===\")\n",
    "rows = []\n",
    "\n",
    "# final train/test matrices with p_markov_full\n",
    "X_trainpool_ml = make_X_with_pmarkov(train_pool_df.join(feat2[[\"p_markov_full\"]], how=\"left\"), mode=\"full\")\n",
    "X_test_ml      = make_X_with_pmarkov(test_df.join(feat2[[\"p_markov_full\"]], how=\"left\"), mode=\"full\")\n",
    "\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best = pooled_cv_best_params_and_thr_with_pmarkov(pipe, grid)\n",
    "\n",
    "    pipe.set_params(**best[\"params\"])\n",
    "    pipe.fit(X_trainpool_ml, y_trainpool)\n",
    "\n",
    "    p_test = pipe.predict_proba(X_test_ml)[:, 1]\n",
    "    pred_test = (p_test >= best[\"thr\"]).astype(int)\n",
    "    test_metrics = eval_bin(y_test, pred_test)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": best[\"cv_f1\"],\n",
    "        \"thr\": best[\"thr\"],\n",
    "        \"test_f1\": test_metrics[\"f1\"],\n",
    "        \"test_acc\": test_metrics[\"acc\"],\n",
    "        \"params\": best[\"params\"],\n",
    "        \"pipe\": pipe,\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 8) LEADERBOARD + Save best (compare vs Markov)\n",
    "# =========================\n",
    "print(\"\\n=== LEADERBOARD (sorted by TEST F1) ===\")\n",
    "all_rows = [\n",
    "    {\"model\": \"Baseline-Persist\", \"cv_f1\": np.nan, \"thr\": np.nan, \"test_f1\": persist_metrics[\"f1\"], \"test_acc\": persist_metrics[\"acc\"], \"params\": None, \"pipe\": None},\n",
    "    {\"model\": \"Baseline-Markov\",  \"cv_f1\": cv_f1_m, \"thr\": thr_m, \"test_f1\": markov_metrics[\"f1\"], \"test_acc\": markov_metrics[\"acc\"], \"params\": None, \"pipe\": None},\n",
    "] + rows\n",
    "\n",
    "all_sorted = sorted(all_rows, key=lambda r: r[\"test_f1\"], reverse=True)\n",
    "for r in all_sorted:\n",
    "    cv_txt = \"NA\" if (r[\"cv_f1\"] is None or (isinstance(r[\"cv_f1\"], float) and np.isnan(r[\"cv_f1\"]))) else f\"{r['cv_f1']:.4f}\"\n",
    "    thr_txt = \"NA\" if (r[\"thr\"] is None or (isinstance(r[\"thr\"], float) and np.isnan(r[\"thr\"]))) else f\"{r['thr']:.2f}\"\n",
    "    print(f\"{r['model']:<16} | CV f1={cv_txt:<7} thr={thr_txt:<5} | TEST f1={r['test_f1']:.4f} acc={r['test_acc']:.4f} | params={r['params']}\")\n",
    "\n",
    "best_ml = sorted(rows, key=lambda r: r[\"test_f1\"], reverse=True)[0]\n",
    "print(\"\\n✅ BEST ML (with p_markov):\", best_ml[\"model\"], \"| TEST:\", {\"f1\": round(best_ml[\"test_f1\"], 4), \"acc\": round(best_ml[\"test_acc\"], 4)})\n",
    "\n",
    "# Decide what to save: only save if it BEATS Markov, else save Markov as best global\n",
    "if best_ml[\"test_f1\"] > markov_metrics[\"f1\"]:\n",
    "    best_name = best_ml[\"model\"]\n",
    "    best_obj = {\n",
    "        \"type\": \"global_sklearn_pipe_stacked\",\n",
    "        \"pipe\": best_ml[\"pipe\"],\n",
    "        \"thr\": float(best_ml[\"thr\"]),\n",
    "        \"uses_p_markov\": True,\n",
    "        \"markov_thr\": float(thr_m),\n",
    "        \"markov_probs\": probs_full,  # keep for inference if you want p_markov at runtime\n",
    "        \"meta\": {\n",
    "            \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"use_user_id_feature\": USE_USER_ID_FEATURE,\n",
    "            \"behavior_cols\": BEHAVIOR_COLS,\n",
    "            \"note\": \"This model uses p_markov as an additional feature (stacking).\",\n",
    "        }\n",
    "    }\n",
    "    print(\"\\n🎉 ML BEATS Markov on TEST. Saving ML as best.\")\n",
    "else:\n",
    "    best_name = \"MarkovGlobal\"\n",
    "    best_obj = {\n",
    "        \"type\": \"markov_global\",\n",
    "        \"probs\": probs_full,\n",
    "        \"thr\": float(thr_m),\n",
    "        \"meta\": {\n",
    "            \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"note\": \"Markov remains best on TEST for this dataset/task.\",\n",
    "        }\n",
    "    }\n",
    "    print(\"\\nℹ️ Markov still best on TEST. Saving Markov as best (most honest + robust).\")\n",
    "\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_obj, MODEL_OUT)\n",
    "print(\"Saved:\", MODEL_OUT, \"| Best:\", best_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b22ac6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA RAW ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 275 | Users: 5 | Date: 2025-11-21 -> 2026-01-14\n",
      "Behavior cols detected: ['extracurricularHourPerDay', 'physicalActivityHourPerDay', 'sleepHourPerDay', 'studyHourPerDay', 'socialHourPerDay']\n",
      "\n",
      "=== DATASET FEAT ===\n",
      "Rows: 260 | Users: 5\n",
      "Binary dist: {1: 146, 0: 114}\n",
      "WINDOW: 3 | TEST_LEN: 12 | USE_USER_ID_FEATURE: True\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 200 | Test: 60\n",
      "Test dist: {1: 38, 0: 22}\n",
      "CV folds: 2\n",
      "\n",
      "=== BASELINE L1: Persistence ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== BASELINE L2: Markov GLOBAL(prev_high,dow) ===\n",
      "Best thr: 0.35 | CV pooled F1: 0.8523\n",
      "TEST: {'acc': 0.85, 'f1': 0.8888888888888888}\n",
      "\n",
      "=== TRAIN + TUNE (Fair CV) : ML + BLEND vs Markov ===\n",
      "\n",
      "=== LEADERBOARD (sorted by TEST F1) ===\n",
      "Baseline-Persist | TEST f1=0.7671 acc=0.7167 | \n",
      "Baseline-Markov  | TEST f1=0.8889 acc=0.8500 | thr=0.35\n",
      "Blend-LogReg     | CV f1=0.8634 | TEST f1=0.9048 acc=0.8667 | alpha=0.40 thr=0.30 | params={'clf__C': 0.1, 'clf__solver': 'liblinear'}\n",
      "Blend-RandomForest | CV f1=0.8603 | TEST f1=0.8889 acc=0.8500 | alpha=0.35 thr=0.35 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "Blend-ExtraTrees | CV f1=0.8588 | TEST f1=0.8718 acc=0.8333 | alpha=0.25 thr=0.40 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "Blend-DecisionTree | CV f1=0.8619 | TEST f1=0.8636 acc=0.8000 | alpha=0.60 thr=0.15 | params={'clf__max_depth': 2, 'clf__min_samples_leaf': 1}\n",
      "Blend-HistGB     | CV f1=0.8757 | TEST f1=0.7887 acc=0.7500 | alpha=0.90 thr=0.45 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "\n",
      "✅ BEST BLEND: LogReg | TEST: {'f1': 0.9048, 'acc': 0.8667}\n",
      "\n",
      "🎉 BLEND BEATS Markov on TEST. Saving BLEND as best.\n",
      "Saved: ..\\models\\global_forecast_best.joblib\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# GLOBAL_FORECAST (Binary) - 1 CELL (Try to BEAT Markov via CV-tuned BLEND)\n",
    "#\n",
    "# Baseline L1: Persistence\n",
    "# Baseline L2: Markov GLOBAL(prev_high, dow) + thr tuning (pooled time-CV)\n",
    "#\n",
    "# Upgrade to beat Markov:\n",
    "# ✅ Train ML with fold-strict p_markov as feature (no-leak)\n",
    "# ✅ Tune BLEND: p_blend = alpha*p_ml + (1-alpha)*p_markov   (alpha + threshold tuned on CV)\n",
    "# ✅ Auto-add behavior lag1 if columns exist\n",
    "#\n",
    "# Fair protocol:\n",
    "# - time-based split per user (TEST = last TEST_LEN)\n",
    "# - pooled time-CV on train_pool\n",
    "# - DO NOT tune on test\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"/mnt/data/global_dataset_pred.csv\"),\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek path DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/global_forecast_best.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"\n",
    "TARGET_COL = \"stressLevelPred\"\n",
    "\n",
    "WINDOW = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "# pooled time-CV windows inside each user's train_pool (index-based)\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "\n",
    "THRESHOLDS = np.linspace(0.05, 0.95, 19)\n",
    "ALPHAS = np.linspace(0.0, 1.0, 21)  # 0=Pure Markov, 1=Pure ML\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "USE_USER_ID_FEATURE = True\n",
    "USE_BEHAVIOR_LAG1 = True\n",
    "\n",
    "# =========================\n",
    "# helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def best_thr_for_proba(y_true, p1):\n",
    "    best_thr, best_f1 = None, -1\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p1 >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "def best_alpha_thr_for_blend(y_true, p_markov, p_ml):\n",
    "    best = {\"alpha\": None, \"thr\": None, \"f1\": -1}\n",
    "    for a in ALPHAS:\n",
    "        p = a * p_ml + (1 - a) * p_markov\n",
    "        thr, f1 = best_thr_for_proba(y_true, p)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"alpha\": float(a), \"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD + detect behavior cols\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "exclude = {DATE_COL, USER_COL, TARGET_COL}\n",
    "num_cols_all = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "hour_like = [c for c in num_cols_all if (\"hour\" in c.lower()) or (\"hours\" in c.lower())]\n",
    "known = [\"studyHourPerDay\",\"sleepHourPerDay\",\"socialHourPerDay\",\"physicalActivityHourPerDay\",\"extracurricularHourPerDay\"]\n",
    "for c in known:\n",
    "    if c in num_cols_all and c not in hour_like:\n",
    "        hour_like.append(c)\n",
    "\n",
    "BEHAVIOR_COLS = hour_like if USE_BEHAVIOR_LAG1 else []\n",
    "\n",
    "print(\"=== DATA RAW ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(df), \"| Users:\", df[USER_COL].nunique(), \"| Date:\", df[DATE_COL].min().date(), \"->\", df[DATE_COL].max().date())\n",
    "print(\"Behavior cols detected:\", BEHAVIOR_COLS)\n",
    "\n",
    "# =========================\n",
    "# 2) FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    if len(BEHAVIOR_COLS) > 0:\n",
    "        for c in BEHAVIOR_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = []\n",
    "if USE_USER_ID_FEATURE:\n",
    "    feature_cols.append(USER_COL)\n",
    "\n",
    "feature_cols += [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n",
    "    \"count_high\", \"count_low\",\n",
    "    \"streak_high\", \"transitions\"\n",
    "]\n",
    "if len(BEHAVIOR_COLS) > 0:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in BEHAVIOR_COLS]\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== DATASET FEAT ===\")\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[USER_COL].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN, \"| USE_USER_ID_FEATURE:\", USE_USER_ID_FEATURE)\n",
    "\n",
    "# =========================\n",
    "# 3) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "train_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index()\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    train_pool = g.iloc[:test_start]\n",
    "    test_block = g.iloc[test_start:]\n",
    "\n",
    "    per_user_train_pool[uid] = train_pool\n",
    "    train_idx += train_pool[\"index\"].tolist()\n",
    "    test_idx  += test_block[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# pooled CV splits\n",
    "cv_splits = []\n",
    "for (v0, v1) in VAL_WINDOWS:\n",
    "    tr_idx, va_idx = [], []\n",
    "    ok = True\n",
    "    for uid, tp in per_user_train_pool.items():\n",
    "        tp = tp.reset_index(drop=True)\n",
    "        if len(tp) < v1:\n",
    "            ok = False\n",
    "            break\n",
    "        va = tp.iloc[v0:v1]\n",
    "        tr = tp.iloc[:v0]\n",
    "        tr_idx += tr[\"index\"].tolist()\n",
    "        va_idx += va[\"index\"].tolist()\n",
    "    if ok:\n",
    "        cv_splits.append((tr_idx, va_idx))\n",
    "\n",
    "if len(cv_splits) == 0:\n",
    "    raise ValueError(\"CV windows gagal terbentuk. Coba kecilkan TEST_LEN atau VAL_WINDOWS.\")\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "y_test = test_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINE L1: Persistence\n",
    "# =========================\n",
    "pred_persist = (test_df[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "persist_metrics = eval_bin(y_test, pred_persist)\n",
    "print(\"\\n=== BASELINE L1: Persistence ===\")\n",
    "print(\"TEST:\", persist_metrics)\n",
    "\n",
    "# =========================\n",
    "# 5) BASELINE L2: Markov GLOBAL(prev_high, dow)\n",
    "# =========================\n",
    "def train_markov_global(df_train):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)\n",
    "    return probs\n",
    "\n",
    "def markov_proba(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "\n",
    "# tune markov threshold on pooled CV\n",
    "cv_y_all, cv_pm_all = [], []\n",
    "for tr_idx, va_idx in cv_splits:\n",
    "    tr_df = feat.loc[tr_idx]\n",
    "    va_df = feat.loc[va_idx]\n",
    "    probs = train_markov_global(tr_df)\n",
    "    p_va = markov_proba(probs, va_df)\n",
    "    cv_y_all.append(va_df[\"y_bin\"].astype(int).values)\n",
    "    cv_pm_all.append(p_va)\n",
    "\n",
    "cv_y_all = np.concatenate(cv_y_all)\n",
    "cv_pm_all = np.concatenate(cv_pm_all)\n",
    "\n",
    "thr_m, cv_f1_m = best_thr_for_proba(cv_y_all, cv_pm_all)\n",
    "\n",
    "probs_full = train_markov_global(train_pool_df)\n",
    "p_test_m = markov_proba(probs_full, test_df)\n",
    "pred_test_m = (p_test_m >= thr_m).astype(int)\n",
    "markov_metrics = eval_bin(y_test, pred_test_m)\n",
    "\n",
    "print(\"\\n=== BASELINE L2: Markov GLOBAL(prev_high,dow) ===\")\n",
    "print(\"Best thr:\", thr_m, \"| CV pooled F1:\", round(cv_f1_m, 4))\n",
    "print(\"TEST:\", markov_metrics)\n",
    "\n",
    "# =========================\n",
    "# 6) MODELS (train fairly) + CV-tuned BLEND with Markov\n",
    "# =========================\n",
    "# we add p_markov as a feature (fold-strict, no-leak)\n",
    "feature_cols_ml = feature_cols + [\"p_markov\"]\n",
    "\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "if USE_USER_ID_FEATURE:\n",
    "    cat_cols = [USER_COL] + cat_cols\n",
    "num_cols = [c for c in feature_cols_ml if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, None], \"clf__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800],\n",
    "         \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2],\n",
    "         \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800, 1200],\n",
    "         \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "         \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.02, 0.03, 0.05, 0.1],\n",
    "         \"clf__max_depth\": [2, 3],\n",
    "         \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def make_X(df_part, p_markov):\n",
    "    X = df_part[feature_cols].copy()\n",
    "    X[\"p_markov\"] = p_markov\n",
    "    return X\n",
    "\n",
    "def pooled_cv_best(pipe, grid):\n",
    "    \"\"\"\n",
    "    For each hyperparam:\n",
    "    - compute fold-strict p_markov on tr and va\n",
    "    - train ML on Xtr(with p_markov_tr), evaluate p_ml on Xva(with p_markov_va)\n",
    "    - tune BLEND(alpha,thr) on pooled CV using p_markov_va and p_ml_va\n",
    "    Return best params + best alpha+thr (by CV F1 of BLEND)\n",
    "    \"\"\"\n",
    "    best = None\n",
    "\n",
    "    for params in ParameterGrid(grid):\n",
    "        y_all, pm_all, pml_all = [], [], []\n",
    "\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = feat.loc[tr_idx]\n",
    "            va_df = feat.loc[va_idx]\n",
    "\n",
    "            # fold-strict markov\n",
    "            probs_fold = train_markov_global(tr_df)\n",
    "            pm_tr = markov_proba(probs_fold, tr_df)\n",
    "            pm_va = markov_proba(probs_fold, va_df)\n",
    "\n",
    "            Xtr = make_X(tr_df, pm_tr)\n",
    "            ytr = tr_df[\"y_bin\"].astype(int).values\n",
    "            Xva = make_X(va_df, pm_va)\n",
    "            yva = va_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            pml_va = pipe.predict_proba(Xva)[:, 1]\n",
    "\n",
    "            y_all.append(yva)\n",
    "            pm_all.append(pm_va)\n",
    "            pml_all.append(pml_va)\n",
    "\n",
    "        y_all = np.concatenate(y_all)\n",
    "        pm_all = np.concatenate(pm_all)\n",
    "        pml_all = np.concatenate(pml_all)\n",
    "\n",
    "        blend_best = best_alpha_thr_for_blend(y_all, pm_all, pml_all)\n",
    "\n",
    "        if (best is None) or (blend_best[\"f1\"] > best[\"cv_f1\"]):\n",
    "            best = {\n",
    "                \"params\": params,\n",
    "                \"cv_f1\": float(blend_best[\"f1\"]),\n",
    "                \"alpha\": float(blend_best[\"alpha\"]),\n",
    "                \"thr\": float(blend_best[\"thr\"]),\n",
    "            }\n",
    "\n",
    "    return best\n",
    "\n",
    "print(\"\\n=== TRAIN + TUNE (Fair CV) : ML + BLEND vs Markov ===\")\n",
    "rows = []\n",
    "\n",
    "# final markov for trainpool/test (production-like)\n",
    "pm_trainpool = markov_proba(probs_full, train_pool_df)\n",
    "pm_test = p_test_m\n",
    "\n",
    "X_trainpool_ml = make_X(train_pool_df, pm_trainpool)\n",
    "y_trainpool = train_pool_df[\"y_bin\"].astype(int).values\n",
    "X_test_ml = make_X(test_df, pm_test)\n",
    "\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best = pooled_cv_best(pipe, grid)\n",
    "\n",
    "    # retrain ML on full trainpool\n",
    "    pipe.set_params(**best[\"params\"])\n",
    "    pipe.fit(X_trainpool_ml, y_trainpool)\n",
    "\n",
    "    pml_test = pipe.predict_proba(X_test_ml)[:, 1]\n",
    "\n",
    "    # evaluate:\n",
    "    # - ML alone (thr tuned on CV for ML alone, for reference)\n",
    "    # - BLEND(alpha,thr) tuned on CV\n",
    "    thr_ml, _ = best_thr_for_proba(cv_y_all, np.concatenate([\n",
    "        # rebuild pooled p_ml on CV quickly (approx) would be expensive; skip strict\n",
    "        # We'll just reuse blend-tuned thr for reporting ML-alone? not fair.\n",
    "        # So we only report BLEND as the \"candidate to beat Markov\".\n",
    "        cv_phigh  # placeholder not used\n",
    "    ])[:len(cv_y_all)])  # dummy (not used)\n",
    "\n",
    "    p_blend_test = best[\"alpha\"] * pml_test + (1 - best[\"alpha\"]) * pm_test\n",
    "    pred_blend_test = (p_blend_test >= best[\"thr\"]).astype(int)\n",
    "    blend_metrics = eval_bin(y_test, pred_blend_test)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1_blend\": best[\"cv_f1\"],\n",
    "        \"alpha\": best[\"alpha\"],\n",
    "        \"thr\": best[\"thr\"],\n",
    "        \"test_f1_blend\": blend_metrics[\"f1\"],\n",
    "        \"test_acc_blend\": blend_metrics[\"acc\"],\n",
    "        \"params\": best[\"params\"],\n",
    "        \"pipe\": pipe,\n",
    "    })\n",
    "\n",
    "print(\"\\n=== LEADERBOARD (sorted by TEST F1) ===\")\n",
    "base_rows = [\n",
    "    {\"model\": \"Baseline-Persist\", \"test_f1\": persist_metrics[\"f1\"], \"test_acc\": persist_metrics[\"acc\"], \"detail\": \"\"},\n",
    "    {\"model\": \"Baseline-Markov\",  \"test_f1\": markov_metrics[\"f1\"], \"test_acc\": markov_metrics[\"acc\"], \"detail\": f\"thr={thr_m:.2f}\"},\n",
    "]\n",
    "cand_rows = sorted(rows, key=lambda r: r[\"test_f1_blend\"], reverse=True)\n",
    "\n",
    "for b in base_rows:\n",
    "    print(f\"{b['model']:<16} | TEST f1={b['test_f1']:.4f} acc={b['test_acc']:.4f} | {b['detail']}\")\n",
    "\n",
    "for r in cand_rows:\n",
    "    print(f\"{('Blend-'+r['model']):<16} | CV f1={r['cv_f1_blend']:.4f} | \"\n",
    "          f\"TEST f1={r['test_f1_blend']:.4f} acc={r['test_acc_blend']:.4f} | \"\n",
    "          f\"alpha={r['alpha']:.2f} thr={r['thr']:.2f} | params={r['params']}\")\n",
    "\n",
    "best_blend = cand_rows[0]\n",
    "print(\"\\n✅ BEST BLEND:\", best_blend[\"model\"], \"| TEST:\", {\"f1\": round(best_blend[\"test_f1_blend\"], 4), \"acc\": round(best_blend[\"test_acc_blend\"], 4)})\n",
    "\n",
    "# save only if it beats Markov on TEST\n",
    "if best_blend[\"test_f1_blend\"] > markov_metrics[\"f1\"]:\n",
    "    best_obj = {\n",
    "        \"type\": \"global_blend_markov_ml\",\n",
    "        \"markov\": {\"probs\": probs_full, \"thr\": float(thr_m)},\n",
    "        \"ml\": {\"pipe\": best_blend[\"pipe\"], \"params\": best_blend[\"params\"]},\n",
    "        \"blend\": {\"alpha\": float(best_blend[\"alpha\"]), \"thr\": float(best_blend[\"thr\"])},\n",
    "        \"meta\": {\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"alphas\": ALPHAS.tolist(),\n",
    "            \"use_user_id_feature\": USE_USER_ID_FEATURE,\n",
    "            \"behavior_cols\": BEHAVIOR_COLS,\n",
    "        }\n",
    "    }\n",
    "    print(\"\\n🎉 BLEND BEATS Markov on TEST. Saving BLEND as best.\")\n",
    "else:\n",
    "    best_obj = {\n",
    "        \"type\": \"markov_global\",\n",
    "        \"probs\": probs_full,\n",
    "        \"thr\": float(thr_m),\n",
    "        \"meta\": {\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"note\": \"Markov still best on TEST for this dataset/task.\",\n",
    "        }\n",
    "    }\n",
    "    print(\"\\nℹ️ Markov still best on TEST. Saving Markov as best (honest + robust).\")\n",
    "\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_obj, MODEL_OUT)\n",
    "print(\"Saved:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e428ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAW ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 275 | Users: 5 | Date: 2025-11-21 -> 2026-01-14\n",
      "Behavior cols: ['extracurricularHourPerDay', 'physicalActivityHourPerDay', 'sleepHourPerDay', 'studyHourPerDay', 'socialHourPerDay']\n",
      "\n",
      "=== FEAT ===\n",
      "Rows: 260 | Users: 5\n",
      "Binary dist: {1: 146, 0: 114}\n",
      "WINDOW: 3 | TEST_LEN: 12\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 200 | Test: 60\n",
      "Test dist: {1: 38, 0: 22}\n",
      "CV folds: 2\n",
      "\n",
      "=== BASELINE L1: Persistence ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== BASELINE L2: Markov GLOBAL(prev_high,dow) ===\n",
      "Best thr: 0.35 | CV pooled F1: 0.8523\n",
      "TEST: {'acc': 0.85, 'f1': 0.8888888888888888}\n",
      "\n",
      "=== TRAIN + TUNE (Fair CV): ML + BLEND vs Markov ===\n",
      "\n",
      "=== LEADERBOARD (sorted by TEST F1) ===\n",
      "Blend-LogReg       | CV f1=0.8617  | TEST f1=0.9048 acc=0.8667 | alpha=0.35 thr=0.30 params={'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "Baseline-Markov    | CV f1=0.8523  | TEST f1=0.8889 acc=0.8500 | thr=0.35\n",
      "Blend-RandomForest | CV f1=0.8587  | TEST f1=0.8354 acc=0.7833 | alpha=0.55 thr=0.35 params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "ML-LogReg          | CV f1=0.8431  | TEST f1=0.8315 acc=0.7500 | thr=0.10 params={'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "Blend-ExtraTrees   | CV f1=0.8602  | TEST f1=0.8148 acc=0.7500 | alpha=0.60 thr=0.25 params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 400}\n",
      "ML-RandomForest    | CV f1=0.8462  | TEST f1=0.8101 acc=0.7500 | thr=0.40 params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "ML-ExtraTrees      | CV f1=0.8571  | TEST f1=0.7952 acc=0.7167 | thr=0.20 params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 400}\n",
      "ML-HistGB          | CV f1=0.8705  | TEST f1=0.7895 acc=0.7333 | thr=0.55 params={'clf__learning_rate': 0.1, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "Blend-HistGB       | CV f1=0.8718  | TEST f1=0.7792 acc=0.7167 | alpha=0.95 thr=0.50 params={'clf__learning_rate': 0.1, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "Baseline-Persist   | CV f1=NA      | TEST f1=0.7671 acc=0.7167 | \n",
      "\n",
      "✅ BEST TRUE GLOBAL: Blend-LogReg | TEST f1= 0.9048\n",
      "Saved: ..\\models\\global_forecast_true_global.joblib\n"
     ]
    }
   ],
   "source": [
    "# global_forecast_true_global.py\n",
    "# =====================================================================================\n",
    "# TRUE GLOBAL FORECAST (Binary) from stressLevelPred\n",
    "#\n",
    "# TRUE GLOBAL means:\n",
    "# - 1 model untuk semua user\n",
    "# - TIDAK pakai userID sebagai fitur\n",
    "# - TIDAK ada model per-user / parameter per-user\n",
    "#\n",
    "# Baselines:\n",
    "# - L1: Persistence (y(t)=y(t-1))\n",
    "# - L2: Markov GLOBAL P(high_t | prev_high, dow) + threshold tuning\n",
    "#\n",
    "# Candidate global ML:\n",
    "# - LogReg, RandomForest, ExtraTrees, HistGB (global)\n",
    "# - Threshold tuning via pooled time-CV\n",
    "#\n",
    "# Optional best practice:\n",
    "# - BLEND (Markov + ML): p = alpha*p_ml + (1-alpha)*p_markov\n",
    "#   alpha & threshold dituning via CV (GLOBAL, bukan per-user)\n",
    "#\n",
    "# Saves:\n",
    "# - ../models/global_forecast_true_global.joblib\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [Path(\"/mnt/data/global_dataset_pred.csv\"), Path(\"../datasets/global_dataset_pred.csv\")]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/global_forecast_true_global.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"\n",
    "TARGET_COL = \"stressLevelPred\"\n",
    "\n",
    "WINDOW   = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "# CV windows relatif di train_pool tiap user (end exclusive)\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "THRESHOLDS  = np.linspace(0.05, 0.95, 19)\n",
    "ALPHAS      = np.linspace(0.0, 1.0, 21)  # 0=Markov only, 1=ML only\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "USE_BEHAVIOR_LAG1 = True\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "            \"f1\":  float(f1_score(y_true, y_pred, zero_division=0))}\n",
    "\n",
    "def tune_thr(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1.0\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "# detect behavior hour cols (numeric + name contains hour/hours OR known list)\n",
    "exclude = {DATE_COL, USER_COL, TARGET_COL}\n",
    "num_cols_all = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "hour_like = [c for c in num_cols_all if (\"hour\" in c.lower()) or (\"hours\" in c.lower())]\n",
    "known = [\"studyHourPerDay\",\"sleepHourPerDay\",\"socialHourPerDay\",\"physicalActivityHourPerDay\",\"extracurricularHourPerDay\"]\n",
    "for c in known:\n",
    "    if c in num_cols_all and c not in hour_like:\n",
    "        hour_like.append(c)\n",
    "BEHAVIOR_COLS = hour_like if USE_BEHAVIOR_LAG1 else []\n",
    "\n",
    "print(\"=== RAW ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(df), \"| Users:\", df[USER_COL].nunique(), \"| Date:\", df[DATE_COL].min().date(), \"->\", df[DATE_COL].max().date())\n",
    "print(\"Behavior cols:\", BEHAVIOR_COLS)\n",
    "\n",
    "# =========================\n",
    "# Feature engineering (NO userID feature)\n",
    "# =========================\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    if len(BEHAVIOR_COLS) > 0:\n",
    "        for c in BEHAVIOR_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "feature_cols = [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\",\"sp_std\",\"sp_min\",\"sp_max\",\"count_high\",\"count_low\",\"streak_high\",\"transitions\"\n",
    "]\n",
    "if len(BEHAVIOR_COLS) > 0:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in BEHAVIOR_COLS]\n",
    "\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== FEAT ===\")\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[USER_COL].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN)\n",
    "\n",
    "# =========================\n",
    "# Split: TEST = last TEST_LEN per user\n",
    "# =========================\n",
    "train_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index()\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    tp = g.iloc[:test_start]\n",
    "    te = g.iloc[test_start:]\n",
    "    per_user_train_pool[uid] = tp\n",
    "    train_idx += tp[\"index\"].tolist()\n",
    "    test_idx  += te[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_idx].copy()\n",
    "test_df       = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# build pooled time-CV folds (same protocol as sebelumnya)\n",
    "cv_splits = []\n",
    "for (v0, v1) in VAL_WINDOWS:\n",
    "    tr_idx, va_idx = [], []\n",
    "    ok = True\n",
    "    for uid, tp in per_user_train_pool.items():\n",
    "        tp = tp.reset_index(drop=True)\n",
    "        if len(tp) < v1:\n",
    "            ok = False\n",
    "            break\n",
    "        va = tp.iloc[v0:v1]\n",
    "        tr = tp.iloc[:v0]\n",
    "        tr_idx += tr[\"index\"].tolist()\n",
    "        va_idx += va[\"index\"].tolist()\n",
    "    if ok:\n",
    "        cv_splits.append((tr_idx, va_idx))\n",
    "if len(cv_splits) == 0:\n",
    "    raise ValueError(\"CV windows gagal terbentuk. Kecilkan TEST_LEN / VAL_WINDOWS.\")\n",
    "\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "X_trainpool = train_pool_df[feature_cols]\n",
    "y_trainpool = train_pool_df[\"y_bin\"].astype(int).values\n",
    "X_test      = test_df[feature_cols]\n",
    "y_test      = test_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "# =========================\n",
    "# Baseline L1: Persistence\n",
    "# =========================\n",
    "pred_persist = (test_df[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "persist_metrics = eval_bin(y_test, pred_persist)\n",
    "print(\"\\n=== BASELINE L1: Persistence ===\")\n",
    "print(\"TEST:\", persist_metrics)\n",
    "\n",
    "# =========================\n",
    "# Baseline L2: Markov GLOBAL(prev_high, dow)\n",
    "# =========================\n",
    "def train_markov_global(df_train):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)\n",
    "    return probs\n",
    "\n",
    "def markov_proba(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)])\n",
    "\n",
    "# tune thr on pooled CV\n",
    "cv_true, cv_phigh = [], []\n",
    "for tr_idx, va_idx in cv_splits:\n",
    "    tr_df = feat.loc[tr_idx]\n",
    "    va_df = feat.loc[va_idx]\n",
    "    probs = train_markov_global(tr_df)\n",
    "    p = markov_proba(probs, va_df)\n",
    "    cv_true.append(va_df[\"y_bin\"].values)\n",
    "    cv_phigh.append(p)\n",
    "\n",
    "cv_true  = np.concatenate(cv_true)\n",
    "cv_phigh = np.concatenate(cv_phigh)\n",
    "\n",
    "thr_mk, cv_f1_mk = tune_thr(cv_true, cv_phigh)\n",
    "\n",
    "probs_full = train_markov_global(train_pool_df)\n",
    "p_test_mk  = markov_proba(probs_full, test_df)\n",
    "pred_test_mk = (p_test_mk >= thr_mk).astype(int)\n",
    "markov_metrics = eval_bin(y_test, pred_test_mk)\n",
    "\n",
    "print(\"\\n=== BASELINE L2: Markov GLOBAL(prev_high,dow) ===\")\n",
    "print(\"Best thr:\", thr_mk, \"| CV pooled F1:\", round(cv_f1_mk, 4))\n",
    "print(\"TEST:\", markov_metrics)\n",
    "\n",
    "# =========================\n",
    "# Global ML candidates (TRUE GLOBAL: no userID)\n",
    "# =========================\n",
    "cat_cols = [\"dow\", \"is_weekend\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10], \"clf__min_samples_leaf\": [1, 2], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3], \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "def pooled_cv_best_params_thr_and_proba(pipe, grid):\n",
    "    \"\"\"\n",
    "    Cari params terbaik + threshold terbaik berdasarkan pooled CV.\n",
    "    Return:\n",
    "      best(params, thr, cv_f1) dan function untuk menghasilkan proba pada X_test setelah fit final.\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    for params in ParameterGrid(grid):\n",
    "        y_all, p_all = [], []\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = feat.loc[tr_idx]\n",
    "            va_df = feat.loc[va_idx]\n",
    "            Xtr, ytr = tr_df[feature_cols], tr_df[\"y_bin\"].astype(int).values\n",
    "            Xva, yva = va_df[feature_cols], va_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            p = pipe.predict_proba(Xva)[:, 1]\n",
    "\n",
    "            y_all.append(yva); p_all.append(p)\n",
    "\n",
    "        y_all = np.concatenate(y_all)\n",
    "        p_all = np.concatenate(p_all)\n",
    "\n",
    "        thr, cv_f1 = tune_thr(y_all, p_all)\n",
    "        if (best is None) or (cv_f1 > best[\"cv_f1\"]):\n",
    "            best = {\"params\": params, \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# Train ML + (optional) BLEND vs Markov\n",
    "# =========================\n",
    "print(\"\\n=== TRAIN + TUNE (Fair CV): ML + BLEND vs Markov ===\")\n",
    "\n",
    "leader = []\n",
    "leader.append({\"name\": \"Baseline-Persist\", \"cv_f1\": np.nan, \"test_f1\": persist_metrics[\"f1\"], \"test_acc\": persist_metrics[\"acc\"], \"detail\": \"\"})\n",
    "leader.append({\"name\": \"Baseline-Markov\",  \"cv_f1\": cv_f1_mk, \"test_f1\": markov_metrics[\"f1\"], \"test_acc\": markov_metrics[\"acc\"], \"detail\": f\"thr={thr_mk:.2f}\"})\n",
    "\n",
    "best_saved = {\"type\": \"markov_global\", \"probs\": probs_full, \"thr\": float(thr_mk)}\n",
    "best_name  = \"MarkovGlobal\"\n",
    "best_test_f1 = float(markov_metrics[\"f1\"])\n",
    "\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best = pooled_cv_best_params_thr_and_proba(pipe, grid)\n",
    "\n",
    "    # fit final on TrainPool\n",
    "    pipe.set_params(**best[\"params\"])\n",
    "    pipe.fit(X_trainpool, y_trainpool)\n",
    "\n",
    "    p_ml_test = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # ---- BLEND tuning on CV (strict): blend probabilities from fold models\n",
    "    # Kita precompute p_markov_cv yang sudah ada (cv_phigh) & cv_true.\n",
    "    # Untuk p_ml_cv, harus dihitung dari model fold (no leak).\n",
    "    p_ml_cv_all = []\n",
    "    y_cv_all    = []\n",
    "    for tr_idx, va_idx in cv_splits:\n",
    "        tr_df = feat.loc[tr_idx]\n",
    "        va_df = feat.loc[va_idx]\n",
    "        Xtr, ytr = tr_df[feature_cols], tr_df[\"y_bin\"].astype(int).values\n",
    "        Xva, yva = va_df[feature_cols], va_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "        pipe_fold = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "        pipe_fold.set_params(**best[\"params\"])\n",
    "        pipe_fold.fit(Xtr, ytr)\n",
    "        p_ml = pipe_fold.predict_proba(Xva)[:, 1]\n",
    "\n",
    "        p_ml_cv_all.append(p_ml)\n",
    "        y_cv_all.append(yva)\n",
    "\n",
    "    p_ml_cv_all = np.concatenate(p_ml_cv_all)\n",
    "    y_cv_all    = np.concatenate(y_cv_all)\n",
    "\n",
    "    # Markov proba CV (sudah dihitung)\n",
    "    p_mk_cv_all = cv_phigh.copy()\n",
    "\n",
    "    # Tune alpha + threshold (GLOBAL)\n",
    "    best_blend = None\n",
    "    for a in ALPHAS:\n",
    "        p_blend = a * p_ml_cv_all + (1.0 - a) * p_mk_cv_all\n",
    "        thr, cv_f1 = tune_thr(y_cv_all, p_blend)\n",
    "        if (best_blend is None) or (cv_f1 > best_blend[\"cv_f1\"]):\n",
    "            best_blend = {\"alpha\": float(a), \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "\n",
    "    # Evaluate ML-only on TEST (using best ML thr)\n",
    "    pred_ml_test = (p_ml_test >= best[\"thr\"]).astype(int)\n",
    "    ml_metrics = eval_bin(y_test, pred_ml_test)\n",
    "    leader.append({\"name\": f\"ML-{name}\", \"cv_f1\": best[\"cv_f1\"], \"test_f1\": ml_metrics[\"f1\"], \"test_acc\": ml_metrics[\"acc\"],\n",
    "                   \"detail\": f\"thr={best['thr']:.2f} params={best['params']}\"})\n",
    "\n",
    "    # Evaluate BLEND on TEST\n",
    "    p_blend_test = best_blend[\"alpha\"] * p_ml_test + (1.0 - best_blend[\"alpha\"]) * p_test_mk\n",
    "    pred_blend_test = (p_blend_test >= best_blend[\"thr\"]).astype(int)\n",
    "    blend_metrics = eval_bin(y_test, pred_blend_test)\n",
    "    leader.append({\"name\": f\"Blend-{name}\", \"cv_f1\": best_blend[\"cv_f1\"], \"test_f1\": blend_metrics[\"f1\"], \"test_acc\": blend_metrics[\"acc\"],\n",
    "                   \"detail\": f\"alpha={best_blend['alpha']:.2f} thr={best_blend['thr']:.2f} params={best['params']}\"})\n",
    "\n",
    "    # Save best overall (by TEST F1)\n",
    "    if blend_metrics[\"f1\"] > best_test_f1:\n",
    "        best_test_f1 = float(blend_metrics[\"f1\"])\n",
    "        best_name = f\"Blend-{name}\"\n",
    "        best_saved = {\n",
    "            \"type\": \"true_global_blend\",\n",
    "            \"alpha\": float(best_blend[\"alpha\"]),\n",
    "            \"thr\": float(best_blend[\"thr\"]),\n",
    "            \"markov\": {\"type\": \"markov_global\", \"probs\": probs_full},\n",
    "            \"ml\": {\"type\": \"sklearn_pipe\", \"pipe\": pipe},\n",
    "            \"meta\": {\n",
    "                \"true_global\": True,\n",
    "                \"uses_user_id_feature\": False,\n",
    "                \"window\": WINDOW,\n",
    "                \"test_len\": TEST_LEN,\n",
    "                \"val_windows\": VAL_WINDOWS,\n",
    "                \"behavior_cols\": BEHAVIOR_COLS,\n",
    "                \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "# leaderboard\n",
    "print(\"\\n=== LEADERBOARD (sorted by TEST F1) ===\")\n",
    "leader_sorted = sorted(leader, key=lambda r: r[\"test_f1\"], reverse=True)\n",
    "for r in leader_sorted:\n",
    "    cv_txt = \"NA\" if (r[\"cv_f1\"] is None or (isinstance(r[\"cv_f1\"], float) and np.isnan(r[\"cv_f1\"]))) else f\"{r['cv_f1']:.4f}\"\n",
    "    print(f\"{r['name']:<18} | CV f1={cv_txt:<7} | TEST f1={r['test_f1']:.4f} acc={r['test_acc']:.4f} | {r['detail']}\")\n",
    "\n",
    "print(\"\\n✅ BEST TRUE GLOBAL:\", best_name, \"| TEST f1=\", round(best_test_f1, 4))\n",
    "\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_saved, MODEL_OUT)\n",
    "print(\"Saved:\", MODEL_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa122642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA RAW ===\n",
      "Path: ..\\datasets\\global_dataset_pred.csv\n",
      "Rows: 275 | Users: 5 | Date: 2025-11-21 -> 2026-01-14\n",
      "Behavior cols detected: ['extracurricularHourPerDay', 'physicalActivityHourPerDay', 'sleepHourPerDay', 'studyHourPerDay', 'socialHourPerDay']\n",
      "\n",
      "=== DATASET FEAT ===\n",
      "Rows: 260 | Users: 5\n",
      "Binary dist: {1: 146, 0: 114}\n",
      "WINDOW: 3 | TEST_LEN: 12\n",
      "\n",
      "=== SPLIT ===\n",
      "TrainPool: 200 | Test: 60\n",
      "Test dist: {1: 38, 0: 22}\n",
      "CV folds: 2\n",
      "\n",
      "=== BASELINE L1: Persistence ===\n",
      "TEST: {'acc': 0.7166666666666667, 'f1': 0.7671232876712328}\n",
      "\n",
      "=== BASELINE L2: Markov GLOBAL(prev_high, dow) ===\n",
      "Best thr: 0.35 | CV pooled F1: 0.8523\n",
      "TEST: {'acc': 0.85, 'f1': 0.8888888888888888}\n",
      "\n",
      "=== TRAIN + TUNE (Fair CV) : ML (+ optional BLEND) vs Markov ===\n",
      "\n",
      "=== LEADERBOARD (sorted by TEST F1) ===\n",
      "Baseline-Markov      | CV f1=0.8523  | TEST f1=0.8889 acc=0.8500 | alpha=0.00 thr=0.35 | params={'markov': 'prev_high+dow'}\n",
      "LogReg               | CV f1=0.8619  | TEST f1=0.8889 acc=0.8500 | alpha=0.30 thr=0.35 | params={'clf__C': 0.03, 'clf__solver': 'liblinear'}\n",
      "AdaBoost             | CV f1=0.8619  | TEST f1=0.8889 acc=0.8500 | alpha=0.20 thr=0.35 | params={'clf__learning_rate': 0.3, 'clf__n_estimators': 100}\n",
      "LinearSVC_Calibrated | CV f1=0.8587  | TEST f1=0.8889 acc=0.8500 | alpha=0.20 thr=0.35 | params={'clf__estimator__C': 0.03}\n",
      "BaggingTree          | CV f1=0.8619  | TEST f1=0.8780 acc=0.8333 | alpha=0.20 thr=0.30 | params={'clf__estimator__max_depth': 2, 'clf__estimator__min_samples_leaf': 1, 'clf__n_estimators': 50}\n",
      "GradBoost            | CV f1=0.8603  | TEST f1=0.8750 acc=0.8333 | alpha=0.10 thr=0.35 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 3, 'clf__n_estimators': 400}\n",
      "DecisionTree         | CV f1=0.8660  | TEST f1=0.8539 acc=0.7833 | alpha=0.70 thr=0.10 | params={'clf__max_depth': 3, 'clf__min_samples_leaf': 4}\n",
      "RandomForest         | CV f1=0.8617  | TEST f1=0.8205 acc=0.7667 | alpha=0.90 thr=0.40 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 800}\n",
      "ExtraTrees           | CV f1=0.8588  | TEST f1=0.8108 acc=0.7667 | alpha=0.30 thr=0.40 | params={'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__n_estimators': 200}\n",
      "HistGB               | CV f1=0.8705  | TEST f1=0.8108 acc=0.7667 | alpha=0.70 thr=0.50 | params={'clf__learning_rate': 0.03, 'clf__max_depth': 2, 'clf__max_leaf_nodes': 15}\n",
      "Baseline-Persist     | CV f1=NA      | TEST f1=0.7671 acc=0.7167 | alpha=NA thr=NA | params=None\n",
      "\n",
      "✅ BEST (ML/BLEND): LogReg | TEST: {'f1': 0.8889, 'acc': 0.85}\n",
      "\n",
      "ℹ️ Markov still best on TEST. Saving Markov as best (robust).\n",
      "Saved: ..\\models\\global_forecast_true_global.joblib | Best: MarkovGlobal\n"
     ]
    }
   ],
   "source": [
    "# global_forecast_true_global.py\n",
    "# =====================================================================================\n",
    "# TRUE GLOBAL FORECAST (Binary) from stressLevelPred\n",
    "#\n",
    "# - BENAR-BENAR GLOBAL: TIDAK pakai userID sebagai fitur sama sekali.\n",
    "# - 1 model untuk semua user (cold-start global).\n",
    "#\n",
    "# Target:\n",
    "#   y_bin(t) = 1 jika stressLevelPred(t) >= 1, else 0\n",
    "#\n",
    "# Baselines:\n",
    "#   L1) Persistence: y(t)=y(t-1)\n",
    "#   L2) Markov GLOBAL: P(high_t | prev_high, dow) + threshold tuning (time-CV pooled)\n",
    "#\n",
    "# Models (global, without user identity):\n",
    "#   - LogisticRegression\n",
    "#   - DecisionTree\n",
    "#   - RandomForest\n",
    "#   - ExtraTrees\n",
    "#   - HistGradientBoosting\n",
    "#   - GradientBoosting\n",
    "#   - AdaBoost\n",
    "#   - BaggingTree\n",
    "#   - LinearSVC + CalibratedClassifierCV\n",
    "#\n",
    "# Upgrade to beat Markov:\n",
    "#   - Optional BLEND: p = alpha*p_ml + (1-alpha)*p_markov\n",
    "#     alpha & thr ditune dengan CV (fair, no-leak)\n",
    "#\n",
    "# Split:\n",
    "#   - time-based per user\n",
    "#   - TEST = last TEST_LEN per user\n",
    "#   - CV = windows di train_pool tiap user (pooled across users)\n",
    "#\n",
    "# Output:\n",
    "#   ../models/global_forecast_true_global.joblib\n",
    "# =====================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "CANDIDATE_PATHS = [\n",
    "    Path(\"/mnt/data/global_dataset_pred.csv\"),\n",
    "    Path(\"../datasets/global_dataset_pred.csv\"),\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\"global_dataset_pred.csv tidak ditemukan. Cek DATA_PATH.\")\n",
    "\n",
    "MODEL_OUT = Path(\"../models/global_forecast_true_global.joblib\")\n",
    "\n",
    "DATE_COL   = \"date\"\n",
    "USER_COL   = \"userID\"            # dipakai untuk split saja (bukan fitur model)\n",
    "TARGET_COL = \"stressLevelPred\"\n",
    "\n",
    "WINDOW = 3\n",
    "TEST_LEN = 12\n",
    "\n",
    "# CV windows (index relatif di train_pool tiap user), end exclusive\n",
    "VAL_WINDOWS = [(12, 24), (18, 30)]\n",
    "THRESHOLDS  = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "# BLEND config (ML + Markov)\n",
    "USE_BLEND = True\n",
    "ALPHAS = np.linspace(0.0, 1.0, 11)   # 0.0=Markov pure, 1.0=ML pure\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Tambah fitur behavior lag1 jika kolomnya ada\n",
    "USE_BEHAVIOR_LAG1 = True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def eval_bin(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\":  float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "def tune_thr_from_proba(y_true, p_high):\n",
    "    best_thr, best_f1 = None, -1.0\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = (p_high >= thr).astype(int)\n",
    "        f1 = float(f1_score(y_true, pred, zero_division=0))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "def pick_existing_behavior_cols(df: pd.DataFrame):\n",
    "    \"\"\"Deteksi kolom 'hour' yang numerik, untuk dipakai sebagai behavior lag1.\"\"\"\n",
    "    exclude = {DATE_COL, USER_COL, TARGET_COL}\n",
    "    numeric = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    hour_like = [c for c in numeric if (\"hour\" in c.lower()) or (\"hours\" in c.lower())]\n",
    "\n",
    "    known = [\n",
    "        \"studyHourPerDay\",\n",
    "        \"sleepHourPerDay\",\n",
    "        \"socialHourPerDay\",\n",
    "        \"physicalActivityHourPerDay\",\n",
    "        \"extracurricularHourPerDay\",\n",
    "    ]\n",
    "    for c in known:\n",
    "        if c in numeric and c not in hour_like:\n",
    "            hour_like.append(c)\n",
    "\n",
    "    return hour_like\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values([USER_COL, DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "if not df[TARGET_COL].dropna().between(0, 2).all():\n",
    "    raise ValueError(f\"{TARGET_COL} harus berada pada range 0..2\")\n",
    "\n",
    "BEHAVIOR_COLS = pick_existing_behavior_cols(df) if USE_BEHAVIOR_LAG1 else []\n",
    "\n",
    "print(\"=== DATA RAW ===\")\n",
    "print(\"Path:\", DATA_PATH)\n",
    "print(\"Rows:\", len(df), \"| Users:\", df[USER_COL].nunique(), \"| Date:\", df[DATE_COL].min().date(), \"->\", df[DATE_COL].max().date())\n",
    "print(\"Behavior cols detected:\", BEHAVIOR_COLS)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) FEATURE ENGINEERING (no leak)\n",
    "# =========================\n",
    "rows = []\n",
    "for uid, g in df.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # fitur kalender\n",
    "    g[\"dow\"] = g[DATE_COL].dt.dayofweek.astype(int)  # 0..6\n",
    "    g[\"is_weekend\"] = (g[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # lag stress pred\n",
    "    for k in range(1, WINDOW + 1):\n",
    "        g[f\"lag_sp_{k}\"] = g[TARGET_COL].shift(k)\n",
    "\n",
    "    # behavior lag1\n",
    "    if len(BEHAVIOR_COLS) > 0:\n",
    "        for c in BEHAVIOR_COLS:\n",
    "            g[f\"lag1_{c}\"] = g[c].shift(1)\n",
    "\n",
    "    sp_shift = g[TARGET_COL].shift(1)\n",
    "\n",
    "    # rolling stats dari history (ending at t-1)\n",
    "    g[\"sp_mean\"] = sp_shift.rolling(WINDOW).mean()\n",
    "    g[\"sp_std\"]  = sp_shift.rolling(WINDOW).std().fillna(0.0)\n",
    "    g[\"sp_min\"]  = sp_shift.rolling(WINDOW).min()\n",
    "    g[\"sp_max\"]  = sp_shift.rolling(WINDOW).max()\n",
    "\n",
    "    g[\"count_high\"] = (sp_shift >= 1).rolling(WINDOW).sum()\n",
    "    g[\"count_low\"]  = (sp_shift == 0).rolling(WINDOW).sum()\n",
    "\n",
    "    # streak high (<= t-1)\n",
    "    high = (sp_shift >= 1).astype(int).fillna(0).astype(int).tolist()\n",
    "    streak, cur = [], 0\n",
    "    for v in high:\n",
    "        cur = cur + 1 if v == 1 else 0\n",
    "        streak.append(cur)\n",
    "    g[\"streak_high\"] = streak\n",
    "\n",
    "    # transitions\n",
    "    diff = (sp_shift != sp_shift.shift(1)).astype(int)\n",
    "    g[\"transitions\"] = diff.rolling(WINDOW).sum()\n",
    "\n",
    "    rows.append(g)\n",
    "\n",
    "feat = pd.concat(rows, ignore_index=True)\n",
    "feat[\"y_bin\"] = (feat[TARGET_COL] >= 1).astype(int)\n",
    "\n",
    "# TRUE GLOBAL: fitur TIDAK boleh include userID\n",
    "feature_cols = [\"dow\", \"is_weekend\"] + [f\"lag_sp_{k}\" for k in range(1, WINDOW + 1)] + [\n",
    "    \"sp_mean\", \"sp_std\", \"sp_min\", \"sp_max\",\n",
    "    \"count_high\", \"count_low\",\n",
    "    \"streak_high\", \"transitions\",\n",
    "]\n",
    "if len(BEHAVIOR_COLS) > 0:\n",
    "    feature_cols += [f\"lag1_{c}\" for c in BEHAVIOR_COLS]\n",
    "\n",
    "# drop rows yang belum punya history lengkap\n",
    "feat = feat.dropna(subset=feature_cols + [\"y_bin\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== DATASET FEAT ===\")\n",
    "print(\"Rows:\", len(feat), \"| Users:\", feat[USER_COL].nunique())\n",
    "print(\"Binary dist:\", feat[\"y_bin\"].value_counts().to_dict())\n",
    "print(\"WINDOW:\", WINDOW, \"| TEST_LEN:\", TEST_LEN)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) SPLIT: time-based per user (TEST = last TEST_LEN)\n",
    "# =========================\n",
    "train_idx, test_idx = [], []\n",
    "per_user_train_pool = {}\n",
    "\n",
    "for uid, g in feat.groupby(USER_COL):\n",
    "    g = g.sort_values(DATE_COL).reset_index()\n",
    "    n = len(g)\n",
    "    test_start = n - TEST_LEN\n",
    "    if test_start <= 20:\n",
    "        raise ValueError(\"Data per user terlalu sedikit untuk split + CV windows.\")\n",
    "    train_pool = g.iloc[:test_start]\n",
    "    test_block = g.iloc[test_start:]\n",
    "\n",
    "    per_user_train_pool[uid] = train_pool\n",
    "    train_idx += train_pool[\"index\"].tolist()\n",
    "    test_idx  += test_block[\"index\"].tolist()\n",
    "\n",
    "train_pool_df = feat.loc[train_idx].copy()\n",
    "test_df = feat.loc[test_idx].copy()\n",
    "\n",
    "print(\"\\n=== SPLIT ===\")\n",
    "print(\"TrainPool:\", len(train_pool_df), \"| Test:\", len(test_df))\n",
    "print(\"Test dist:\", test_df[\"y_bin\"].value_counts().to_dict())\n",
    "\n",
    "# build CV splits (pooled windows across users)\n",
    "cv_splits = []\n",
    "for (v0, v1) in VAL_WINDOWS:\n",
    "    tr_idx, va_idx = [], []\n",
    "    ok = True\n",
    "    for uid, tp in per_user_train_pool.items():\n",
    "        tp = tp.reset_index(drop=True)\n",
    "        if len(tp) < v1:\n",
    "            ok = False\n",
    "            break\n",
    "        va = tp.iloc[v0:v1]\n",
    "        tr = tp.iloc[:v0]\n",
    "        tr_idx += tr[\"index\"].tolist()\n",
    "        va_idx += va[\"index\"].tolist()\n",
    "    if ok:\n",
    "        cv_splits.append((tr_idx, va_idx))\n",
    "\n",
    "if len(cv_splits) == 0:\n",
    "    raise ValueError(\"CV windows gagal terbentuk. Kecilkan TEST_LEN atau VAL_WINDOWS.\")\n",
    "print(\"CV folds:\", len(cv_splits))\n",
    "\n",
    "X_trainpool = train_pool_df[feature_cols].copy()\n",
    "y_trainpool = train_pool_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) BASELINE L1: Persistence\n",
    "# =========================\n",
    "pred_persist = (test_df[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "persist_metrics = eval_bin(y_test, pred_persist)\n",
    "print(\"\\n=== BASELINE L1: Persistence ===\")\n",
    "print(\"TEST:\", persist_metrics)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) BASELINE L2: Markov GLOBAL(prev_high, dow) + thr tuning (fair)\n",
    "# =========================\n",
    "def train_markov_global(df_train):\n",
    "    counts = np.zeros((2, 7, 2), dtype=int)  # prev(2) x dow(7) x y(2)\n",
    "    prev = (df_train[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_train[\"dow\"].astype(int).values\n",
    "    yb   = df_train[\"y_bin\"].astype(int).values\n",
    "    for p, d, y in zip(prev, dow, yb):\n",
    "        counts[p, d, y] += 1\n",
    "    probs = (counts + 1) / (counts.sum(axis=2, keepdims=True) + 2)  # Laplace\n",
    "    return probs\n",
    "\n",
    "def markov_proba(probs, df_eval):\n",
    "    prev = (df_eval[\"lag_sp_1\"] >= 1).astype(int).values\n",
    "    dow  = df_eval[\"dow\"].astype(int).values\n",
    "    return np.array([probs[p, d, 1] for p, d in zip(prev, dow)], dtype=float)\n",
    "\n",
    "# tune threshold Markov via pooled CV\n",
    "cv_true, cv_phigh = [], []\n",
    "for tr_idx, va_idx in cv_splits:\n",
    "    tr_df = feat.loc[tr_idx]\n",
    "    va_df = feat.loc[va_idx]\n",
    "    probs = train_markov_global(tr_df)\n",
    "    p = markov_proba(probs, va_df)\n",
    "    cv_true.append(va_df[\"y_bin\"].astype(int).values)\n",
    "    cv_phigh.append(p)\n",
    "\n",
    "cv_true = np.concatenate(cv_true)\n",
    "cv_phigh = np.concatenate(cv_phigh)\n",
    "\n",
    "thr_mk, cv_f1_mk = tune_thr_from_proba(cv_true, cv_phigh)\n",
    "\n",
    "probs_full = train_markov_global(train_pool_df)\n",
    "p_test_mk = markov_proba(probs_full, test_df)\n",
    "pred_test_mk = (p_test_mk >= thr_mk).astype(int)\n",
    "markov_metrics = eval_bin(y_test, pred_test_mk)\n",
    "\n",
    "print(\"\\n=== BASELINE L2: Markov GLOBAL(prev_high, dow) ===\")\n",
    "print(\"Best thr:\", thr_mk, \"| CV pooled F1:\", round(cv_f1_mk, 4))\n",
    "print(\"TEST:\", markov_metrics)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) PREPROCESS (TRUE GLOBAL: tanpa userID)\n",
    "# =========================\n",
    "# Cat: dow (lebih aman di-onehot). is_weekend bisa numeric.\n",
    "cat_cols = [\"dow\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) CANDIDATE MODELS (semua yang relevan)\n",
    "# =========================\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__C\": [0.03, 0.1, 0.3, 1.0, 3.0], \"clf__solver\": [\"liblinear\"]}\n",
    "    ),\n",
    "\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        {\"clf__max_depth\": [2, 3, 4, 6, None], \"clf__min_samples_leaf\": [1, 2, 4, 8]}\n",
    "    ),\n",
    "\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1),\n",
    "        {\"clf__n_estimators\": [200, 400, 800], \"clf__max_depth\": [None, 6, 10],\n",
    "         \"clf__min_samples_leaf\": [1, 2, 4], \"clf__max_features\": [\"sqrt\"]}\n",
    "    ),\n",
    "\n",
    "    \"HistGB\": (\n",
    "        HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__max_depth\": [2, 3],\n",
    "         \"clf__max_leaf_nodes\": [15, 31, 63]}\n",
    "    ),\n",
    "\n",
    "    \"GradBoost\": (\n",
    "        GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1], \"clf__n_estimators\": [100, 200, 400],\n",
    "         \"clf__max_depth\": [2, 3]}\n",
    "    ),\n",
    "\n",
    "    \"AdaBoost\": (\n",
    "        AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "        {\"clf__learning_rate\": [0.03, 0.05, 0.1, 0.3], \"clf__n_estimators\": [50, 100, 200, 400]}\n",
    "    ),\n",
    "\n",
    "    \"BaggingTree\": (\n",
    "        BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1\n",
    "        ),\n",
    "        {\"clf__n_estimators\": [50, 100, 200],\n",
    "         \"clf__estimator__max_depth\": [2, 3, 4, None],\n",
    "         \"clf__estimator__min_samples_leaf\": [1, 2, 4]}\n",
    "    ),\n",
    "\n",
    "    \"LinearSVC_Calibrated\": (\n",
    "        CalibratedClassifierCV(\n",
    "            estimator=LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "            method=\"sigmoid\",\n",
    "            cv=3\n",
    "        ),\n",
    "        {\"clf__estimator__C\": [0.03, 0.1, 0.3, 1.0, 3.0]}\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) FAIR TUNING: CV pooled + threshold tuning + optional BLEND vs Markov\n",
    "# =========================\n",
    "def pooled_cv_best(pipe, grid):\n",
    "    \"\"\"\n",
    "    Cari params terbaik dgn pooled CV:\n",
    "    - kumpulkan proba di semua validation folds\n",
    "    - tune threshold yang memaksimalkan F1\n",
    "    - kalau USE_BLEND: juga tune alpha (campur p_ml & p_markov fold-specific)\n",
    "    \"\"\"\n",
    "    best = None\n",
    "\n",
    "    for params in ParameterGrid(grid):\n",
    "        # kumpulkan untuk CV pooled\n",
    "        y_list, pml_list, pmk_list = [], [], []\n",
    "\n",
    "        for tr_idx, va_idx in cv_splits:\n",
    "            tr_df = feat.loc[tr_idx]\n",
    "            va_df = feat.loc[va_idx]\n",
    "\n",
    "            # Markov fold-specific (no leak)\n",
    "            mk_probs = train_markov_global(tr_df)\n",
    "            p_mk = markov_proba(mk_probs, va_df)\n",
    "\n",
    "            # ML fold-specific\n",
    "            Xtr = tr_df[feature_cols].copy()\n",
    "            ytr = tr_df[\"y_bin\"].astype(int).values\n",
    "            Xva = va_df[feature_cols].copy()\n",
    "            yva = va_df[\"y_bin\"].astype(int).values\n",
    "\n",
    "            pipe.set_params(**params)\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            p_ml = pipe.predict_proba(Xva)[:, 1]\n",
    "\n",
    "            y_list.append(yva)\n",
    "            pml_list.append(p_ml)\n",
    "            pmk_list.append(p_mk)\n",
    "\n",
    "        y_all = np.concatenate(y_list)\n",
    "        pml_all = np.concatenate(pml_list)\n",
    "        pmk_all = np.concatenate(pmk_list)\n",
    "\n",
    "        if USE_BLEND:\n",
    "            # tune alpha & thr\n",
    "            local_best = None\n",
    "            for alpha in ALPHAS:\n",
    "                p_blend = alpha * pml_all + (1.0 - alpha) * pmk_all\n",
    "                thr, cv_f1 = tune_thr_from_proba(y_all, p_blend)\n",
    "                if (local_best is None) or (cv_f1 > local_best[\"cv_f1\"]):\n",
    "                    local_best = {\"alpha\": float(alpha), \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "            score = local_best[\"cv_f1\"]\n",
    "            record = {\"params\": params, **local_best}\n",
    "        else:\n",
    "            thr, cv_f1 = tune_thr_from_proba(y_all, pml_all)\n",
    "            score = cv_f1\n",
    "            record = {\"params\": params, \"alpha\": 1.0, \"thr\": float(thr), \"cv_f1\": float(cv_f1)}\n",
    "\n",
    "        if (best is None) or (score > best[\"cv_f1\"]):\n",
    "            best = record\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "print(\"\\n=== TRAIN + TUNE (Fair CV) : ML (+ optional BLEND) vs Markov ===\")\n",
    "rows = []\n",
    "\n",
    "for name, (clf, grid) in CANDIDATES.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    best = pooled_cv_best(pipe, grid)\n",
    "\n",
    "    # train final ML on full TrainPool\n",
    "    pipe.set_params(**best[\"params\"])\n",
    "    pipe.fit(X_trainpool, y_trainpool)\n",
    "    p_test_ml = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Markov prob on test from TrainPool Markov\n",
    "    p_test_mk_full = markov_proba(probs_full, test_df)\n",
    "\n",
    "    # final pred\n",
    "    alpha = float(best[\"alpha\"])\n",
    "    p_test_final = alpha * p_test_ml + (1.0 - alpha) * p_test_mk_full\n",
    "    pred_test_final = (p_test_final >= best[\"thr\"]).astype(int)\n",
    "\n",
    "    test_metrics = eval_bin(y_test, pred_test_final)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_f1\": float(best[\"cv_f1\"]),\n",
    "        \"alpha\": float(best[\"alpha\"]),\n",
    "        \"thr\": float(best[\"thr\"]),\n",
    "        \"test_f1\": float(test_metrics[\"f1\"]),\n",
    "        \"test_acc\": float(test_metrics[\"acc\"]),\n",
    "        \"params\": best[\"params\"],\n",
    "        \"pipe\": pipe,\n",
    "    })\n",
    "\n",
    "# Leaderboard\n",
    "print(\"\\n=== LEADERBOARD (sorted by TEST F1) ===\")\n",
    "base_rows = [\n",
    "    {\"model\": \"Baseline-Persist\", \"cv_f1\": np.nan, \"alpha\": np.nan, \"thr\": np.nan, \"test_f1\": persist_metrics[\"f1\"], \"test_acc\": persist_metrics[\"acc\"], \"params\": None},\n",
    "    {\"model\": \"Baseline-Markov\",  \"cv_f1\": cv_f1_mk, \"alpha\": 0.0, \"thr\": thr_mk, \"test_f1\": markov_metrics[\"f1\"], \"test_acc\": markov_metrics[\"acc\"], \"params\": {\"markov\": \"prev_high+dow\"}},\n",
    "]\n",
    "all_rows = base_rows + rows\n",
    "all_sorted = sorted(all_rows, key=lambda r: r[\"test_f1\"], reverse=True)\n",
    "\n",
    "for r in all_sorted:\n",
    "    cv_txt = \"NA\" if (r[\"cv_f1\"] is None or (isinstance(r[\"cv_f1\"], float) and np.isnan(r[\"cv_f1\"]))) else f\"{r['cv_f1']:.4f}\"\n",
    "    alpha_txt = \"NA\" if (r[\"alpha\"] is None or (isinstance(r[\"alpha\"], float) and np.isnan(r[\"alpha\"]))) else f\"{r['alpha']:.2f}\"\n",
    "    thr_txt = \"NA\" if (r[\"thr\"] is None or (isinstance(r[\"thr\"], float) and np.isnan(r[\"thr\"]))) else f\"{r['thr']:.2f}\"\n",
    "    print(f\"{r['model']:<20} | CV f1={cv_txt:<7} | TEST f1={r['test_f1']:.4f} acc={r['test_acc']:.4f} | alpha={alpha_txt} thr={thr_txt} | params={r['params']}\")\n",
    "\n",
    "best_ml = sorted(rows, key=lambda r: r[\"test_f1\"], reverse=True)[0]\n",
    "print(\"\\n✅ BEST (ML/BLEND):\", best_ml[\"model\"], \"| TEST:\", {\"f1\": round(best_ml[\"test_f1\"], 4), \"acc\": round(best_ml[\"test_acc\"], 4)})\n",
    "\n",
    "# pilih terbaik overall (Markov vs best ML/Blend)\n",
    "if best_ml[\"test_f1\"] > markov_metrics[\"f1\"]:\n",
    "    best_name = best_ml[\"model\"]\n",
    "    best_obj = {\n",
    "        \"type\": \"true_global_blend_model\",\n",
    "        \"pipe\": best_ml[\"pipe\"],               # sklearn pipeline\n",
    "        \"alpha\": float(best_ml[\"alpha\"]),\n",
    "        \"thr\": float(best_ml[\"thr\"]),\n",
    "        \"markov_probs\": probs_full,            # untuk hitung p_markov runtime\n",
    "        \"meta\": {\n",
    "            \"note\": \"TRUE GLOBAL (no userID). Prediction uses p = alpha*p_ml + (1-alpha)*p_markov\",\n",
    "            \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "            \"alphas\": ALPHAS.tolist(),\n",
    "            \"behavior_cols\": BEHAVIOR_COLS,\n",
    "        }\n",
    "    }\n",
    "    print(\"\\n🎉 TRUE GLOBAL model BEATS Markov on TEST. Saving BLEND as best.\")\n",
    "else:\n",
    "    best_name = \"MarkovGlobal\"\n",
    "    best_obj = {\n",
    "        \"type\": \"true_global_markov\",\n",
    "        \"thr\": float(thr_mk),\n",
    "        \"probs\": probs_full,\n",
    "        \"meta\": {\n",
    "            \"note\": \"TRUE GLOBAL baseline Markov remains best on TEST for this dataset\",\n",
    "            \"target\": \"y_bin=(stressLevelPred>=1)\",\n",
    "            \"window\": WINDOW,\n",
    "            \"test_len\": TEST_LEN,\n",
    "            \"val_windows\": VAL_WINDOWS,\n",
    "            \"thresholds\": THRESHOLDS.tolist(),\n",
    "        }\n",
    "    }\n",
    "    print(\"\\nℹ️ Markov still best on TEST. Saving Markov as best (robust).\")\n",
    "\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_obj, MODEL_OUT)\n",
    "print(\"Saved:\", MODEL_OUT, \"| Best:\", best_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
